{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math \nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython. display import clear_output\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\n#os.system('pip install iterative-stratification==0.1.7')\n#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nsys.path.append('../input/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nos.system('pip uninstall -y transformers')\nos.system('pip uninstall -y tokenizers')\nos.system('python -m pip install --no-index --find-links=../input/fb3-my-pip-wheels transformers')\nos.system('python -m pip install --no-index --find-links=../input/fb3-my-pip-wheels tokenizers')\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom transformers import DataCollatorWithPadding\n%env TOKENIZERS_PARALLELISM=false\n\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.linear_model import RidgeCV\n\nclear_output()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nprint('device:', device)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":46.885994,"end_time":"2022-10-27T03:31:20.251905","exception":false,"start_time":"2022-10-27T03:30:33.365911","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-28T02:42:43.762617Z","iopub.execute_input":"2022-11-28T02:42:43.763502Z","iopub.status.idle":"2022-11-28T02:43:22.961443Z","shell.execute_reply.started":"2022-11-28T02:42:43.763373Z","shell.execute_reply":"2022-11-28T02:43:22.960506Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"tokenizers.__version__: 0.12.1\ntransformers.__version__: 4.20.1\ndevice: cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\nfrom sklearn.linear_model import Ridge, Lasso, BayesianRidge, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom joblib import dump, load\nif str(device) == 'cpu':\n    from sklearn.svm import SVR\nelse:\n    from cuml.svm import SVR\nfrom sklearn.pipeline import FeatureUnion\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:22.963708Z","iopub.execute_input":"2022-11-28T02:43:22.964809Z","iopub.status.idle":"2022-11-28T02:43:23.968201Z","shell.execute_reply.started":"2022-11-28T02:43:22.964769Z","shell.execute_reply":"2022-11-28T02:43:23.966389Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/feedback-prize-english-language-learning'\nSUBMISSION_PATH = os.path.join(BASE_PATH, 'sample_submission.csv')\nTRAIN_PATH = os.path.join(BASE_PATH, 'train.csv')\nTEST_PATH = os.path.join(BASE_PATH, 'test.csv')","metadata":{"papermill":{"duration":0.014254,"end_time":"2022-10-27T03:31:20.272389","exception":false,"start_time":"2022-10-27T03:31:20.258135","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-28T02:43:23.969851Z","iopub.execute_input":"2022-11-28T02:43:23.970251Z","iopub.status.idle":"2022-11-28T02:43:23.976692Z","shell.execute_reply.started":"2022-11-28T02:43:23.970214Z","shell.execute_reply":"2022-11-28T02:43:23.975631Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class FB3Dataset(Dataset):\n    def __init__(self, cfg, data):\n        self.cfg = cfg\n        self.xs = preprocess(data['full_text'])\n        self.ys = data[cfg.target_cols].values \n        \n    def __len__(self):\n        return len(self.xs)\n    \n    def __getitem__(self, idx):\n        x = encode_text(self.cfg, self.xs[idx])\n        y = torch.tensor(self.ys[idx], dtype=torch.float)\n        return x, y\n    \ndef collate(inputs):\n    # Trimming input.\n    mask_len = int(inputs['attention_mask'].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    #print(type(inputs), inputs['attention_mask'].size())\n    return inputs\n\n##################################################################################\n","metadata":{"papermill":{"duration":0.046562,"end_time":"2022-10-27T03:31:20.325116","exception":false,"start_time":"2022-10-27T03:31:20.278554","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-28T02:43:23.978823Z","iopub.execute_input":"2022-11-28T02:43:23.979090Z","iopub.status.idle":"2022-11-28T02:43:23.987987Z","shell.execute_reply.started":"2022-11-28T02:43:23.979066Z","shell.execute_reply":"2022-11-28T02:43:23.986771Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n    \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features, *args):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()\n    \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        last_hidden_state_masked = last_hidden_state\n        last_hidden_state_masked[input_mask_expanded == 0] = -1e-9 \n        max_embeddings = torch.max(last_hidden_state_masked, 1)[0]\n        return max_embeddings\n    \nclass LSTMPooling(nn.Module):\n    def __init__(self, num_layers, hidden_size, hiddendim_lstm):\n        super(LSTMPooling, self).__init__()\n        self.num_hidden_layers = num_layers\n        self.hidden_size = hidden_size\n        self.hiddendim_lstm = hiddendim_lstm\n        self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n        self.dropout = nn.Dropout(0.1)\n    \n    def forward(self, all_hidden_states):\n        # only use first and last\n        hidden_states = torch.stack([\n            all_hidden_states[layer_i][:, 0].squeeze()\n            for layer_i in (-1, 0)],\n            dim=-1)\n        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n        out, _ = self.lstm(hidden_states, None)\n        #out = self.dropout(out[:, -1, :])\n        out = self.dropout(out.mean(dim=1))\n        return out\n\nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 9, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n        return weighted_average\n\nclass AttentionPooling(nn.Module):\n    def __init__(self, in_dim):\n        super().__init__()\n        self.attention = nn.Sequential(\n        nn.Linear(in_dim, in_dim),\n        nn.LayerNorm(in_dim),\n        nn.GELU(),\n        nn.Linear(in_dim, 1),\n        )\n\n    def forward(self, last_hidden_state, attention_mask):\n        w = self.attention(last_hidden_state).float()\n        w[attention_mask==0]=float('-inf')\n        w = torch.softmax(w,1)\n        attention_embeddings = torch.sum(w * last_hidden_state, dim=1)\n        return attention_embeddings\n\nclass MultiSampleDropout(nn.Module):\n    def __init__(self, fc, num_dropout, prob_dropout):\n        super(MultiSampleDropout, self).__init__()\n        self.dropout = nn.Dropout\n        self.num_dropout = num_dropout\n        self.prob_dropout = prob_dropout\n        self.classifier = fc\n    def forward(self, out):\n        if not type(self.prob_dropout) in [float, int]:            \n            fcs = [self.classifier(self.dropout(p)(out)) for p in self.prob_dropout]\n        else:\n            fcs = [self.classifier(self.dropout(self.prob_dropout)(out)) for _ in range(self.num_dropout)]\n        \n        return torch.mean(torch.stack(fcs, dim=0), dim=0)\n\n# ====================================================\n# Model classes\n# ====================================================\nclass FB3Model(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg \n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n            # Turn off dropouts.\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n            #LOGGER.info(self.config)\n        else:\n            self.config = torch.load(config_path)\n        \n        if pretrained:\n            self.deberta_v3 = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.deberta_v3 = AutoModel.from_config(self.config)\n\n        #if self.cfg.reinit_last_layer:\n        #    # Re-init last layer of deberta.\n        #    for module in self.deberta_v3.encoder.layer[-1:].modules():\n        #        self._init_weights(module)\n        # self.deberta_v3.gradient_checkpointing_enable()\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            #nn.init.xavier_uniform_(module.weight.data, gain=1.0)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\nclass WMPoolModel(FB3Model):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__(cfg, config_path=config_path, pretrained=pretrained)\n\n        # Poolings.\n        self.mean_head = MeanPooling()\n        self.wpool_head = WeightedLayerPooling(self.config.num_hidden_layers, layer_start=12)\n\n        self.fc_out = nn.Linear(self.config.hidden_size, cfg.num_target)\n        self._init_weights(self.fc_out)\n        \n        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n        self.qa_output = torch.nn.Linear(self.config.hidden_size, 2)\n        self.attention_head = AttentionHead(self.config.hidden_size*4, self.config.hidden_size)\n    \n    def feature(self, x):\n        pt_out = self.deberta_v3(**x)\n        all_hidden_states = torch.stack(pt_out.hidden_states)\n        # Weighted pooling of last n layers.\n        logits = self.wpool_head(all_hidden_states)[:, 0] # Bx768\n        return logits\n    \n    def forward(self, x):\n        logits = self.feature(x)\n        y_hat = self.fc_out(logits)\n        return y_hat\n\nclass MultiPoolModel(FB3Model):\n    def __init__(self, cfg, config_path=None, pretrained=False, pool='mean'):\n        super().__init__(cfg, config_path=config_path, pretrained=pretrained)\n        \n        # Define model layers.\n        self.pool_name = cfg.pool_head\n        self.fc = nn.Linear(self.config.hidden_size, self.cfg.num_target)\n        if cfg.pool_head in ['mean', 'attention', 'weighted']:\n            self.pool = self._pool_layer(cfg.pool_head)\n        elif '-' in cfg.pool_head:\n            pools = cfg.pool_head.split('-')\n            self.pool = nn.ModuleList([])\n            for pool_ in pools:\n                self.pool.append(self._pool_layer(pool_))\n            self.fc = nn.Linear(self.config.hidden_size * len(self.pool), self.cfg.num_target)\n        self._init_weights(self.fc)\n        \n        # Multi-sample dropout.\n        self.multi_dropout = MultiSampleDropout(self.fc, self.cfg.num_dropout, self.cfg.prob_dropout)\n    \n    def _pool_layer(self, pool_name):\n        assert pool_name in ['mean', 'attention', 'weighted']\n        if pool_name == 'mean':\n            pool = MeanPooling()\n        elif pool_name == 'attention':\n            pool = AttentionHead(self.config.hidden_size, self.config.hidden_size)\n        elif pool_name == 'weighted':\n            pool = WeightedLayerPooling(\n                self.config.num_hidden_layers, \n                layer_start=9,\n                layer_weights=None)\n        return pool\n    \n    def _pool_feature(self, pool, pool_name, pt_outputs, attention_mask):\n        assert pool_name in ['mean', 'attention', 'weighted']\n        last_hidden_state = pt_outputs.last_hidden_state #batch_size x max_len x hidden_size\n        all_hidden_states = torch.stack(pt_outputs.hidden_states) #num_layer x batch_size x max_len x hidden_size\n        \n        if pool_name == 'mean':\n            pool_feature = pool(last_hidden_state, attention_mask)\n        elif pool_name == 'attention':\n            pool_feature = pool(last_hidden_state)\n        elif pool_name == 'weighted':\n            # Take the CLS token only.\n            pool_feature = pool(all_hidden_states)[:, 0]\n        return pool_feature\n\n    def feature(self, x):\n        pt_outputs = self.deberta_v3(**x)\n        \n        # Pooling feat.\n        if type(self.pool) == nn.ModuleList:\n            pool_features = []\n            pool_names = self.pool_name.split('-')\n            \n            for pool_name, pool in zip(pool_names, self.pool):\n                pool_features.append(self._pool_feature(pool, pool_name, pt_outputs, x['attention_mask']))\n            pool_features = torch.cat(pool_features, dim=1)\n        else:\n            pool_features = self._pool_feature(self.pool, self.pool_name, pt_outputs, x['attention_mask'])\n        return pool_features\n    \n    def forward(self, x):\n        feature = self.feature(x)\n        if self.cfg.use_dropout and self.training:\n            y_hat = self.multi_dropout(feature)\n        else:\n            y_hat = self.fc(feature)\n        return y_hat\n\n\nclass Attention4Model(FB3Model):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__(cfg, config_path=config_path, pretrained=pretrained)\n        \n        self.head = AttentionHead(self.config.hidden_size*4, self.config.hidden_size)\n        self.fc_out = nn.Linear(self.config.hidden_size*4*2, self.cfg.num_target)\n        self._init_weights(self.fc_out)\n\n    def forward(self, x):\n        pt_out = self.deberta_v3(**x)\n        \n        all_hidden_states = torch.stack(pt_out.hidden_states)\n        cat_over_last_layers = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),\n            -1)\n        # [CLS] embedding.\n        cls_pooling = cat_over_last_layers[:, 0]   \n        # Concat of 4 last layers.\n        head_logits = self.head(cat_over_last_layers)\n\n        if self.cfg.use_dropout and self.training:\n            y_hat = self.multi_dropout(torch.cat([head_logits, cls_pooling], -1))\n        else:\n            y_hat = self.fc_out(torch.cat([head_logits, cls_pooling], -1))\n\n        return y_hat\n    \nclass AttentionModel(FB3Model):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__(cfg, config_path=config_path, pretrained=pretrained)\n\n        # Poolings.\n        self.att = nn.Sequential(\n            nn.Linear(self.config.hidden_size, self.cfg.ap_hidden_size),\n            nn.Tanh(),\n            nn.Linear(self.cfg.ap_hidden_size, 1),\n            nn.Softmax(dim=1),\n        )\n        self._init_weights(self.att)\n        \n        self.fc_out = nn.Linear(self.config.hidden_size, cfg.num_target)\n        self._init_weights(self.fc_out)\n\n    def forward(self, x):\n        pt_out = self.deberta_v3(**x)\n        last_hidden_states = pt_out.last_hidden_state\n        att_weights = self.att(last_hidden_states)\n        logits =  torch.sum(att_weights * last_hidden_states, dim=1)\n        \n        y_hat = self.fc_out(logits)\n        return y_hat\n    \nclass MeanModel(FB3Model):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__(cfg, config_path=config_path, pretrained=pretrained)\n\n        # Poolings.\n        self.mean_head = MeanPooling()\n\n        # Head.\n        self.fc_out = nn.Linear(self.config.hidden_size, cfg.num_target)\n        self._init_weights(self.fc_out)\n    \n    def forward(self, x):\n        pt_out = self.deberta_v3(**x)\n        # Mean pooling.\n        logits = self.mean_head(pt_out.last_hidden_state, x['attention_mask'])\n        y_hat = self.fc_out(logits)\n        return y_hat\n\n######################################\n\nclass FB3Model(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg \n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n            # Turn off dropouts.\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n            #LOGGER.info(self.config)\n        else:\n            self.config = torch.load(config_path)\n        \n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n\n        #if self.cfg.reinit_last_layer:\n        #    # Re-init last layer of deberta.\n        #    for module in self.model.encoder.layer[-1:].modules():\n        #        self._init_weights(module)\n        #self.model.gradient_checkpointing_enable()\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\nclass WeightedAttentionModel(FB3Model):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__(cfg, config_path=config_path, pretrained=pretrained)\n\n        self.weighted_pool = WeightedLayerPooling(\n            self.config.num_hidden_layers, layer_start=9, layer_weights=None)\n        self.att_pool = AttentionPooling(self.config.hidden_size)\n\n        self.fc_out = nn.Linear(self.config.hidden_size*2, cfg.num_target)\n        self._init_weights(self.fc_out)\n    \n    def feature(self, x):\n        pt_out = self.model(**x)\n        hidden_states = pt_out.hidden_states\n        last_hidden_state = pt_out.last_hidden_state\n        x1 = self.weighted_pool(torch.stack(hidden_states))[:, 0]\n        x2 = self.att_pool(last_hidden_state, x['attention_mask'])\n        return torch.cat([x1, x2], dim=1)\n    \n    def forward(self, x):\n        feature = self.feature(x)\n        y_hat = self.fc_out(feature)\n        return y_hat\n    \n########################\n\nclass WeightedLayerPooling_(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layers = None, layer_weights = None):\n        super(WeightedLayerPooling_, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        \n        if layers:\n            self.layer_weights = layer_weights if layer_weights is not None \\\n                else nn.Parameter(\n                    torch.tensor([1] * len(layers), dtype=torch.float)\n                )\n            self.layers = layers\n        else:\n            self.layer_weights = layer_weights if layer_weights is not None \\\n                else nn.Parameter(\n                   torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n                )\n            self.layers = list(range(layer_start, num_hidden_layers+1))\n            \n\n    def forward(self, ft_all_layers):\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layers, :, :, :]\n        \n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        return weighted_average\n    \nclass MeanAttentionModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg \n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n            # Turn off dropouts.\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n            #LOGGER.info(self.config)\n        else:\n            self.config = torch.load(config_path)\n        \n        if pretrained:\n            self.deberta_v3 = AutoModel.from_pretrained(cfg.model, config=self.config)\n            # Expand embedding dim for new tokens.\n            self.deberta_v3.resize_token_embeddings(len(cfg.tokenizer))\n        else:\n            self.deberta_v3 = AutoModel.from_config(self.config)\n            \n        self.deberta_v3.gradient_checkpointing_enable()\n        \n        # Define model layers.\n        self.fc = nn.Linear(self.config.hidden_size, 6)\n\n        if cfg.pool == 'mean':\n            self.pool = MeanPooling()\n        elif cfg.pool == 'attention':\n            self.pool = AttentionHead(self.config.hidden_size, self.config.hidden_size)\n        elif cfg.pool == 'mean-attention':\n            self.pool = nn.ModuleList([\n                MeanPooling(),\n                AttentionHead(self.config.hidden_size, self.config.hidden_size)\n            ])\n            self.fc = nn.Linear(self.config.hidden_size * len(self.pool), 6)\n        elif cfg.pool == 'mean-attention-with-mask':\n            self.pool = nn.ModuleList([\n                MeanPooling(),\n                AttentionPooling(self.config.hidden_size)\n            ])\n            self.fc = nn.Linear(self.config.hidden_size * len(self.pool), 6)\n        # Re-init weights.\n        self._init_weights(self.fc)\n        \n        # Multi-sample dropout.\n        self.multi_dropout = MultiSampleDropout(self.fc, cfg.num_dropout, cfg.prob_dropout)\n        \n    def global_avg_pool(x):\n        return torch.mean(x.view(x.size(0), x.size(1), -1), dim=-1)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    \n    def feature(self, x):\n        pt_outputs = self.deberta_v3(**x)\n        last_hidden_states = pt_outputs[0] # N x max_len x 768\n        # Pooling feat.\n        if type(self.pool) == nn.ModuleList:\n            pool_feature = [pool(last_hidden_states, x['attention_mask']) for pool in self.pool]\n            pool_feature = torch.cat(pool_feature, dim=1)\n        else:\n            pool_feature = self.pool(last_hidden_states, x['attention_mask']) # N x 768\n        return pool_feature\n    \n    def forward(self, x, y=None, loss_fn=None):\n        feature = self.feature(x)\n        # if self.training:\n        #    out = self.multi_dropout(feature)\n        # else:\n        #    out = self.fc(feature)\n        out = self.fc(feature)\n        return out\n    \nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        self.cfg = cfg\n        super().__init__()\n        \n        self.config = AutoConfig.from_pretrained(config_path, output_hidden_states=True)\n        self.model = AutoModel.from_config(self.config)\n        self.pretrained = pretrained\n        \n        if cfg.pooling == 'mean':\n            self.pool = MeanPooling()\n        elif cfg.pooling == 'max':\n            self.pool = MaxPooling()\n        elif cfg.pooling == 'min':\n            self.pool = MinPooling()\n        elif cfg.pooling == 'attention':\n            self.pool = AttentionPooling(self.config.hidden_size)\n        elif cfg.pooling == 'weightedlayer':\n            self.pool = WeightedLayerPooling_(self.config.num_hidden_layers, layer_start = cfg.layer_start, layer_weights = None)        \n        elif self.cfg.pooling == 'attention4':\n            self.pool = AttentionHead(self.config.hidden_size*4, 512)\n\n        if self.cfg.pooling == 'attention4':\n            self.fc = nn.Linear(self.config.hidden_size*8, 6)\n        else:\n            self.fc = nn.Linear(self.config.hidden_size, 6)\n                \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        if self.cfg.pooling == 'attention4':\n            all_layer_embeddings = torch.stack(outputs.hidden_states)\n            cat_over_last_layers = torch.cat((all_layer_embeddings[-1], all_layer_embeddings[-2], all_layer_embeddings[-3], all_layer_embeddings[-4]), -1)\n            cls_pooling = cat_over_last_layers[:, 0]\n            head_logits = self.pool(cat_over_last_layers)\n            feature = torch.cat([head_logits, cls_pooling], -1)\n        elif self.cfg.pooling != 'weightedlayer':\n            last_hidden_states = outputs[0]\n            feature = self.pool(last_hidden_states, inputs['attention_mask'])\n        else:\n            all_layer_embeddings = outputs[1]\n            feature = self.pool(all_layer_embeddings)[:, 0]\n            \n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:23.990723Z","iopub.execute_input":"2022-11-28T02:43:23.991127Z","iopub.status.idle":"2022-11-28T02:43:24.374675Z","shell.execute_reply.started":"2022-11-28T02:43:23.991097Z","shell.execute_reply":"2022-11-28T02:43:24.372545Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Config(dict):\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n    \n    def init(self, kwargs):\n        super().init(kwargs)\n\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)","metadata":{"papermill":{"duration":0.014157,"end_time":"2022-10-27T03:31:20.345369","exception":false,"start_time":"2022-10-27T03:31:20.331212","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-28T02:43:24.376507Z","iopub.execute_input":"2022-11-28T02:43:24.377267Z","iopub.status.idle":"2022-11-28T02:43:24.385832Z","shell.execute_reply.started":"2022-11-28T02:43:24.377227Z","shell.execute_reply":"2022-11-28T02:43:24.383323Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"loggers = {}\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.propagate = False\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = get_logger()\n\ndef seed_everything(seed=42):\n    '''\n    Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.\n    '''\n    random.seed(seed)\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nseed_everything(seed=42)\n\ndef mc_rmse(y_true, y_pred):\n    scores = []\n    ncols = y_true.shape[1]\n    \n    for n in range(ncols):\n        yn_true = y_true[:, n]\n        yn_pred = y_pred[:, n]\n        rmse_ = mean_squared_error(yn_true, yn_pred, squared=False)\n        scores.append(rmse_)\n    score = np.mean(scores) \n    return score, scores\n\ndef get_result(cfg, oof_df, verbose=1):\n    labels = oof_df[cfg.target_cols].values\n    preds = oof_df[[f\"pred_{c}\" for c in cfg.target_cols]].values\n    score, scores = mc_rmse(labels, preds)\n    if verbose == 1:\n        print(f'score: {score:<.8f}  scores: {scores}')\n    return score","metadata":{"papermill":{"duration":0.022462,"end_time":"2022-10-27T03:31:20.373972","exception":false,"start_time":"2022-10-27T03:31:20.351510","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-28T02:43:24.388519Z","iopub.execute_input":"2022-11-28T02:43:24.388985Z","iopub.status.idle":"2022-11-28T02:43:24.421094Z","shell.execute_reply.started":"2022-11-28T02:43:24.388947Z","shell.execute_reply":"2022-11-28T02:43:24.419892Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef encode_text(cfg, text):\n    if cfg.pretrained:\n        inputs = cfg.tokenizer(\n            text,\n            None,\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=cfg.max_len,\n            return_token_type_ids=True,\n            return_tensors='pt'\n        )\n        inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n    else:\n        inputs = cfg.tokenizer.encode_plus(\n            text, \n            return_tensors=None, \n            add_special_tokens=True, \n            #max_length=CFG.max_len,\n            #pad_to_max_length=True,\n            #truncation=True\n        )\n        for k, v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs \n\ndef preprocess(texts):\n    texts = (\n        texts\n        .str.replace(r'\\r\\n', '<newline>', regex=True)\n        .str.replace(r'\\n', '<newline>', regex=True)\n        .str.replace('<newline><newline>', '<newline>', regex=False)\n        .values \n    )\n    return texts\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        if not cfg.pretrained and cfg.version in ['1', 'mean-attention']:\n            print('preprocess')\n            self.texts = preprocess(df['full_text'])\n        else:\n            self.texts = df['full_text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = encode_text(self.cfg, self.texts[item])\n        return inputs","metadata":{"papermill":{"duration":0.018786,"end_time":"2022-10-27T03:31:20.398676","exception":false,"start_time":"2022-10-27T03:31:20.379890","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-28T02:43:24.422708Z","iopub.execute_input":"2022-11-28T02:43:24.423274Z","iopub.status.idle":"2022-11-28T02:43:24.435510Z","shell.execute_reply.started":"2022-11-28T02:43:24.423233Z","shell.execute_reply":"2022-11-28T02:43:24.434473Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def load_config(input_path, inference_weight=1):\n    # Load CFG class.\n    cfg = Config(**json.load(open(os.path.join(input_path, 'CFG.json'), 'r')))\n    cfg.path = input_path\n    cfg.config_path = os.path.join(cfg.path, 'config.pth')\n    # Load tokenizer.\n    tokenizer = AutoTokenizer.from_pretrained(os.path.join(cfg.path, 'tokenizer'))\n    cfg.tokenizer = tokenizer\n    \n    cfg.inference_weight = inference_weight\n    return cfg\n\ndef load_model(cfg, fold, version='1', **model_kwargs):\n    # Load torch model.\n    if version == '1':\n        model = MultiPoolModel(cfg, config_path=cfg.config_path, pretrained=False, **model_kwargs)\n    elif version == '2':\n        model = Attention4Model(cfg, config_path=cfg.config_path, pretrained=False, **model_kwargs)\n    elif version == '21':\n        model = WMPoolModel(cfg, config_path=cfg.config_path, pretrained=False, **model_kwargs)\n    elif version == 'weighted-attention':\n        model = WeightedAttentionModel(cfg, config_path=cfg.config_path, pretrained=False, **model_kwargs)\n    elif version == 'custom':\n        model = CustomModel(cfg, config_path=cfg.config_path, pretrained=False, **model_kwargs)\n    elif version == 'mean-attention':\n        model = MeanAttentionModel(cfg, config_path=cfg.config_path, pretrained=False)\n        \n    state = torch.load(\n        os.path.join(cfg.path, f\"{cfg.model.replace('/', '-')}_fold{fold}_best.pth\"),\n        map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    return model","metadata":{"papermill":{"duration":0.016253,"end_time":"2022-10-27T03:31:20.422220","exception":false,"start_time":"2022-10-27T03:31:20.405967","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-28T02:43:24.436934Z","iopub.execute_input":"2022-11-28T02:43:24.437238Z","iopub.status.idle":"2022-11-28T02:43:24.455504Z","shell.execute_reply.started":"2022-11-28T02:43:24.437212Z","shell.execute_reply":"2022-11-28T02:43:24.453975Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output.last_hidden_state.detach().cpu()\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    #tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in test_loader:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\n\nclass Inferencer:\n    def __init__(self, input_path=None, cfg=None, inference_weight=1):\n        if cfg == None:\n            self.cfg = load_config(input_path, inference_weight)\n        else:\n            self.cfg = cfg\n    \n    def predict(self, test_loader, device, stat_fn=np.mean):\n        preds = []\n        start = time.time()\n        print('#'*10, self.cfg.path, '#'*10)\n        for fold in self.cfg.trn_fold:\n            print(f'Predicting fold {fold}...')\n            model = load_model(self.cfg, fold, version=self.cfg.version)\n            pred = inference_fn(test_loader, model, device)\n            preds.append(pred)\n            del model, pred; gc.collect()\n            torch.cuda.empty_cache()\n        end = time.time() - start\n        print('#'*10, f'ETA: {end:.2f}s', '#'*10, '\\n')\n        \n        self.preds = stat_fn(preds, axis=0) \n        self.preds = np.clip(self.preds, 1, 5)\n        return self.preds\n    \n    def get_oof_result(self, file_type='pkl', verbose=1):\n        return get_result(self.cfg, self.get_oof_df(file_type), verbose)\n    \n    def get_oof_df(self, file_type='pkl'):\n        if file_type == 'pkl':\n            return pd.read_pickle(os.path.join(self.cfg.path, 'oof_df.pkl'))\n        return pd.read_csv(os.path.join(self.cfg.path, 'oof_df.csv'))\n    \n    def get_text_embedding(self, data_loader, device, fold=None): \n        # pretrained=True: not fine-tuned models.\n        if not self.cfg.pretrained:\n            model = load_model(self.cfg, fold, version=self.cfg.version)            \n        else:\n            model = AutoModel.from_pretrained(self.cfg.model)\n        model.to(device)\n        model.eval()\n            \n        fold_emb = []\n        for inputs in data_loader:\n            for k, v in inputs.items():\n                inputs[k] = v.to(device)\n            input_ids = inputs['input_ids'].to(device)\n            attention_mask = inputs['attention_mask'].to(device)\n            token_type_ids = inputs['token_type_ids'].to(device)\n            \n            if not self.cfg.pretrained:\n                with torch.no_grad():\n                    # emb = model(inputs)\n                    # output = model.deberta_v3(**inputs)\n                    emb = model.feature(inputs)\n            else:    \n                with torch.no_grad():\n                    try:\n                        output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                    except:\n                        output = model(input_ids=input_ids, attention_mask=attention_mask)\n                emb = mean_pooling(output, attention_mask.detach().cpu())\n                emb = F.normalize(emb, p=2, dim=1)\n                emb = emb.squeeze(0)\n            fold_emb.extend(emb.detach().cpu().numpy())\n            del emb; gc.collect(); torch.cuda.empty_cache();\n            #print(torch.cuda.memory_allocated() /1024/1024)\n            \n        fold_emb = np.array(fold_emb)\n        return fold_emb","metadata":{"papermill":{"duration":0.025419,"end_time":"2022-10-27T03:31:20.453489","exception":false,"start_time":"2022-10-27T03:31:20.428070","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-28T02:43:24.459953Z","iopub.execute_input":"2022-11-28T02:43:24.460369Z","iopub.status.idle":"2022-11-28T02:43:24.482122Z","shell.execute_reply.started":"2022-11-28T02:43:24.460334Z","shell.execute_reply":"2022-11-28T02:43:24.480927Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from scipy import optimize\ndef find_optimal_weights(oof_preds, labels, bounds=(0.0, 1.0), method='SLSQP'):\n    weights = [1] * len(oof_preds)\n    def loss(weights):\n        return mc_rmse(labels, np.clip(np.average(oof_preds, weights=weights, axis=0), 1, 5))[0]\n\n    opt_weights = optimize.minimize(\n        loss,\n        [1/len(oof_preds)] * len(oof_preds),\n        constraints=({'type': 'eq','fun': lambda w: 1-sum(w)}),\n        method= method, #'Nelder-Mead',\n        bounds=[bounds] * len(oof_preds),\n        options = {'ftol':1e-10},\n    )['x']\n\n    opt_weights = np.array(opt_weights) / sum(opt_weights)\n    #print('\\n')\n    #print(\"score:\", loss(opt_weights))\n    #print(', '.join([str(i) for i in opt_weights]))\n    return opt_weights, loss(opt_weights)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:24.483472Z","iopub.execute_input":"2022-11-28T02:43:24.484260Z","iopub.status.idle":"2022-11-28T02:43:24.501965Z","shell.execute_reply.started":"2022-11-28T02:43:24.484224Z","shell.execute_reply":"2022-11-28T02:43:24.500050Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"lgbm_params=\\\n  {'n_estimators':3000,\n   'boosting_type': 'gbdt',\n   'objective':'regression',\n   'metric':'rmse',\n   'subsample': 0.7, \n   'subsample_freq': 1,\n   #'num_leaves':124,\n   'min_data_in_leaf':40,\n   'feature_fraction_bynode':np.sqrt(0.9),\n   'feature_fraction': np.sqrt(0.9),            \n   'learning_rate': 0.001,\n   'max_bin':255,\n   #'cat_l2':10,\n   #'max_depth':5,\n   'boost_from_average':True,\n   'nthread' : 8,\n    'lambda_l1': 2,  \n    'lambda_l2': 20,\n  #'min_gain_to_split':0.0001\n   #'early_stopping_rounds':200,\n   'verbose':-1\n    }","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:24.503636Z","iopub.execute_input":"2022-11-28T02:43:24.503980Z","iopub.status.idle":"2022-11-28T02:43:24.521520Z","shell.execute_reply.started":"2022-11-28T02:43:24.503949Z","shell.execute_reply":"2022-11-28T02:43:24.519814Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# load configs","metadata":{}},{"cell_type":"code","source":"CFG = Config(\n    n_fold=15,\n    seed=42,\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n)\n\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)\nsubmission = pd.read_csv(SUBMISSION_PATH)\n\nmskfold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n\nfor n, (train_idx, val_idx) in enumerate(mskfold.split(train, train[CFG.target_cols])):\n    train.loc[val_idx, 'fold'] = int(n)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:24.523261Z","iopub.execute_input":"2022-11-28T02:43:24.523690Z","iopub.status.idle":"2022-11-28T02:43:24.915377Z","shell.execute_reply.started":"2022-11-28T02:43:24.523654Z","shell.execute_reply":"2022-11-28T02:43:24.913895Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"v114_CFG = Config(\n    model=\"microsoft/deberta-v3-base\",\n    version='1',\n    num_target = 6,\n    reinit_last_layer=True,\n    reinit_fc=True,\n    weight_decay=0.01,\n    learning_rate=2e-5,\n    layerwise_learning_rate_decay=1.5,\n    use_dropout=False,\n    prob_dropout=[0.06, 0.08, 0.1, 0.12, 0.14],\n    num_dropout=5,\n    pool_head='mean-attention',\n    seed=42,\n    n_fold=4,\n    trn_fold=[0,1,2,3],\n    path='../input/fb3models/v114/',\n    config_path='../input/fb3models/v114/config.pth',\n    tokenizer=AutoTokenizer.from_pretrained('../input/fb3models/v114/tokenizer')\n)\n\nweightedpool_CFG = Config(\n    model='microsoft/deberta-v3-base',\n    name='weightedpool',\n    version='1',\n    num_target=6,\n    reinit_last_layer=True,\n    reinit_fc=True,\n    weight_decay=0.01,\n    learning_rate=1.5e-5,\n    layerwise_learning_rate_decay=1.5,\n    use_dropout=False,\n    prob_dropout=[0.06, 0.08, 0.1, 0.12, 0.14],\n    num_dropout=5,\n    pool_head='weighted',\n    seed=42,\n    n_fold=4,\n    trn_fold=[0,1,2,3],\n    train=True,\n    path='../input/fb3-train/',\n    config_path='../input/fb3-train/config.pth',\n    tokenizer=AutoTokenizer.from_pretrained('../input/fb3-train/tokenizer'),\n    inference_weight=1.0)\n\nv116_CFG = load_config('../input/fb3-colab-models/v116', inference_weight=1.0)\nv116_CFG.path = '../input/fb3models/v116'\nv116_CFG.config_path = '../input/fb3models/v116/config.pth'\nv116_CFG.version = '1'\nv116_CFG.name = 'v116'\n\nv112_CFG = Config(\n    num_workers=1,\n    batch_size=3,\n    max_len=512,\n    model=\"microsoft/deberta-v3-base\",\n    name='v112',\n    version='1',\n    num_target = 6,\n    reinit_last_layer=True,\n    reinit_fc=True,\n    weight_decay=0.01,\n    learning_rate=2e-5,\n    layerwise_learning_rate_decay=1.5,\n    use_dropout=False,\n    prob_dropout=[0.06, 0.08, 0.1, 0.12, 0.14],\n    num_dropout=5,\n    pool_head='attention',\n    seed=42,\n    n_fold=4,\n    trn_fold=[0,1,2,3],\n    train=True,\n    path='../input/fb3models/v112/',\n    config_path='../input/fb3models/v112/config.pth',\n    inference_weight=1.0,\n    tokenizer=AutoTokenizer.from_pretrained('../input/fb3models/v112/tokenizer')\n)\n\n#####\nv2_CFG = load_config('../input/fb3models/v2/', inference_weight=1.0)\nv2_CFG.name = 'v2'\nv2_CFG.version = '2'\nv2_CFG.trn_fold = [0,1,2,3]\n\n#####\nv21_CFG = load_config('../input/fb3models/v21/', inference_weight=1)\nv21_CFG.name = 'v21'\nv21_CFG.version = '21'\n\n#####\nattention_fgm_CFG = load_config('../input/fb3models/20221114-192943-deberta-v3-base/', inference_weight=1.0)\nattention_fgm_CFG.name = 'attention_fgm'\nattention_fgm_CFG.version = 'custom'\nattention_fgm_CFG.config_path = '../input/fb3models/20221114-192943-deberta-v3-base/config/config.json'\n\nweighted_fgm_CFG = Config(\n    pretrained=False,\n    path='../input/fb3models/20221115-061243-deberta-v3-base',\n    config_path='../input/fb3models/20221115-061243-deberta-v3-base/config/config.json',\n    tokenizer=AutoTokenizer.from_pretrained('../input/fb3models/20221115-061243-deberta-v3-base/tokenizer'),\n    name='weighted_fgm',\n    version='custom',\n    train = True,\n    debug = False,\n    offline = False,\n    models_path = 'FB3-models',\n    epochs = 5,\n    save_all_models = False,\n    competition = 'FB3',\n    apex = True,\n    print_freq = 20,\n    num_workers = 4,\n    model = 'microsoft/deberta-v3-base', #If you want to train on the kaggle platform, v3-base is realistic. v3-large will time out.\n    loss_func = 'SmoothL1', # 'SmoothL1', 'RMSE'\n    gradient_checkpointing = True,\n    scheduler = 'cosine',\n    batch_scheduler = True,\n    num_cycles = 0.5,\n    num_warmup_steps = 0,\n    encoder_lr = 2e-5,\n    decoder_lr = 2e-5,\n    min_lr = 1e-6,\n    #Layer-Wise Learning Rate Decay\n    llrd = True,\n    layerwise_lr = 5e-5,\n    layerwise_lr_decay = 0.9,\n    layerwise_weight_decay = 0.01,\n    layerwise_adam_epsilon = 1e-6,\n    layerwise_use_bertadam = False,\n    #pooling\n    pooling = 'weightedlayer', # mean, max, min, attention, weightedlayer\n    layer_start = 11,\n    layers=None,\n    #init_weight\n    init_weight = 'normal', # normal, xavier_uniform, xavier_normal, kaiming_uniform, kaiming_normal, orthogonal\n    #re-init\n    reinit = True,\n    reinit_n = 1,\n    #adversarial\n    fgm = True,\n    awp = False,\n    adv_lr = 1,\n    adv_eps = 0.2,\n    unscale = False,\n    eps = 1e-6,\n    betas = (0.9, 0.999),\n    max_len = 512,\n    weight_decay = 0.01,\n    gradient_accumulation_steps = 1,\n    max_grad_norm = 1000,\n    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n    seed = 42,\n    cv_seed = 42,\n    n_fold = 4,\n    trn_fold = [0,1,2,3],\n    batch_size = 8,\n    n_targets = 6,\n    gpu_id = 0) \n\nweighted_attention_CFG = load_config('../input/fb3models/weighted_attention_v3', inference_weight=1.0)\nweighted_attention_CFG.name = 'weighted_attention'\nweighted_attention_CFG.version = 'weighted-attention'\n\nmean_attention_no_fgm_CFG = load_config('../input/fb3models/20221117-183420-deberta-v3-base-mean-attention-with-mask', inference_weight=1.0)\nmean_attention_no_fgm_CFG.name = 'mean_attention_no_fgm'\nmean_attention_no_fgm_CFG.version = 'mean-attention'\n\nattention_large_fgm_CFG = load_config('../input/fb3models/20221118-164148-deberta-v3-large-attention_fgm')\nattention_large_fgm_CFG.name = 'attention_large_fgm'\nattention_large_fgm_CFG.version = 'custom'\nattention_large_fgm_CFG.config_path = '../input/fb3models/20221118-164148-deberta-v3-large-attention_fgm/config/config.json'\n\nattention_fgm_512_CFG = load_config('../input/fb3models/20221121-143655-deberta-v3-base-attention_fgm_512')\nattention_fgm_512_CFG.name = 'attention_fgm_512'\nattention_fgm_512_CFG.version = 'custom'\nattention_fgm_512_CFG.config_path = '../input/fb3models/20221121-143655-deberta-v3-base-attention_fgm_512/config/config.json'\n\nattention_fgm_768_CFG = load_config('../input/fb3models/20221120-072218-deberta-v3-base-attention_fgm')\nattention_fgm_768_CFG.name = 'attention_fgm_768'\nattention_fgm_768_CFG.version = 'custom'\nattention_fgm_768_CFG.config_path = '../input/fb3models/20221120-072218-deberta-v3-base-attention_fgm/config/config.json'\n\nweighted2last_fgm_512_CFG = load_config('../input/fb3models/20221124-060246-deberta-v3-base-weighted2last_fgm')\nweighted2last_fgm_512_CFG.name = 'weighted2last_fgm_512'\nweighted2last_fgm_512_CFG.version = 'custom'\nweighted2last_fgm_512_CFG.config_path = '../input/fb3models/20221124-060246-deberta-v3-base-weighted2last_fgm/config/config.json'\n\nweightedmean2last_fgm_512_CFG = load_config('../input/fb3models/20221124-160318-deberta-v3-base-weightedmean2last_fgm')\nweightedmean2last_fgm_512_CFG.name = 'weightedmean2last_fgm_512'\nweightedmean2last_fgm_512_CFG.version = 'custom'\nweightedmean2last_fgm_512_CFG.config_path = '../input/fb3models/20221124-160318-deberta-v3-base-weightedmean2last_fgm/config/config.json'\n\nroberta_attention_fgm_CFG = load_config('../input/fb3models/20221121-173739-roberta-base')\nroberta_attention_fgm_CFG.name = 'roberta_attention_large_fgm'\nroberta_attention_fgm_CFG.version = 'custom'\nroberta_attention_fgm_CFG.config_path = '../input/fb3models/20221121-173739-roberta-base/config/config.json'\n\n##########\nv112_CFG.pretrained = False\nv114_CFG.pretrained = False\nv116_CFG.pretrained = False\nv21_CFG.pretrained = False\nv2_CFG.pretrained = False\nattention_fgm_CFG.pretrained = False\nweighted_attention_CFG.pretrained = False\nweightedpool_CFG.pretrained = False\nmean_attention_no_fgm_CFG.pretrained=False\nattention_large_fgm_CFG.pretrained=False\nattention_fgm_512_CFG.pretrained = False\nattention_fgm_768_CFG.pretrained = False\nweighted2last_fgm_512_CFG.pretrained = False\nweightedmean2last_fgm_512_CFG.pretrained = False\nroberta_attention_fgm_CFG.pretrained = False\n\nweighted_fgm_CFG.inference_weight = 1.0\nv114_CFG.inference_weight = 1.0 \nv116_CFG.inference_weight = 1.0\nv2_CFG.inference_weight = 1.0 \nv21_CFG.inference_weight = 1.0\nattention_fgm_CFG.inference_weight = 1.0\nweighted_attention_CFG.inference_weight = 1.0\nattention_large_fgm_CFG.inference_weight = 1.0\nattention_fgm_512_CFG.inference_weight = 1.0\nattention_fgm_768_CFG.inference_weight = 1.0\nweighted2last_fgm_512_CFG.inference_weight = 1.0\nweightedmean2last_fgm_512_CFG.inference_weight = 1.0\nroberta_attention_fgm_CFG.inference_weight = 1.0","metadata":{"papermill":{"duration":1.529511,"end_time":"2022-10-27T03:37:55.244731","exception":false,"start_time":"2022-10-27T03:37:53.715220","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-11-28T02:43:24.916959Z","iopub.execute_input":"2022-11-28T02:43:24.917312Z","iopub.status.idle":"2022-11-28T02:43:30.751344Z","shell.execute_reply.started":"2022-11-28T02:43:24.917285Z","shell.execute_reply":"2022-11-28T02:43:30.749736Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"strong_models = [\n    weighted2last_fgm_512_CFG, weightedmean2last_fgm_512_CFG, attention_fgm_512_CFG, attention_fgm_768_CFG, attention_fgm_CFG,\n]\nstrong_weights = [0.23303419, 0.28660832, 0.12286597, 0.26682334, 0.09066818]\n\nweak_models = [\n    v21_CFG, v112_CFG, mean_attention_no_fgm_CFG, weighted_attention_CFG, attention_large_fgm_CFG, roberta_attention_fgm_CFG,\n]\nweak_weights = [0.13595877, 0.20074556, 0.19400446, 0.0293235,0.35083864,0.08912907]","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:30.753302Z","iopub.execute_input":"2022-11-28T02:43:30.754503Z","iopub.status.idle":"2022-11-28T02:43:30.763373Z","shell.execute_reply.started":"2022-11-28T02:43:30.754379Z","shell.execute_reply":"2022-11-28T02:43:30.761707Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# for i, ftm_cfg in enumerate(fine_tuned_models_cfg):\n#     print(ftm_cfg.path, opt_weights[i])\n#     fine_tuned_models_cfg[i].inference_weight = opt_weights[i]","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:30.765407Z","iopub.execute_input":"2022-11-28T02:43:30.766363Z","iopub.status.idle":"2022-11-28T02:43:30.777020Z","shell.execute_reply.started":"2022-11-28T02:43:30.766314Z","shell.execute_reply":"2022-11-28T02:43:30.775442Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def train_stacking(oof_train, learners, save=False):\n    oof_scores = []\n    oof_oof_train = pd.DataFrame()\n    \n    def models_fit_predict(models, X_train, y_train, X_val, y_val, fold, save=False):\n        preds = []\n        for model in models:\n            model_name = type(model).__name__.lower()\n            if type(model) != LinearRegression:\n                model = MultiOutputRegressor(model)\n            model.fit(X_train, y_train)\n            if save:\n                #dump(model, f'{model_name}_strong_fold{fold}.model')\n                dump(model, f'{model_name}_fold{fold}.model')\n            preds.append(model.predict(X_val))\n            #for estimator in model.estimators_:\n            #    print(estimator.coef_)\n        return np.mean(preds, axis=0)\n\n    for fold in range(CFG.n_fold):\n        print(f'\\nFold {fold+1}/{CFG.n_fold}')\n\n        X_train = oof_train[oof_train['fold']!=fold][oof_train.columns[8:]].values\n        #assert X_train.shape[1] == len(fine_tuned_models_cfg)*6 + 6\n        y_train = oof_train[oof_train['fold']!=fold][CFG.target_cols].values\n        X_val = oof_train[oof_train['fold']==fold][oof_train.columns[8:]].values\n        y_val = oof_train[oof_train['fold']==fold][CFG.target_cols].values\n\n        pred_val = models_fit_predict(\n            learners,\n            X_train, y_train, X_val, y_val,\n            fold, save=save,\n        )\n\n        val_fold = oof_train[oof_train['fold']==fold].reset_index(drop=True)\n        val_fold[[f'pred_{c}' for c in CFG.target_cols]] = pred_val\n\n        oof_oof_train = pd.concat([oof_oof_train, val_fold])\n\n        oof_score, _ = mc_rmse(y_val, pred_val)\n        oof_scores.append(oof_score)\n        print(f'Score: {oof_score}')\n        print('#'*50)\n    get_result(CFG, oof_oof_train)\n\nlearners = [\n    Ridge(alpha=48.0, random_state=CFG.seed), \n    #BayesianRidge(),\n    #Lasso(alpha=1.0, random_state=CFG.seed),\n    LinearRegression(normalize=True, positive=True),\n    #SVR(kernel='linear', gamma='auto'),\n    #LGBMRegressor(**lgbm_params)\n]","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:48:42.926111Z","iopub.execute_input":"2022-11-28T02:48:42.926518Z","iopub.status.idle":"2022-11-28T02:48:42.939326Z","shell.execute_reply.started":"2022-11-28T02:48:42.926486Z","shell.execute_reply":"2022-11-28T02:48:42.937922Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"#######################################\ndef get_ensemble_oof(cfgs, return_oof_train=False, verbose=True):\n    oof_dfs = pd.DataFrame()\n    oof_train = pd.DataFrame()\n    total_weight = 0\n    for cfg in cfgs:\n        cfg.target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    \n        infer_ = Inferencer(cfg=cfg, inference_weight=cfg.inference_weight)\n\n        if cfg.path in [attention_fgm_CFG.path, attention_fgm_768_CFG.path, attention_fgm_512_CFG.path, attention_large_fgm_CFG.path, roberta_attention_fgm_CFG.path, weightedmean2last_fgm_512_CFG.path, weighted2last_fgm_512_CFG.path]:\n            file_type = 'csv'\n        else:\n            file_type = 'pkl'\n        oof_df = infer_.get_oof_df(file_type)\n        total_weight += infer_.cfg.inference_weight\n        \n        if verbose:\n            print(cfg.path)\n            get_result(cfg, oof_df)\n            print('\\n')\n\n        pred_cols = [f'pred_{col}' for col in ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']]\n        oof_df_copy = oof_df.copy()\n        oof_df[pred_cols] = oof_df[pred_cols] * infer_.cfg.inference_weight\n        oof_dfs = oof_dfs.append(oof_df)\n\n        oof_df = oof_df_copy.copy()\n        oof_df = oof_df[['text_id'] + pred_cols]\n        oof_df.columns = ['text_id'] + [cfg.name + '_' + col for col in pred_cols]\n\n        if len(oof_train) == 0:\n            oof_train = (\n                train\n                .merge(oof_df, on=['text_id'], how='left')\n                .drop(columns=['full_text']))\n        else:\n            oof_train = (\n                oof_train.merge(oof_df, on=['text_id'], how='left'))\n        del infer_; gc.collect()\n    \n    pred_cols = [f'pred_{col}' for col in ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']]\n    # oof_dfs_mean = oof_dfs.groupby('text_id')[pred_cols].mean()\n    oof_dfs_mean = oof_dfs.groupby('text_id')[pred_cols].sum() / total_weight\n    oof_dfs_mean = oof_dfs_mean.join(train.set_index('text_id'))\n    \n    if return_oof_train:\n        get_result(CFG, oof_dfs_mean, verbose=1)\n        return oof_train\n    return get_result(CFG, oof_dfs_mean, verbose=0), cfgs","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:30.795942Z","iopub.execute_input":"2022-11-28T02:43:30.797601Z","iopub.status.idle":"2022-11-28T02:43:30.814391Z","shell.execute_reply.started":"2022-11-28T02:43:30.797544Z","shell.execute_reply":"2022-11-28T02:43:30.811675Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:30.816796Z","iopub.execute_input":"2022-11-28T02:43:30.817370Z","iopub.status.idle":"2022-11-28T02:43:30.827852Z","shell.execute_reply.started":"2022-11-28T02:43:30.817321Z","shell.execute_reply":"2022-11-28T02:43:30.826880Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## all models","metadata":{}},{"cell_type":"code","source":"all_models = [\n    attention_large_fgm_CFG, weighted2last_fgm_512_CFG, weightedmean2last_fgm_512_CFG, attention_fgm_512_CFG, attention_fgm_768_CFG, \n    v21_CFG, v112_CFG, mean_attention_no_fgm_CFG, v2_CFG, weightedpool_CFG,\n    attention_fgm_CFG\n]\noptimal_weights = [\n    0.353356890459364,\n    0.1660777385159011,\n    0.10247349823321557,\n    0.05653710247349824,\n    0.06007067137809188,\n    0.049469964664310966,\n    0.08127208480565372,\n    0.06713780918727916,\n    0.03886925795053004,\n    0.02473498233215548\n]\n\nall_models_oof_train = get_ensemble_oof(all_models, True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-28T02:43:30.829451Z","iopub.execute_input":"2022-11-28T02:43:30.830544Z","iopub.status.idle":"2022-11-28T02:43:35.999184Z","shell.execute_reply.started":"2022-11-28T02:43:30.830495Z","shell.execute_reply":"2022-11-28T02:43:35.998079Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"../input/fb3models/20221118-164148-deberta-v3-large-attention_fgm\nscore: 0.45206291  scores: [0.48157374223152627, 0.4452090627651757, 0.4154085184288849, 0.4536476489584571, 0.47240178087223106, 0.44413673294015477]\n\n\n../input/fb3models/20221124-060246-deberta-v3-base-weighted2last_fgm\nscore: 0.45303342  scores: [0.48389889660631685, 0.44678944048963176, 0.41465519997691047, 0.45360604538329913, 0.4717536101123449, 0.4474973036716195]\n\n\n../input/fb3models/20221124-160318-deberta-v3-base-weightedmean2last_fgm\nscore: 0.45276573  scores: [0.48130576936482516, 0.4463892153474189, 0.4154102670831803, 0.4533288332620337, 0.471398800994229, 0.4487615040883342]\n\n\n../input/fb3models/20221121-143655-deberta-v3-base-attention_fgm_512\nscore: 0.45415400  scores: [0.48439478507550315, 0.4476619277192037, 0.41633279856096794, 0.4540781761924119, 0.4753848249763217, 0.4470715017077387]\n\n\n../input/fb3models/20221120-072218-deberta-v3-base-attention_fgm\nscore: 0.45289948  scores: [0.4811456435689267, 0.4475128373931811, 0.4152303639507731, 0.4542057256276823, 0.47347977633523913, 0.4458225341021222]\n\n\n../input/fb3models/v21/\nscore: 0.45710246  scores: [0.48798883116384423, 0.44872254666199257, 0.42012263776409287, 0.4555113322262075, 0.4788518700393329, 0.45141752961770576]\n\n\n../input/fb3models/v112/\nscore: 0.45351683  scores: [0.4855633786238422, 0.44703209067484795, 0.41299155171034585, 0.4534606468878697, 0.4717306751532209, 0.4503226293834107]\n\n\n../input/fb3models/20221117-183420-deberta-v3-base-mean-attention-with-mask\nscore: 0.45292423  scores: [0.48674365143324116, 0.44712902084312334, 0.41187184815036604, 0.4543429818169591, 0.4707566701582764, 0.44670122183285366]\n\n\n../input/fb3models/v2/\nscore: 0.45665927  scores: [0.4862241326420343, 0.45220519968314205, 0.41968210267694495, 0.45746617426033787, 0.4755639270504457, 0.44881410849161313]\n\n\n../input/fb3-train/\nscore: 0.46577662  scores: [0.4974350833689818, 0.4560072906674801, 0.42235798291514465, 0.46613323072377133, 0.4908856845317672, 0.46184042368634104]\n\n\n../input/fb3models/20221114-192943-deberta-v3-base/\nscore: 0.45426260  scores: [0.4842567673971252, 0.44804979963987146, 0.4164547697759723, 0.45478343500954116, 0.4747050515151748, 0.4473257540335437]\n\n\nscore: 0.44664384  scores: [0.47603561976901504, 0.44110270980048194, 0.4092970632166753, 0.4479291318325048, 0.46529265795481056, 0.4402058661778941]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### find correlation between models","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans, DBSCAN","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:36.000334Z","iopub.execute_input":"2022-11-28T02:43:36.000845Z","iopub.status.idle":"2022-11-28T02:43:36.085748Z","shell.execute_reply.started":"2022-11-28T02:43:36.000814Z","shell.execute_reply":"2022-11-28T02:43:36.083546Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"all_model_preds = []\nfor model in all_models:\n    model_preds = []\n    model_name = model.name\n    print(model_name)\n    for target in target_cols:\n        model_target_pred = all_models_oof_train[f'{model_name}_pred_{target}'].values.reshape(1,-1)\n        model_preds.extend(model_target_pred)\n    model_preds = np.array(model_preds).reshape(1, -1)\n    all_model_preds.append(model_preds)\nall_model_preds = np.concatenate(all_model_preds)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:36.087668Z","iopub.execute_input":"2022-11-28T02:43:36.088360Z","iopub.status.idle":"2022-11-28T02:43:36.101604Z","shell.execute_reply.started":"2022-11-28T02:43:36.088323Z","shell.execute_reply":"2022-11-28T02:43:36.100102Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"attention_large_fgm\nweighted2last_fgm_512\nweightedmean2last_fgm_512\nattention_fgm_512\nattention_fgm_768\nv21\nv112\nmean_attention_no_fgm\nv2\nweightedpool\nattention_fgm\n","output_type":"stream"}]},{"cell_type":"code","source":"dbs = DBSCAN(eps=2048, min_samples=2, metric='l1')\ndbs_clusters = dbs.fit_predict(all_model_preds)\ndbs_clusters","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:36.103537Z","iopub.execute_input":"2022-11-28T02:43:36.104049Z","iopub.status.idle":"2022-11-28T02:43:36.116199Z","shell.execute_reply.started":"2022-11-28T02:43:36.104012Z","shell.execute_reply":"2022-11-28T02:43:36.114744Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"array([-1,  0,  0,  0,  0, -1, -1, -1, -1, -1,  0])"},"metadata":{}}]},{"cell_type":"code","source":"all_models = [\n    attention_large_fgm_CFG, weighted2last_fgm_512_CFG, weightedmean2last_fgm_512_CFG, attention_fgm_512_CFG, attention_fgm_768_CFG, \n    v21_CFG, v112_CFG, mean_attention_no_fgm_CFG, v2_CFG, weightedpool_CFG,\n    attention_fgm_CFG\n]","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:43:36.118465Z","iopub.execute_input":"2022-11-28T02:43:36.118881Z","iopub.status.idle":"2022-11-28T02:43:36.123920Z","shell.execute_reply.started":"2022-11-28T02:43:36.118847Z","shell.execute_reply":"2022-11-28T02:43:36.122643Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity, manhattan_distances\n\nsim_mat = {}\nfor model1 in all_models:\n    model1_name = model1.name\n    sim_mat[model1_name] = {}\n    for model2 in all_models:\n        sims = []\n        model2_name = model2.name\n        if model2_name == model1_name: continue\n        for target in target_cols:\n            model1_target_pred = all_models_oof_train[f'{model1_name}_pred_{target}'].values.reshape(1,-1)\n            model2_target_pred = all_models_oof_train[f'{model2_name}_pred_{target}'].values.reshape(1,-1)\n            sim = manhattan_distances(model1_target_pred, model2_target_pred)\n            print(f'Cosine similarity between {model1_name} and {model2_name} on {target}: {sim}')\n            sims.append(sim)\n        model1_model2_sim = np.mean(sims)\n        sim_mat[model1_name][model2_name] = model1_model2_sim\n        print(f'Mean cosine similarity between {model1_name} and {model2_name}: {model1_model2_sim:.5f}')\n        print()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-28T02:43:36.125725Z","iopub.execute_input":"2022-11-28T02:43:36.126100Z","iopub.status.idle":"2022-11-28T02:43:36.310209Z","shell.execute_reply.started":"2022-11-28T02:43:36.126067Z","shell.execute_reply":"2022-11-28T02:43:36.307971Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Cosine similarity between attention_large_fgm and weighted2last_fgm_512 on cohesion: [[447.49531734]]\nCosine similarity between attention_large_fgm and weighted2last_fgm_512 on syntax: [[373.54864633]]\nCosine similarity between attention_large_fgm and weighted2last_fgm_512 on vocabulary: [[376.58544242]]\nCosine similarity between attention_large_fgm and weighted2last_fgm_512 on phraseology: [[389.67531335]]\nCosine similarity between attention_large_fgm and weighted2last_fgm_512 on grammar: [[443.73869109]]\nCosine similarity between attention_large_fgm and weighted2last_fgm_512 on conventions: [[424.68960422]]\nMean cosine similarity between attention_large_fgm and weighted2last_fgm_512: 409.28884\n\nCosine similarity between attention_large_fgm and weightedmean2last_fgm_512 on cohesion: [[431.09817967]]\nCosine similarity between attention_large_fgm and weightedmean2last_fgm_512 on syntax: [[381.97617685]]\nCosine similarity between attention_large_fgm and weightedmean2last_fgm_512 on vocabulary: [[375.2385701]]\nCosine similarity between attention_large_fgm and weightedmean2last_fgm_512 on phraseology: [[385.58970075]]\nCosine similarity between attention_large_fgm and weightedmean2last_fgm_512 on grammar: [[449.47791985]]\nCosine similarity between attention_large_fgm and weightedmean2last_fgm_512 on conventions: [[443.85132583]]\nMean cosine similarity between attention_large_fgm and weightedmean2last_fgm_512: 411.20531\n\nCosine similarity between attention_large_fgm and attention_fgm_512 on cohesion: [[451.78031279]]\nCosine similarity between attention_large_fgm and attention_fgm_512 on syntax: [[376.87358459]]\nCosine similarity between attention_large_fgm and attention_fgm_512 on vocabulary: [[358.31612663]]\nCosine similarity between attention_large_fgm and attention_fgm_512 on phraseology: [[381.50093623]]\nCosine similarity between attention_large_fgm and attention_fgm_512 on grammar: [[453.15132568]]\nCosine similarity between attention_large_fgm and attention_fgm_512 on conventions: [[408.14408418]]\nMean cosine similarity between attention_large_fgm and attention_fgm_512: 404.96106\n\nCosine similarity between attention_large_fgm and attention_fgm_768 on cohesion: [[430.16024163]]\nCosine similarity between attention_large_fgm and attention_fgm_768 on syntax: [[348.79580331]]\nCosine similarity between attention_large_fgm and attention_fgm_768 on vocabulary: [[341.81973577]]\nCosine similarity between attention_large_fgm and attention_fgm_768 on phraseology: [[373.44952929]]\nCosine similarity between attention_large_fgm and attention_fgm_768 on grammar: [[430.62732279]]\nCosine similarity between attention_large_fgm and attention_fgm_768 on conventions: [[388.14189929]]\nMean cosine similarity between attention_large_fgm and attention_fgm_768: 385.49909\n\nCosine similarity between attention_large_fgm and v21 on cohesion: [[492.71623254]]\nCosine similarity between attention_large_fgm and v21 on syntax: [[403.07385111]]\nCosine similarity between attention_large_fgm and v21 on vocabulary: [[411.07747233]]\nCosine similarity between attention_large_fgm and v21 on phraseology: [[414.86408687]]\nCosine similarity between attention_large_fgm and v21 on grammar: [[521.31448364]]\nCosine similarity between attention_large_fgm and v21 on conventions: [[468.81555235]]\nMean cosine similarity between attention_large_fgm and v21: 451.97695\n\nCosine similarity between attention_large_fgm and v112 on cohesion: [[468.33740032]]\nCosine similarity between attention_large_fgm and v112 on syntax: [[375.12523425]]\nCosine similarity between attention_large_fgm and v112 on vocabulary: [[363.21144438]]\nCosine similarity between attention_large_fgm and v112 on phraseology: [[381.79164219]]\nCosine similarity between attention_large_fgm and v112 on grammar: [[458.69301915]]\nCosine similarity between attention_large_fgm and v112 on conventions: [[435.81001151]]\nMean cosine similarity between attention_large_fgm and v112: 413.82813\n\nCosine similarity between attention_large_fgm and mean_attention_no_fgm on cohesion: [[460.44029999]]\nCosine similarity between attention_large_fgm and mean_attention_no_fgm on syntax: [[361.44003057]]\nCosine similarity between attention_large_fgm and mean_attention_no_fgm on vocabulary: [[339.70785499]]\nCosine similarity between attention_large_fgm and mean_attention_no_fgm on phraseology: [[373.92420447]]\nCosine similarity between attention_large_fgm and mean_attention_no_fgm on grammar: [[417.76399684]]\nCosine similarity between attention_large_fgm and mean_attention_no_fgm on conventions: [[420.78119469]]\nMean cosine similarity between attention_large_fgm and mean_attention_no_fgm: 395.67626\n\nCosine similarity between attention_large_fgm and v2 on cohesion: [[429.29163957]]\nCosine similarity between attention_large_fgm and v2 on syntax: [[409.16578889]]\nCosine similarity between attention_large_fgm and v2 on vocabulary: [[391.25832832]]\nCosine similarity between attention_large_fgm and v2 on phraseology: [[411.33138061]]\nCosine similarity between attention_large_fgm and v2 on grammar: [[482.35822415]]\nCosine similarity between attention_large_fgm and v2 on conventions: [[431.19491494]]\nMean cosine similarity between attention_large_fgm and v2: 425.76671\n\nCosine similarity between attention_large_fgm and weightedpool on cohesion: [[571.59756863]]\nCosine similarity between attention_large_fgm and weightedpool on syntax: [[471.78197587]]\nCosine similarity between attention_large_fgm and weightedpool on vocabulary: [[448.18218958]]\nCosine similarity between attention_large_fgm and weightedpool on phraseology: [[529.43094695]]\nCosine similarity between attention_large_fgm and weightedpool on grammar: [[639.50539243]]\nCosine similarity between attention_large_fgm and weightedpool on conventions: [[584.6061455]]\nMean cosine similarity between attention_large_fgm and weightedpool: 540.85070\n\nCosine similarity between attention_large_fgm and attention_fgm on cohesion: [[410.15437609]]\nCosine similarity between attention_large_fgm and attention_fgm on syntax: [[353.95186162]]\nCosine similarity between attention_large_fgm and attention_fgm on vocabulary: [[336.9159708]]\nCosine similarity between attention_large_fgm and attention_fgm on phraseology: [[362.97130787]]\nCosine similarity between attention_large_fgm and attention_fgm on grammar: [[424.88914764]]\nCosine similarity between attention_large_fgm and attention_fgm on conventions: [[384.57606483]]\nMean cosine similarity between attention_large_fgm and attention_fgm: 378.90979\n\nCosine similarity between weighted2last_fgm_512 and attention_large_fgm on cohesion: [[447.49531734]]\nCosine similarity between weighted2last_fgm_512 and attention_large_fgm on syntax: [[373.54864633]]\nCosine similarity between weighted2last_fgm_512 and attention_large_fgm on vocabulary: [[376.58544242]]\nCosine similarity between weighted2last_fgm_512 and attention_large_fgm on phraseology: [[389.67531335]]\nCosine similarity between weighted2last_fgm_512 and attention_large_fgm on grammar: [[443.73869109]]\nCosine similarity between weighted2last_fgm_512 and attention_large_fgm on conventions: [[424.68960422]]\nMean cosine similarity between weighted2last_fgm_512 and attention_large_fgm: 409.28884\n\nCosine similarity between weighted2last_fgm_512 and weightedmean2last_fgm_512 on cohesion: [[242.85016913]]\nCosine similarity between weighted2last_fgm_512 and weightedmean2last_fgm_512 on syntax: [[203.5209236]]\nCosine similarity between weighted2last_fgm_512 and weightedmean2last_fgm_512 on vocabulary: [[228.305938]]\nCosine similarity between weighted2last_fgm_512 and weightedmean2last_fgm_512 on phraseology: [[204.02267317]]\nCosine similarity between weighted2last_fgm_512 and weightedmean2last_fgm_512 on grammar: [[218.58504908]]\nCosine similarity between weighted2last_fgm_512 and weightedmean2last_fgm_512 on conventions: [[224.53337182]]\nMean cosine similarity between weighted2last_fgm_512 and weightedmean2last_fgm_512: 220.30302\n\nCosine similarity between weighted2last_fgm_512 and attention_fgm_512 on cohesion: [[320.87038176]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm_512 on syntax: [[279.2119292]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm_512 on vocabulary: [[265.26597689]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm_512 on phraseology: [[263.38463305]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm_512 on grammar: [[296.56557288]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm_512 on conventions: [[291.36264059]]\nMean cosine similarity between weighted2last_fgm_512 and attention_fgm_512: 286.11019\n\nCosine similarity between weighted2last_fgm_512 and attention_fgm_768 on cohesion: [[317.46806136]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm_768 on syntax: [[287.07531178]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm_768 on vocabulary: [[268.07163775]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm_768 on phraseology: [[284.86794114]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm_768 on grammar: [[322.38722789]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm_768 on conventions: [[288.80145955]]\nMean cosine similarity between weighted2last_fgm_512 and attention_fgm_768: 294.77861\n\nCosine similarity between weighted2last_fgm_512 and v21 on cohesion: [[385.27085483]]\nCosine similarity between weighted2last_fgm_512 and v21 on syntax: [[322.9173454]]\nCosine similarity between weighted2last_fgm_512 and v21 on vocabulary: [[347.22522378]]\nCosine similarity between weighted2last_fgm_512 and v21 on phraseology: [[328.04329336]]\nCosine similarity between weighted2last_fgm_512 and v21 on grammar: [[413.52580237]]\nCosine similarity between weighted2last_fgm_512 and v21 on conventions: [[345.68701845]]\nMean cosine similarity between weighted2last_fgm_512 and v21: 357.11159\n\nCosine similarity between weighted2last_fgm_512 and v112 on cohesion: [[446.33012748]]\nCosine similarity between weighted2last_fgm_512 and v112 on syntax: [[338.62730145]]\nCosine similarity between weighted2last_fgm_512 and v112 on vocabulary: [[331.01030242]]\nCosine similarity between weighted2last_fgm_512 and v112 on phraseology: [[341.313146]]\nCosine similarity between weighted2last_fgm_512 and v112 on grammar: [[382.15691018]]\nCosine similarity between weighted2last_fgm_512 and v112 on conventions: [[394.39296514]]\nMean cosine similarity between weighted2last_fgm_512 and v112: 372.30513\n\nCosine similarity between weighted2last_fgm_512 and mean_attention_no_fgm on cohesion: [[439.46418917]]\nCosine similarity between weighted2last_fgm_512 and mean_attention_no_fgm on syntax: [[336.00379527]]\nCosine similarity between weighted2last_fgm_512 and mean_attention_no_fgm on vocabulary: [[330.42085326]]\nCosine similarity between weighted2last_fgm_512 and mean_attention_no_fgm on phraseology: [[364.87117386]]\nCosine similarity between weighted2last_fgm_512 and mean_attention_no_fgm on grammar: [[358.16683555]]\nCosine similarity between weighted2last_fgm_512 and mean_attention_no_fgm on conventions: [[389.52317995]]\nMean cosine similarity between weighted2last_fgm_512 and mean_attention_no_fgm: 369.74167\n\nCosine similarity between weighted2last_fgm_512 and v2 on cohesion: [[438.15707958]]\nCosine similarity between weighted2last_fgm_512 and v2 on syntax: [[401.08843219]]\nCosine similarity between weighted2last_fgm_512 and v2 on vocabulary: [[397.52591586]]\nCosine similarity between weighted2last_fgm_512 and v2 on phraseology: [[400.5723604]]\nCosine similarity between weighted2last_fgm_512 and v2 on grammar: [[436.3241601]]\nCosine similarity between weighted2last_fgm_512 and v2 on conventions: [[396.40660316]]\nMean cosine similarity between weighted2last_fgm_512 and v2: 411.67909\n\nCosine similarity between weighted2last_fgm_512 and weightedpool on cohesion: [[546.56103539]]\nCosine similarity between weighted2last_fgm_512 and weightedpool on syntax: [[448.18063331]]\nCosine similarity between weighted2last_fgm_512 and weightedpool on vocabulary: [[403.31440926]]\nCosine similarity between weighted2last_fgm_512 and weightedpool on phraseology: [[480.45081711]]\nCosine similarity between weighted2last_fgm_512 and weightedpool on grammar: [[578.18615496]]\nCosine similarity between weighted2last_fgm_512 and weightedpool on conventions: [[525.39857775]]\nMean cosine similarity between weighted2last_fgm_512 and weightedpool: 497.01527\n\nCosine similarity between weighted2last_fgm_512 and attention_fgm on cohesion: [[334.64357775]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm on syntax: [[295.71736562]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm on vocabulary: [[290.73094428]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm on phraseology: [[299.0470984]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm on grammar: [[327.01792967]]\nCosine similarity between weighted2last_fgm_512 and attention_fgm on conventions: [[312.89411396]]\nMean cosine similarity between weighted2last_fgm_512 and attention_fgm: 310.00850\n\nCosine similarity between weightedmean2last_fgm_512 and attention_large_fgm on cohesion: [[431.09817967]]\nCosine similarity between weightedmean2last_fgm_512 and attention_large_fgm on syntax: [[381.97617685]]\nCosine similarity between weightedmean2last_fgm_512 and attention_large_fgm on vocabulary: [[375.2385701]]\nCosine similarity between weightedmean2last_fgm_512 and attention_large_fgm on phraseology: [[385.58970075]]\nCosine similarity between weightedmean2last_fgm_512 and attention_large_fgm on grammar: [[449.47791985]]\nCosine similarity between weightedmean2last_fgm_512 and attention_large_fgm on conventions: [[443.85132583]]\nMean cosine similarity between weightedmean2last_fgm_512 and attention_large_fgm: 411.20531\n\nCosine similarity between weightedmean2last_fgm_512 and weighted2last_fgm_512 on cohesion: [[242.85016913]]\nCosine similarity between weightedmean2last_fgm_512 and weighted2last_fgm_512 on syntax: [[203.5209236]]\nCosine similarity between weightedmean2last_fgm_512 and weighted2last_fgm_512 on vocabulary: [[228.305938]]\nCosine similarity between weightedmean2last_fgm_512 and weighted2last_fgm_512 on phraseology: [[204.02267317]]\nCosine similarity between weightedmean2last_fgm_512 and weighted2last_fgm_512 on grammar: [[218.58504908]]\nCosine similarity between weightedmean2last_fgm_512 and weighted2last_fgm_512 on conventions: [[224.53337182]]\nMean cosine similarity between weightedmean2last_fgm_512 and weighted2last_fgm_512: 220.30302\n\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_512 on cohesion: [[330.4473929]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_512 on syntax: [[295.3425464]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_512 on vocabulary: [[247.3155232]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_512 on phraseology: [[266.5617407]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_512 on grammar: [[319.9865646]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_512 on conventions: [[301.94495636]]\nMean cosine similarity between weightedmean2last_fgm_512 and attention_fgm_512: 293.59979\n\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_768 on cohesion: [[313.47398242]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_768 on syntax: [[303.91942424]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_768 on vocabulary: [[255.46728996]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_768 on phraseology: [[286.01142785]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_768 on grammar: [[337.48039118]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm_768 on conventions: [[304.31711541]]\nMean cosine similarity between weightedmean2last_fgm_512 and attention_fgm_768: 300.11161\n\nCosine similarity between weightedmean2last_fgm_512 and v21 on cohesion: [[403.36686204]]\nCosine similarity between weightedmean2last_fgm_512 and v21 on syntax: [[347.65009839]]\nCosine similarity between weightedmean2last_fgm_512 and v21 on vocabulary: [[356.97157916]]\nCosine similarity between weightedmean2last_fgm_512 and v21 on phraseology: [[331.05185615]]\nCosine similarity between weightedmean2last_fgm_512 and v21 on grammar: [[421.21448031]]\nCosine similarity between weightedmean2last_fgm_512 and v21 on conventions: [[346.77965347]]\nMean cosine similarity between weightedmean2last_fgm_512 and v21: 367.83909\n\nCosine similarity between weightedmean2last_fgm_512 and v112 on cohesion: [[420.72122504]]\nCosine similarity between weightedmean2last_fgm_512 and v112 on syntax: [[350.24174859]]\nCosine similarity between weightedmean2last_fgm_512 and v112 on vocabulary: [[320.95868628]]\nCosine similarity between weightedmean2last_fgm_512 and v112 on phraseology: [[342.96524851]]\nCosine similarity between weightedmean2last_fgm_512 and v112 on grammar: [[369.50376917]]\nCosine similarity between weightedmean2last_fgm_512 and v112 on conventions: [[402.33042933]]\nMean cosine similarity between weightedmean2last_fgm_512 and v112: 367.78685\n\nCosine similarity between weightedmean2last_fgm_512 and mean_attention_no_fgm on cohesion: [[430.89880933]]\nCosine similarity between weightedmean2last_fgm_512 and mean_attention_no_fgm on syntax: [[344.65689249]]\nCosine similarity between weightedmean2last_fgm_512 and mean_attention_no_fgm on vocabulary: [[321.29209848]]\nCosine similarity between weightedmean2last_fgm_512 and mean_attention_no_fgm on phraseology: [[353.12425404]]\nCosine similarity between weightedmean2last_fgm_512 and mean_attention_no_fgm on grammar: [[368.57186492]]\nCosine similarity between weightedmean2last_fgm_512 and mean_attention_no_fgm on conventions: [[397.75342942]]\nMean cosine similarity between weightedmean2last_fgm_512 and mean_attention_no_fgm: 369.38289\n\nCosine similarity between weightedmean2last_fgm_512 and v2 on cohesion: [[416.88494006]]\nCosine similarity between weightedmean2last_fgm_512 and v2 on syntax: [[402.13383804]]\nCosine similarity between weightedmean2last_fgm_512 and v2 on vocabulary: [[376.92115127]]\nCosine similarity between weightedmean2last_fgm_512 and v2 on phraseology: [[393.69886639]]\nCosine similarity between weightedmean2last_fgm_512 and v2 on grammar: [[408.32441863]]\nCosine similarity between weightedmean2last_fgm_512 and v2 on conventions: [[391.56587083]]\nMean cosine similarity between weightedmean2last_fgm_512 and v2: 398.25485\n\nCosine similarity between weightedmean2last_fgm_512 and weightedpool on cohesion: [[532.63622247]]\nCosine similarity between weightedmean2last_fgm_512 and weightedpool on syntax: [[449.34850336]]\nCosine similarity between weightedmean2last_fgm_512 and weightedpool on vocabulary: [[406.52901536]]\nCosine similarity between weightedmean2last_fgm_512 and weightedpool on phraseology: [[478.45703673]]\nCosine similarity between weightedmean2last_fgm_512 and weightedpool on grammar: [[569.41970963]]\nCosine similarity between weightedmean2last_fgm_512 and weightedpool on conventions: [[516.97919159]]\nMean cosine similarity between weightedmean2last_fgm_512 and weightedpool: 492.22828\n\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm on cohesion: [[326.60734709]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm on syntax: [[305.36618488]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm on vocabulary: [[280.02462064]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm on phraseology: [[295.49237678]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm on grammar: [[343.4280082]]\nCosine similarity between weightedmean2last_fgm_512 and attention_fgm on conventions: [[315.85901001]]\nMean cosine similarity between weightedmean2last_fgm_512 and attention_fgm: 311.12959\n\nCosine similarity between attention_fgm_512 and attention_large_fgm on cohesion: [[451.78031279]]\nCosine similarity between attention_fgm_512 and attention_large_fgm on syntax: [[376.87358459]]\nCosine similarity between attention_fgm_512 and attention_large_fgm on vocabulary: [[358.31612663]]\nCosine similarity between attention_fgm_512 and attention_large_fgm on phraseology: [[381.50093623]]\nCosine similarity between attention_fgm_512 and attention_large_fgm on grammar: [[453.15132568]]\nCosine similarity between attention_fgm_512 and attention_large_fgm on conventions: [[408.14408418]]\nMean cosine similarity between attention_fgm_512 and attention_large_fgm: 404.96106\n\nCosine similarity between attention_fgm_512 and weighted2last_fgm_512 on cohesion: [[320.87038176]]\nCosine similarity between attention_fgm_512 and weighted2last_fgm_512 on syntax: [[279.2119292]]\nCosine similarity between attention_fgm_512 and weighted2last_fgm_512 on vocabulary: [[265.26597689]]\nCosine similarity between attention_fgm_512 and weighted2last_fgm_512 on phraseology: [[263.38463305]]\nCosine similarity between attention_fgm_512 and weighted2last_fgm_512 on grammar: [[296.56557288]]\nCosine similarity between attention_fgm_512 and weighted2last_fgm_512 on conventions: [[291.36264059]]\nMean cosine similarity between attention_fgm_512 and weighted2last_fgm_512: 286.11019\n\nCosine similarity between attention_fgm_512 and weightedmean2last_fgm_512 on cohesion: [[330.4473929]]\nCosine similarity between attention_fgm_512 and weightedmean2last_fgm_512 on syntax: [[295.3425464]]\nCosine similarity between attention_fgm_512 and weightedmean2last_fgm_512 on vocabulary: [[247.3155232]]\nCosine similarity between attention_fgm_512 and weightedmean2last_fgm_512 on phraseology: [[266.5617407]]\nCosine similarity between attention_fgm_512 and weightedmean2last_fgm_512 on grammar: [[319.9865646]]\nCosine similarity between attention_fgm_512 and weightedmean2last_fgm_512 on conventions: [[301.94495636]]\nMean cosine similarity between attention_fgm_512 and weightedmean2last_fgm_512: 293.59979\n\nCosine similarity between attention_fgm_512 and attention_fgm_768 on cohesion: [[294.1925925]]\nCosine similarity between attention_fgm_512 and attention_fgm_768 on syntax: [[256.7684589]]\nCosine similarity between attention_fgm_512 and attention_fgm_768 on vocabulary: [[211.72684272]]\nCosine similarity between attention_fgm_512 and attention_fgm_768 on phraseology: [[228.01983388]]\nCosine similarity between attention_fgm_512 and attention_fgm_768 on grammar: [[265.52440476]]\nCosine similarity between attention_fgm_512 and attention_fgm_768 on conventions: [[248.76703748]]\nMean cosine similarity between attention_fgm_512 and attention_fgm_768: 250.83320\n\nCosine similarity between attention_fgm_512 and v21 on cohesion: [[465.42311806]]\nCosine similarity between attention_fgm_512 and v21 on syntax: [[368.44966801]]\nCosine similarity between attention_fgm_512 and v21 on vocabulary: [[359.46566979]]\nCosine similarity between attention_fgm_512 and v21 on phraseology: [[375.36248112]]\nCosine similarity between attention_fgm_512 and v21 on grammar: [[466.81367864]]\nCosine similarity between attention_fgm_512 and v21 on conventions: [[401.06072469]]\nMean cosine similarity between attention_fgm_512 and v21: 406.09589\n\nCosine similarity between attention_fgm_512 and v112 on cohesion: [[474.8025987]]\nCosine similarity between attention_fgm_512 and v112 on syntax: [[344.54754651]]\nCosine similarity between attention_fgm_512 and v112 on vocabulary: [[322.26666641]]\nCosine similarity between attention_fgm_512 and v112 on phraseology: [[342.8398713]]\nCosine similarity between attention_fgm_512 and v112 on grammar: [[418.40966657]]\nCosine similarity between attention_fgm_512 and v112 on conventions: [[404.68748128]]\nMean cosine similarity between attention_fgm_512 and v112: 384.59231\n\nCosine similarity between attention_fgm_512 and mean_attention_no_fgm on cohesion: [[450.65884366]]\nCosine similarity between attention_fgm_512 and mean_attention_no_fgm on syntax: [[353.67427656]]\nCosine similarity between attention_fgm_512 and mean_attention_no_fgm on vocabulary: [[312.75000421]]\nCosine similarity between attention_fgm_512 and mean_attention_no_fgm on phraseology: [[352.33482768]]\nCosine similarity between attention_fgm_512 and mean_attention_no_fgm on grammar: [[370.43765572]]\nCosine similarity between attention_fgm_512 and mean_attention_no_fgm on conventions: [[393.13700246]]\nMean cosine similarity between attention_fgm_512 and mean_attention_no_fgm: 372.16544\n\nCosine similarity between attention_fgm_512 and v2 on cohesion: [[492.70282446]]\nCosine similarity between attention_fgm_512 and v2 on syntax: [[400.31939891]]\nCosine similarity between attention_fgm_512 and v2 on vocabulary: [[393.21264968]]\nCosine similarity between attention_fgm_512 and v2 on phraseology: [[411.17021969]]\nCosine similarity between attention_fgm_512 and v2 on grammar: [[490.28848487]]\nCosine similarity between attention_fgm_512 and v2 on conventions: [[423.14658055]]\nMean cosine similarity between attention_fgm_512 and v2: 435.14003\n\nCosine similarity between attention_fgm_512 and weightedpool on cohesion: [[560.9723899]]\nCosine similarity between attention_fgm_512 and weightedpool on syntax: [[474.62304121]]\nCosine similarity between attention_fgm_512 and weightedpool on vocabulary: [[413.30405742]]\nCosine similarity between attention_fgm_512 and weightedpool on phraseology: [[487.615989]]\nCosine similarity between attention_fgm_512 and weightedpool on grammar: [[610.25309245]]\nCosine similarity between attention_fgm_512 and weightedpool on conventions: [[538.0705499]]\nMean cosine similarity between attention_fgm_512 and weightedpool: 514.13985\n\nCosine similarity between attention_fgm_512 and attention_fgm on cohesion: [[250.96339653]]\nCosine similarity between attention_fgm_512 and attention_fgm on syntax: [[207.99832564]]\nCosine similarity between attention_fgm_512 and attention_fgm on vocabulary: [[185.81710273]]\nCosine similarity between attention_fgm_512 and attention_fgm on phraseology: [[190.10409123]]\nCosine similarity between attention_fgm_512 and attention_fgm on grammar: [[212.52178816]]\nCosine similarity between attention_fgm_512 and attention_fgm on conventions: [[198.32500548]]\nMean cosine similarity between attention_fgm_512 and attention_fgm: 207.62162\n\nCosine similarity between attention_fgm_768 and attention_large_fgm on cohesion: [[430.16024163]]\nCosine similarity between attention_fgm_768 and attention_large_fgm on syntax: [[348.79580331]]\nCosine similarity between attention_fgm_768 and attention_large_fgm on vocabulary: [[341.81973577]]\nCosine similarity between attention_fgm_768 and attention_large_fgm on phraseology: [[373.44952929]]\nCosine similarity between attention_fgm_768 and attention_large_fgm on grammar: [[430.62732279]]\nCosine similarity between attention_fgm_768 and attention_large_fgm on conventions: [[388.14189929]]\nMean cosine similarity between attention_fgm_768 and attention_large_fgm: 385.49909\n\nCosine similarity between attention_fgm_768 and weighted2last_fgm_512 on cohesion: [[317.46806136]]\nCosine similarity between attention_fgm_768 and weighted2last_fgm_512 on syntax: [[287.07531178]]\nCosine similarity between attention_fgm_768 and weighted2last_fgm_512 on vocabulary: [[268.07163775]]\nCosine similarity between attention_fgm_768 and weighted2last_fgm_512 on phraseology: [[284.86794114]]\nCosine similarity between attention_fgm_768 and weighted2last_fgm_512 on grammar: [[322.38722789]]\nCosine similarity between attention_fgm_768 and weighted2last_fgm_512 on conventions: [[288.80145955]]\nMean cosine similarity between attention_fgm_768 and weighted2last_fgm_512: 294.77861\n\nCosine similarity between attention_fgm_768 and weightedmean2last_fgm_512 on cohesion: [[313.47398242]]\nCosine similarity between attention_fgm_768 and weightedmean2last_fgm_512 on syntax: [[303.91942424]]\nCosine similarity between attention_fgm_768 and weightedmean2last_fgm_512 on vocabulary: [[255.46728996]]\nCosine similarity between attention_fgm_768 and weightedmean2last_fgm_512 on phraseology: [[286.01142785]]\nCosine similarity between attention_fgm_768 and weightedmean2last_fgm_512 on grammar: [[337.48039118]]\nCosine similarity between attention_fgm_768 and weightedmean2last_fgm_512 on conventions: [[304.31711541]]\nMean cosine similarity between attention_fgm_768 and weightedmean2last_fgm_512: 300.11161\n\nCosine similarity between attention_fgm_768 and attention_fgm_512 on cohesion: [[294.1925925]]\nCosine similarity between attention_fgm_768 and attention_fgm_512 on syntax: [[256.7684589]]\nCosine similarity between attention_fgm_768 and attention_fgm_512 on vocabulary: [[211.72684272]]\nCosine similarity between attention_fgm_768 and attention_fgm_512 on phraseology: [[228.01983388]]\nCosine similarity between attention_fgm_768 and attention_fgm_512 on grammar: [[265.52440476]]\nCosine similarity between attention_fgm_768 and attention_fgm_512 on conventions: [[248.76703748]]\nMean cosine similarity between attention_fgm_768 and attention_fgm_512: 250.83320\n\nCosine similarity between attention_fgm_768 and v21 on cohesion: [[405.24630049]]\nCosine similarity between attention_fgm_768 and v21 on syntax: [[337.86643171]]\nCosine similarity between attention_fgm_768 and v21 on vocabulary: [[336.76271522]]\nCosine similarity between attention_fgm_768 and v21 on phraseology: [[350.44583404]]\nCosine similarity between attention_fgm_768 and v21 on grammar: [[447.89839041]]\nCosine similarity between attention_fgm_768 and v21 on conventions: [[345.42944819]]\nMean cosine similarity between attention_fgm_768 and v21: 370.60819\n\nCosine similarity between attention_fgm_768 and v112 on cohesion: [[432.04760662]]\nCosine similarity between attention_fgm_768 and v112 on syntax: [[316.973225]]\nCosine similarity between attention_fgm_768 and v112 on vocabulary: [[294.60690141]]\nCosine similarity between attention_fgm_768 and v112 on phraseology: [[331.34592044]]\nCosine similarity between attention_fgm_768 and v112 on grammar: [[382.68680084]]\nCosine similarity between attention_fgm_768 and v112 on conventions: [[371.76662904]]\nMean cosine similarity between attention_fgm_768 and v112: 354.90451\n\nCosine similarity between attention_fgm_768 and mean_attention_no_fgm on cohesion: [[424.58608773]]\nCosine similarity between attention_fgm_768 and mean_attention_no_fgm on syntax: [[320.25539303]]\nCosine similarity between attention_fgm_768 and mean_attention_no_fgm on vocabulary: [[293.5669117]]\nCosine similarity between attention_fgm_768 and mean_attention_no_fgm on phraseology: [[335.41470981]]\nCosine similarity between attention_fgm_768 and mean_attention_no_fgm on grammar: [[353.3158499]]\nCosine similarity between attention_fgm_768 and mean_attention_no_fgm on conventions: [[363.71743554]]\nMean cosine similarity between attention_fgm_768 and mean_attention_no_fgm: 348.47606\n\nCosine similarity between attention_fgm_768 and v2 on cohesion: [[452.81750587]]\nCosine similarity between attention_fgm_768 and v2 on syntax: [[399.13067603]]\nCosine similarity between attention_fgm_768 and v2 on vocabulary: [[385.26101649]]\nCosine similarity between attention_fgm_768 and v2 on phraseology: [[412.28403437]]\nCosine similarity between attention_fgm_768 and v2 on grammar: [[469.38681209]]\nCosine similarity between attention_fgm_768 and v2 on conventions: [[394.15101606]]\nMean cosine similarity between attention_fgm_768 and v2: 418.83851\n\nCosine similarity between attention_fgm_768 and weightedpool on cohesion: [[533.74748579]]\nCosine similarity between attention_fgm_768 and weightedpool on syntax: [[453.40404713]]\nCosine similarity between attention_fgm_768 and weightedpool on vocabulary: [[385.99938953]]\nCosine similarity between attention_fgm_768 and weightedpool on phraseology: [[486.45818782]]\nCosine similarity between attention_fgm_768 and weightedpool on grammar: [[590.07859254]]\nCosine similarity between attention_fgm_768 and weightedpool on conventions: [[507.98608142]]\nMean cosine similarity between attention_fgm_768 and weightedpool: 492.94563\n\nCosine similarity between attention_fgm_768 and attention_fgm on cohesion: [[196.7452893]]\nCosine similarity between attention_fgm_768 and attention_fgm on syntax: [[179.19097114]]\nCosine similarity between attention_fgm_768 and attention_fgm on vocabulary: [[165.23687506]]\nCosine similarity between attention_fgm_768 and attention_fgm on phraseology: [[182.90750647]]\nCosine similarity between attention_fgm_768 and attention_fgm on grammar: [[205.53425121]]\nCosine similarity between attention_fgm_768 and attention_fgm on conventions: [[178.69447774]]\nMean cosine similarity between attention_fgm_768 and attention_fgm: 184.71823\n\nCosine similarity between v21 and attention_large_fgm on cohesion: [[492.71623254]]\nCosine similarity between v21 and attention_large_fgm on syntax: [[403.07385111]]\nCosine similarity between v21 and attention_large_fgm on vocabulary: [[411.07747233]]\nCosine similarity between v21 and attention_large_fgm on phraseology: [[414.86408687]]\nCosine similarity between v21 and attention_large_fgm on grammar: [[521.31448364]]\nCosine similarity between v21 and attention_large_fgm on conventions: [[468.81555235]]\nMean cosine similarity between v21 and attention_large_fgm: 451.97695\n\nCosine similarity between v21 and weighted2last_fgm_512 on cohesion: [[385.27085483]]\nCosine similarity between v21 and weighted2last_fgm_512 on syntax: [[322.9173454]]\nCosine similarity between v21 and weighted2last_fgm_512 on vocabulary: [[347.22522378]]\nCosine similarity between v21 and weighted2last_fgm_512 on phraseology: [[328.04329336]]\nCosine similarity between v21 and weighted2last_fgm_512 on grammar: [[413.52580237]]\nCosine similarity between v21 and weighted2last_fgm_512 on conventions: [[345.68701845]]\nMean cosine similarity between v21 and weighted2last_fgm_512: 357.11159\n\nCosine similarity between v21 and weightedmean2last_fgm_512 on cohesion: [[403.36686204]]\nCosine similarity between v21 and weightedmean2last_fgm_512 on syntax: [[347.65009839]]\nCosine similarity between v21 and weightedmean2last_fgm_512 on vocabulary: [[356.97157916]]\nCosine similarity between v21 and weightedmean2last_fgm_512 on phraseology: [[331.05185615]]\nCosine similarity between v21 and weightedmean2last_fgm_512 on grammar: [[421.21448031]]\nCosine similarity between v21 and weightedmean2last_fgm_512 on conventions: [[346.77965347]]\nMean cosine similarity between v21 and weightedmean2last_fgm_512: 367.83909\n\nCosine similarity between v21 and attention_fgm_512 on cohesion: [[465.42311806]]\nCosine similarity between v21 and attention_fgm_512 on syntax: [[368.44966801]]\nCosine similarity between v21 and attention_fgm_512 on vocabulary: [[359.46566979]]\nCosine similarity between v21 and attention_fgm_512 on phraseology: [[375.36248112]]\nCosine similarity between v21 and attention_fgm_512 on grammar: [[466.81367864]]\nCosine similarity between v21 and attention_fgm_512 on conventions: [[401.06072469]]\nMean cosine similarity between v21 and attention_fgm_512: 406.09589\n\nCosine similarity between v21 and attention_fgm_768 on cohesion: [[405.24630049]]\nCosine similarity between v21 and attention_fgm_768 on syntax: [[337.86643171]]\nCosine similarity between v21 and attention_fgm_768 on vocabulary: [[336.76271522]]\nCosine similarity between v21 and attention_fgm_768 on phraseology: [[350.44583404]]\nCosine similarity between v21 and attention_fgm_768 on grammar: [[447.89839041]]\nCosine similarity between v21 and attention_fgm_768 on conventions: [[345.42944819]]\nMean cosine similarity between v21 and attention_fgm_768: 370.60819\n\nCosine similarity between v21 and v112 on cohesion: [[491.32103074]]\nCosine similarity between v21 and v112 on syntax: [[362.60409629]]\nCosine similarity between v21 and v112 on vocabulary: [[370.3989774]]\nCosine similarity between v21 and v112 on phraseology: [[367.22717094]]\nCosine similarity between v21 and v112 on grammar: [[493.57374191]]\nCosine similarity between v21 and v112 on conventions: [[426.13958287]]\nMean cosine similarity between v21 and v112: 418.54410\n\nCosine similarity between v21 and mean_attention_no_fgm on cohesion: [[459.11870408]]\nCosine similarity between v21 and mean_attention_no_fgm on syntax: [[376.90502048]]\nCosine similarity between v21 and mean_attention_no_fgm on vocabulary: [[371.25861609]]\nCosine similarity between v21 and mean_attention_no_fgm on phraseology: [[371.86484921]]\nCosine similarity between v21 and mean_attention_no_fgm on grammar: [[466.43412161]]\nCosine similarity between v21 and mean_attention_no_fgm on conventions: [[411.0002681]]\nMean cosine similarity between v21 and mean_attention_no_fgm: 409.43026\n\nCosine similarity between v21 and v2 on cohesion: [[432.04882812]]\nCosine similarity between v21 and v2 on syntax: [[391.6171875]]\nCosine similarity between v21 and v2 on vocabulary: [[423.72363281]]\nCosine similarity between v21 and v2 on phraseology: [[394.41943359]]\nCosine similarity between v21 and v2 on grammar: [[507.93066406]]\nCosine similarity between v21 and v2 on conventions: [[390.27050781]]\nMean cosine similarity between v21 and v2: 423.33504\n\nCosine similarity between v21 and weightedpool on cohesion: [[516.61936772]]\nCosine similarity between v21 and weightedpool on syntax: [[427.88314354]]\nCosine similarity between v21 and weightedpool on vocabulary: [[417.30846143]]\nCosine similarity between v21 and weightedpool on phraseology: [[474.71824539]]\nCosine similarity between v21 and weightedpool on grammar: [[552.54212582]]\nCosine similarity between v21 and weightedpool on conventions: [[476.86729479]]\nMean cosine similarity between v21 and weightedpool: 477.65644\n\nCosine similarity between v21 and attention_fgm on cohesion: [[394.46082193]]\nCosine similarity between v21 and attention_fgm on syntax: [[343.52711225]]\nCosine similarity between v21 and attention_fgm on vocabulary: [[338.43397701]]\nCosine similarity between v21 and attention_fgm on phraseology: [[347.6962105]]\nCosine similarity between v21 and attention_fgm on grammar: [[432.2154783]]\nCosine similarity between v21 and attention_fgm on conventions: [[348.16146886]]\nMean cosine similarity between v21 and attention_fgm: 367.41584\n\nCosine similarity between v112 and attention_large_fgm on cohesion: [[468.33740032]]\nCosine similarity between v112 and attention_large_fgm on syntax: [[375.12523425]]\nCosine similarity between v112 and attention_large_fgm on vocabulary: [[363.21144438]]\nCosine similarity between v112 and attention_large_fgm on phraseology: [[381.79164219]]\nCosine similarity between v112 and attention_large_fgm on grammar: [[458.69301915]]\nCosine similarity between v112 and attention_large_fgm on conventions: [[435.81001151]]\nMean cosine similarity between v112 and attention_large_fgm: 413.82813\n\nCosine similarity between v112 and weighted2last_fgm_512 on cohesion: [[446.33012748]]\nCosine similarity between v112 and weighted2last_fgm_512 on syntax: [[338.62730145]]\nCosine similarity between v112 and weighted2last_fgm_512 on vocabulary: [[331.01030242]]\nCosine similarity between v112 and weighted2last_fgm_512 on phraseology: [[341.313146]]\nCosine similarity between v112 and weighted2last_fgm_512 on grammar: [[382.15691018]]\nCosine similarity between v112 and weighted2last_fgm_512 on conventions: [[394.39296514]]\nMean cosine similarity between v112 and weighted2last_fgm_512: 372.30513\n\nCosine similarity between v112 and weightedmean2last_fgm_512 on cohesion: [[420.72122504]]\nCosine similarity between v112 and weightedmean2last_fgm_512 on syntax: [[350.24174859]]\nCosine similarity between v112 and weightedmean2last_fgm_512 on vocabulary: [[320.95868628]]\nCosine similarity between v112 and weightedmean2last_fgm_512 on phraseology: [[342.96524851]]\nCosine similarity between v112 and weightedmean2last_fgm_512 on grammar: [[369.50376917]]\nCosine similarity between v112 and weightedmean2last_fgm_512 on conventions: [[402.33042933]]\nMean cosine similarity between v112 and weightedmean2last_fgm_512: 367.78685\n\nCosine similarity between v112 and attention_fgm_512 on cohesion: [[474.8025987]]\nCosine similarity between v112 and attention_fgm_512 on syntax: [[344.54754651]]\nCosine similarity between v112 and attention_fgm_512 on vocabulary: [[322.26666641]]\nCosine similarity between v112 and attention_fgm_512 on phraseology: [[342.8398713]]\nCosine similarity between v112 and attention_fgm_512 on grammar: [[418.40966657]]\nCosine similarity between v112 and attention_fgm_512 on conventions: [[404.68748128]]\nMean cosine similarity between v112 and attention_fgm_512: 384.59231\n\nCosine similarity between v112 and attention_fgm_768 on cohesion: [[432.04760662]]\nCosine similarity between v112 and attention_fgm_768 on syntax: [[316.973225]]\nCosine similarity between v112 and attention_fgm_768 on vocabulary: [[294.60690141]]\nCosine similarity between v112 and attention_fgm_768 on phraseology: [[331.34592044]]\nCosine similarity between v112 and attention_fgm_768 on grammar: [[382.68680084]]\nCosine similarity between v112 and attention_fgm_768 on conventions: [[371.76662904]]\nMean cosine similarity between v112 and attention_fgm_768: 354.90451\n\nCosine similarity between v112 and v21 on cohesion: [[491.32103074]]\nCosine similarity between v112 and v21 on syntax: [[362.60409629]]\nCosine similarity between v112 and v21 on vocabulary: [[370.3989774]]\nCosine similarity between v112 and v21 on phraseology: [[367.22717094]]\nCosine similarity between v112 and v21 on grammar: [[493.57374191]]\nCosine similarity between v112 and v21 on conventions: [[426.13958287]]\nMean cosine similarity between v112 and v21: 418.54410\n\nCosine similarity between v112 and mean_attention_no_fgm on cohesion: [[406.84248841]]\nCosine similarity between v112 and mean_attention_no_fgm on syntax: [[305.40059698]]\nCosine similarity between v112 and mean_attention_no_fgm on vocabulary: [[299.71832108]]\nCosine similarity between v112 and mean_attention_no_fgm on phraseology: [[337.6448859]]\nCosine similarity between v112 and mean_attention_no_fgm on grammar: [[361.58107328]]\nCosine similarity between v112 and mean_attention_no_fgm on conventions: [[387.57227623]]\nMean cosine similarity between v112 and mean_attention_no_fgm: 349.79327\n\nCosine similarity between v112 and v2 on cohesion: [[454.47699797]]\nCosine similarity between v112 and v2 on syntax: [[400.76712883]]\nCosine similarity between v112 and v2 on vocabulary: [[395.97141111]]\nCosine similarity between v112 and v2 on phraseology: [[391.37718773]]\nCosine similarity between v112 and v2 on grammar: [[420.13285398]]\nCosine similarity between v112 and v2 on conventions: [[416.02509761]]\nMean cosine similarity between v112 and v2: 413.12511\n\nCosine similarity between v112 and weightedpool on cohesion: [[494.49260783]]\nCosine similarity between v112 and weightedpool on syntax: [[412.58807921]]\nCosine similarity between v112 and weightedpool on vocabulary: [[363.52820456]]\nCosine similarity between v112 and weightedpool on phraseology: [[444.90984166]]\nCosine similarity between v112 and weightedpool on grammar: [[530.18058312]]\nCosine similarity between v112 and weightedpool on conventions: [[505.11686087]]\nMean cosine similarity between v112 and weightedpool: 458.46936\n\nCosine similarity between v112 and attention_fgm on cohesion: [[429.03878349]]\nCosine similarity between v112 and attention_fgm on syntax: [[317.53428209]]\nCosine similarity between v112 and attention_fgm on vocabulary: [[307.16752315]]\nCosine similarity between v112 and attention_fgm on phraseology: [[327.00401556]]\nCosine similarity between v112 and attention_fgm on grammar: [[392.96994865]]\nCosine similarity between v112 and attention_fgm on conventions: [[374.462309]]\nMean cosine similarity between v112 and attention_fgm: 358.02948\n\nCosine similarity between mean_attention_no_fgm and attention_large_fgm on cohesion: [[460.44029999]]\nCosine similarity between mean_attention_no_fgm and attention_large_fgm on syntax: [[361.44003057]]\nCosine similarity between mean_attention_no_fgm and attention_large_fgm on vocabulary: [[339.70785499]]\nCosine similarity between mean_attention_no_fgm and attention_large_fgm on phraseology: [[373.92420447]]\nCosine similarity between mean_attention_no_fgm and attention_large_fgm on grammar: [[417.76399684]]\nCosine similarity between mean_attention_no_fgm and attention_large_fgm on conventions: [[420.78119469]]\nMean cosine similarity between mean_attention_no_fgm and attention_large_fgm: 395.67626\n\nCosine similarity between mean_attention_no_fgm and weighted2last_fgm_512 on cohesion: [[439.46418917]]\nCosine similarity between mean_attention_no_fgm and weighted2last_fgm_512 on syntax: [[336.00379527]]\nCosine similarity between mean_attention_no_fgm and weighted2last_fgm_512 on vocabulary: [[330.42085326]]\nCosine similarity between mean_attention_no_fgm and weighted2last_fgm_512 on phraseology: [[364.87117386]]\nCosine similarity between mean_attention_no_fgm and weighted2last_fgm_512 on grammar: [[358.16683555]]\nCosine similarity between mean_attention_no_fgm and weighted2last_fgm_512 on conventions: [[389.52317995]]\nMean cosine similarity between mean_attention_no_fgm and weighted2last_fgm_512: 369.74167\n\nCosine similarity between mean_attention_no_fgm and weightedmean2last_fgm_512 on cohesion: [[430.89880933]]\nCosine similarity between mean_attention_no_fgm and weightedmean2last_fgm_512 on syntax: [[344.65689249]]\nCosine similarity between mean_attention_no_fgm and weightedmean2last_fgm_512 on vocabulary: [[321.29209848]]\nCosine similarity between mean_attention_no_fgm and weightedmean2last_fgm_512 on phraseology: [[353.12425404]]\nCosine similarity between mean_attention_no_fgm and weightedmean2last_fgm_512 on grammar: [[368.57186492]]\nCosine similarity between mean_attention_no_fgm and weightedmean2last_fgm_512 on conventions: [[397.75342942]]\nMean cosine similarity between mean_attention_no_fgm and weightedmean2last_fgm_512: 369.38289\n\nCosine similarity between mean_attention_no_fgm and attention_fgm_512 on cohesion: [[450.65884366]]\nCosine similarity between mean_attention_no_fgm and attention_fgm_512 on syntax: [[353.67427656]]\nCosine similarity between mean_attention_no_fgm and attention_fgm_512 on vocabulary: [[312.75000421]]\nCosine similarity between mean_attention_no_fgm and attention_fgm_512 on phraseology: [[352.33482768]]\nCosine similarity between mean_attention_no_fgm and attention_fgm_512 on grammar: [[370.43765572]]\nCosine similarity between mean_attention_no_fgm and attention_fgm_512 on conventions: [[393.13700246]]\nMean cosine similarity between mean_attention_no_fgm and attention_fgm_512: 372.16544\n\nCosine similarity between mean_attention_no_fgm and attention_fgm_768 on cohesion: [[424.58608773]]\nCosine similarity between mean_attention_no_fgm and attention_fgm_768 on syntax: [[320.25539303]]\nCosine similarity between mean_attention_no_fgm and attention_fgm_768 on vocabulary: [[293.5669117]]\nCosine similarity between mean_attention_no_fgm and attention_fgm_768 on phraseology: [[335.41470981]]\nCosine similarity between mean_attention_no_fgm and attention_fgm_768 on grammar: [[353.3158499]]\nCosine similarity between mean_attention_no_fgm and attention_fgm_768 on conventions: [[363.71743554]]\nMean cosine similarity between mean_attention_no_fgm and attention_fgm_768: 348.47606\n\nCosine similarity between mean_attention_no_fgm and v21 on cohesion: [[459.11870408]]\nCosine similarity between mean_attention_no_fgm and v21 on syntax: [[376.90502048]]\nCosine similarity between mean_attention_no_fgm and v21 on vocabulary: [[371.25861609]]\nCosine similarity between mean_attention_no_fgm and v21 on phraseology: [[371.86484921]]\nCosine similarity between mean_attention_no_fgm and v21 on grammar: [[466.43412161]]\nCosine similarity between mean_attention_no_fgm and v21 on conventions: [[411.0002681]]\nMean cosine similarity between mean_attention_no_fgm and v21: 409.43026\n\nCosine similarity between mean_attention_no_fgm and v112 on cohesion: [[406.84248841]]\nCosine similarity between mean_attention_no_fgm and v112 on syntax: [[305.40059698]]\nCosine similarity between mean_attention_no_fgm and v112 on vocabulary: [[299.71832108]]\nCosine similarity between mean_attention_no_fgm and v112 on phraseology: [[337.6448859]]\nCosine similarity between mean_attention_no_fgm and v112 on grammar: [[361.58107328]]\nCosine similarity between mean_attention_no_fgm and v112 on conventions: [[387.57227623]]\nMean cosine similarity between mean_attention_no_fgm and v112: 349.79327\n\nCosine similarity between mean_attention_no_fgm and v2 on cohesion: [[466.68241668]]\nCosine similarity between mean_attention_no_fgm and v2 on syntax: [[427.37105131]]\nCosine similarity between mean_attention_no_fgm and v2 on vocabulary: [[383.44024169]]\nCosine similarity between mean_attention_no_fgm and v2 on phraseology: [[414.79615986]]\nCosine similarity between mean_attention_no_fgm and v2 on grammar: [[446.75815725]]\nCosine similarity between mean_attention_no_fgm and v2 on conventions: [[407.06047976]]\nMean cosine similarity between mean_attention_no_fgm and v2: 424.35142\n\nCosine similarity between mean_attention_no_fgm and weightedpool on cohesion: [[463.54803574]]\nCosine similarity between mean_attention_no_fgm and weightedpool on syntax: [[419.23043096]]\nCosine similarity between mean_attention_no_fgm and weightedpool on vocabulary: [[379.94085777]]\nCosine similarity between mean_attention_no_fgm and weightedpool on phraseology: [[500.6889956]]\nCosine similarity between mean_attention_no_fgm and weightedpool on grammar: [[560.35674608]]\nCosine similarity between mean_attention_no_fgm and weightedpool on conventions: [[512.54995191]]\nMean cosine similarity between mean_attention_no_fgm and weightedpool: 472.71917\n\nCosine similarity between mean_attention_no_fgm and attention_fgm on cohesion: [[422.65707785]]\nCosine similarity between mean_attention_no_fgm and attention_fgm on syntax: [[326.22398543]]\nCosine similarity between mean_attention_no_fgm and attention_fgm on vocabulary: [[297.28103924]]\nCosine similarity between mean_attention_no_fgm and attention_fgm on phraseology: [[330.93697047]]\nCosine similarity between mean_attention_no_fgm and attention_fgm on grammar: [[353.18819892]]\nCosine similarity between mean_attention_no_fgm and attention_fgm on conventions: [[367.28127885]]\nMean cosine similarity between mean_attention_no_fgm and attention_fgm: 349.59476\n\nCosine similarity between v2 and attention_large_fgm on cohesion: [[429.29163957]]\nCosine similarity between v2 and attention_large_fgm on syntax: [[409.16578889]]\nCosine similarity between v2 and attention_large_fgm on vocabulary: [[391.25832832]]\nCosine similarity between v2 and attention_large_fgm on phraseology: [[411.33138061]]\nCosine similarity between v2 and attention_large_fgm on grammar: [[482.35822415]]\nCosine similarity between v2 and attention_large_fgm on conventions: [[431.19491494]]\nMean cosine similarity between v2 and attention_large_fgm: 425.76671\n\nCosine similarity between v2 and weighted2last_fgm_512 on cohesion: [[438.15707958]]\nCosine similarity between v2 and weighted2last_fgm_512 on syntax: [[401.08843219]]\nCosine similarity between v2 and weighted2last_fgm_512 on vocabulary: [[397.52591586]]\nCosine similarity between v2 and weighted2last_fgm_512 on phraseology: [[400.5723604]]\nCosine similarity between v2 and weighted2last_fgm_512 on grammar: [[436.3241601]]\nCosine similarity between v2 and weighted2last_fgm_512 on conventions: [[396.40660316]]\nMean cosine similarity between v2 and weighted2last_fgm_512: 411.67909\n\nCosine similarity between v2 and weightedmean2last_fgm_512 on cohesion: [[416.88494006]]\nCosine similarity between v2 and weightedmean2last_fgm_512 on syntax: [[402.13383804]]\nCosine similarity between v2 and weightedmean2last_fgm_512 on vocabulary: [[376.92115127]]\nCosine similarity between v2 and weightedmean2last_fgm_512 on phraseology: [[393.69886639]]\nCosine similarity between v2 and weightedmean2last_fgm_512 on grammar: [[408.32441863]]\nCosine similarity between v2 and weightedmean2last_fgm_512 on conventions: [[391.56587083]]\nMean cosine similarity between v2 and weightedmean2last_fgm_512: 398.25485\n\nCosine similarity between v2 and attention_fgm_512 on cohesion: [[492.70282446]]\nCosine similarity between v2 and attention_fgm_512 on syntax: [[400.31939891]]\nCosine similarity between v2 and attention_fgm_512 on vocabulary: [[393.21264968]]\nCosine similarity between v2 and attention_fgm_512 on phraseology: [[411.17021969]]\nCosine similarity between v2 and attention_fgm_512 on grammar: [[490.28848487]]\nCosine similarity between v2 and attention_fgm_512 on conventions: [[423.14658055]]\nMean cosine similarity between v2 and attention_fgm_512: 435.14003\n\nCosine similarity between v2 and attention_fgm_768 on cohesion: [[452.81750587]]\nCosine similarity between v2 and attention_fgm_768 on syntax: [[399.13067603]]\nCosine similarity between v2 and attention_fgm_768 on vocabulary: [[385.26101649]]\nCosine similarity between v2 and attention_fgm_768 on phraseology: [[412.28403437]]\nCosine similarity between v2 and attention_fgm_768 on grammar: [[469.38681209]]\nCosine similarity between v2 and attention_fgm_768 on conventions: [[394.15101606]]\nMean cosine similarity between v2 and attention_fgm_768: 418.83851\n\nCosine similarity between v2 and v21 on cohesion: [[432.04882812]]\nCosine similarity between v2 and v21 on syntax: [[391.6171875]]\nCosine similarity between v2 and v21 on vocabulary: [[423.72363281]]\nCosine similarity between v2 and v21 on phraseology: [[394.41943359]]\nCosine similarity between v2 and v21 on grammar: [[507.93066406]]\nCosine similarity between v2 and v21 on conventions: [[390.27050781]]\nMean cosine similarity between v2 and v21: 423.33504\n\nCosine similarity between v2 and v112 on cohesion: [[454.47699797]]\nCosine similarity between v2 and v112 on syntax: [[400.76712883]]\nCosine similarity between v2 and v112 on vocabulary: [[395.97141111]]\nCosine similarity between v2 and v112 on phraseology: [[391.37718773]]\nCosine similarity between v2 and v112 on grammar: [[420.13285398]]\nCosine similarity between v2 and v112 on conventions: [[416.02509761]]\nMean cosine similarity between v2 and v112: 413.12511\n\nCosine similarity between v2 and mean_attention_no_fgm on cohesion: [[466.68241668]]\nCosine similarity between v2 and mean_attention_no_fgm on syntax: [[427.37105131]]\nCosine similarity between v2 and mean_attention_no_fgm on vocabulary: [[383.44024169]]\nCosine similarity between v2 and mean_attention_no_fgm on phraseology: [[414.79615986]]\nCosine similarity between v2 and mean_attention_no_fgm on grammar: [[446.75815725]]\nCosine similarity between v2 and mean_attention_no_fgm on conventions: [[407.06047976]]\nMean cosine similarity between v2 and mean_attention_no_fgm: 424.35142\n\nCosine similarity between v2 and weightedpool on cohesion: [[512.80227578]]\nCosine similarity between v2 and weightedpool on syntax: [[479.03068531]]\nCosine similarity between v2 and weightedpool on vocabulary: [[438.56966615]]\nCosine similarity between v2 and weightedpool on phraseology: [[477.89531243]]\nCosine similarity between v2 and weightedpool on grammar: [[555.23283494]]\nCosine similarity between v2 and weightedpool on conventions: [[520.63498425]]\nMean cosine similarity between v2 and weightedpool: 497.36096\n\nCosine similarity between v2 and attention_fgm on cohesion: [[431.38501269]]\nCosine similarity between v2 and attention_fgm on syntax: [[389.29624295]]\nCosine similarity between v2 and attention_fgm on vocabulary: [[372.41327178]]\nCosine similarity between v2 and attention_fgm on phraseology: [[383.45575917]]\nCosine similarity between v2 and attention_fgm on grammar: [[452.15039361]]\nCosine similarity between v2 and attention_fgm on conventions: [[384.94517839]]\nMean cosine similarity between v2 and attention_fgm: 402.27431\n\nCosine similarity between weightedpool and attention_large_fgm on cohesion: [[571.59756863]]\nCosine similarity between weightedpool and attention_large_fgm on syntax: [[471.78197587]]\nCosine similarity between weightedpool and attention_large_fgm on vocabulary: [[448.18218958]]\nCosine similarity between weightedpool and attention_large_fgm on phraseology: [[529.43094695]]\nCosine similarity between weightedpool and attention_large_fgm on grammar: [[639.50539243]]\nCosine similarity between weightedpool and attention_large_fgm on conventions: [[584.6061455]]\nMean cosine similarity between weightedpool and attention_large_fgm: 540.85070\n\nCosine similarity between weightedpool and weighted2last_fgm_512 on cohesion: [[546.56103539]]\nCosine similarity between weightedpool and weighted2last_fgm_512 on syntax: [[448.18063331]]\nCosine similarity between weightedpool and weighted2last_fgm_512 on vocabulary: [[403.31440926]]\nCosine similarity between weightedpool and weighted2last_fgm_512 on phraseology: [[480.45081711]]\nCosine similarity between weightedpool and weighted2last_fgm_512 on grammar: [[578.18615496]]\nCosine similarity between weightedpool and weighted2last_fgm_512 on conventions: [[525.39857775]]\nMean cosine similarity between weightedpool and weighted2last_fgm_512: 497.01527\n\nCosine similarity between weightedpool and weightedmean2last_fgm_512 on cohesion: [[532.63622247]]\nCosine similarity between weightedpool and weightedmean2last_fgm_512 on syntax: [[449.34850336]]\nCosine similarity between weightedpool and weightedmean2last_fgm_512 on vocabulary: [[406.52901536]]\nCosine similarity between weightedpool and weightedmean2last_fgm_512 on phraseology: [[478.45703673]]\nCosine similarity between weightedpool and weightedmean2last_fgm_512 on grammar: [[569.41970963]]\nCosine similarity between weightedpool and weightedmean2last_fgm_512 on conventions: [[516.97919159]]\nMean cosine similarity between weightedpool and weightedmean2last_fgm_512: 492.22828\n\nCosine similarity between weightedpool and attention_fgm_512 on cohesion: [[560.9723899]]\nCosine similarity between weightedpool and attention_fgm_512 on syntax: [[474.62304121]]\nCosine similarity between weightedpool and attention_fgm_512 on vocabulary: [[413.30405742]]\nCosine similarity between weightedpool and attention_fgm_512 on phraseology: [[487.615989]]\nCosine similarity between weightedpool and attention_fgm_512 on grammar: [[610.25309245]]\nCosine similarity between weightedpool and attention_fgm_512 on conventions: [[538.0705499]]\nMean cosine similarity between weightedpool and attention_fgm_512: 514.13985\n\nCosine similarity between weightedpool and attention_fgm_768 on cohesion: [[533.74748579]]\nCosine similarity between weightedpool and attention_fgm_768 on syntax: [[453.40404713]]\nCosine similarity between weightedpool and attention_fgm_768 on vocabulary: [[385.99938953]]\nCosine similarity between weightedpool and attention_fgm_768 on phraseology: [[486.45818782]]\nCosine similarity between weightedpool and attention_fgm_768 on grammar: [[590.07859254]]\nCosine similarity between weightedpool and attention_fgm_768 on conventions: [[507.98608142]]\nMean cosine similarity between weightedpool and attention_fgm_768: 492.94563\n\nCosine similarity between weightedpool and v21 on cohesion: [[516.61936772]]\nCosine similarity between weightedpool and v21 on syntax: [[427.88314354]]\nCosine similarity between weightedpool and v21 on vocabulary: [[417.30846143]]\nCosine similarity between weightedpool and v21 on phraseology: [[474.71824539]]\nCosine similarity between weightedpool and v21 on grammar: [[552.54212582]]\nCosine similarity between weightedpool and v21 on conventions: [[476.86729479]]\nMean cosine similarity between weightedpool and v21: 477.65644\n\nCosine similarity between weightedpool and v112 on cohesion: [[494.49260783]]\nCosine similarity between weightedpool and v112 on syntax: [[412.58807921]]\nCosine similarity between weightedpool and v112 on vocabulary: [[363.52820456]]\nCosine similarity between weightedpool and v112 on phraseology: [[444.90984166]]\nCosine similarity between weightedpool and v112 on grammar: [[530.18058312]]\nCosine similarity between weightedpool and v112 on conventions: [[505.11686087]]\nMean cosine similarity between weightedpool and v112: 458.46936\n\nCosine similarity between weightedpool and mean_attention_no_fgm on cohesion: [[463.54803574]]\nCosine similarity between weightedpool and mean_attention_no_fgm on syntax: [[419.23043096]]\nCosine similarity between weightedpool and mean_attention_no_fgm on vocabulary: [[379.94085777]]\nCosine similarity between weightedpool and mean_attention_no_fgm on phraseology: [[500.6889956]]\nCosine similarity between weightedpool and mean_attention_no_fgm on grammar: [[560.35674608]]\nCosine similarity between weightedpool and mean_attention_no_fgm on conventions: [[512.54995191]]\nMean cosine similarity between weightedpool and mean_attention_no_fgm: 472.71917\n\nCosine similarity between weightedpool and v2 on cohesion: [[512.80227578]]\nCosine similarity between weightedpool and v2 on syntax: [[479.03068531]]\nCosine similarity between weightedpool and v2 on vocabulary: [[438.56966615]]\nCosine similarity between weightedpool and v2 on phraseology: [[477.89531243]]\nCosine similarity between weightedpool and v2 on grammar: [[555.23283494]]\nCosine similarity between weightedpool and v2 on conventions: [[520.63498425]]\nMean cosine similarity between weightedpool and v2: 497.36096\n\nCosine similarity between weightedpool and attention_fgm on cohesion: [[519.01576275]]\nCosine similarity between weightedpool and attention_fgm on syntax: [[446.46801054]]\nCosine similarity between weightedpool and attention_fgm on vocabulary: [[386.8635298]]\nCosine similarity between weightedpool and attention_fgm on phraseology: [[463.50872517]]\nCosine similarity between weightedpool and attention_fgm on grammar: [[568.69957185]]\nCosine similarity between weightedpool and attention_fgm on conventions: [[503.51761925]]\nMean cosine similarity between weightedpool and attention_fgm: 481.34554\n\nCosine similarity between attention_fgm and attention_large_fgm on cohesion: [[410.15437609]]\nCosine similarity between attention_fgm and attention_large_fgm on syntax: [[353.95186162]]\nCosine similarity between attention_fgm and attention_large_fgm on vocabulary: [[336.9159708]]\nCosine similarity between attention_fgm and attention_large_fgm on phraseology: [[362.97130787]]\nCosine similarity between attention_fgm and attention_large_fgm on grammar: [[424.88914764]]\nCosine similarity between attention_fgm and attention_large_fgm on conventions: [[384.57606483]]\nMean cosine similarity between attention_fgm and attention_large_fgm: 378.90979\n\nCosine similarity between attention_fgm and weighted2last_fgm_512 on cohesion: [[334.64357775]]\nCosine similarity between attention_fgm and weighted2last_fgm_512 on syntax: [[295.71736562]]\nCosine similarity between attention_fgm and weighted2last_fgm_512 on vocabulary: [[290.73094428]]\nCosine similarity between attention_fgm and weighted2last_fgm_512 on phraseology: [[299.0470984]]\nCosine similarity between attention_fgm and weighted2last_fgm_512 on grammar: [[327.01792967]]\nCosine similarity between attention_fgm and weighted2last_fgm_512 on conventions: [[312.89411396]]\nMean cosine similarity between attention_fgm and weighted2last_fgm_512: 310.00850\n\nCosine similarity between attention_fgm and weightedmean2last_fgm_512 on cohesion: [[326.60734709]]\nCosine similarity between attention_fgm and weightedmean2last_fgm_512 on syntax: [[305.36618488]]\nCosine similarity between attention_fgm and weightedmean2last_fgm_512 on vocabulary: [[280.02462064]]\nCosine similarity between attention_fgm and weightedmean2last_fgm_512 on phraseology: [[295.49237678]]\nCosine similarity between attention_fgm and weightedmean2last_fgm_512 on grammar: [[343.4280082]]\nCosine similarity between attention_fgm and weightedmean2last_fgm_512 on conventions: [[315.85901001]]\nMean cosine similarity between attention_fgm and weightedmean2last_fgm_512: 311.12959\n\nCosine similarity between attention_fgm and attention_fgm_512 on cohesion: [[250.96339653]]\nCosine similarity between attention_fgm and attention_fgm_512 on syntax: [[207.99832564]]\nCosine similarity between attention_fgm and attention_fgm_512 on vocabulary: [[185.81710273]]\nCosine similarity between attention_fgm and attention_fgm_512 on phraseology: [[190.10409123]]\nCosine similarity between attention_fgm and attention_fgm_512 on grammar: [[212.52178816]]\nCosine similarity between attention_fgm and attention_fgm_512 on conventions: [[198.32500548]]\nMean cosine similarity between attention_fgm and attention_fgm_512: 207.62162\n\nCosine similarity between attention_fgm and attention_fgm_768 on cohesion: [[196.7452893]]\nCosine similarity between attention_fgm and attention_fgm_768 on syntax: [[179.19097114]]\nCosine similarity between attention_fgm and attention_fgm_768 on vocabulary: [[165.23687506]]\nCosine similarity between attention_fgm and attention_fgm_768 on phraseology: [[182.90750647]]\nCosine similarity between attention_fgm and attention_fgm_768 on grammar: [[205.53425121]]\nCosine similarity between attention_fgm and attention_fgm_768 on conventions: [[178.69447774]]\nMean cosine similarity between attention_fgm and attention_fgm_768: 184.71823\n\nCosine similarity between attention_fgm and v21 on cohesion: [[394.46082193]]\nCosine similarity between attention_fgm and v21 on syntax: [[343.52711225]]\nCosine similarity between attention_fgm and v21 on vocabulary: [[338.43397701]]\nCosine similarity between attention_fgm and v21 on phraseology: [[347.6962105]]\nCosine similarity between attention_fgm and v21 on grammar: [[432.2154783]]\nCosine similarity between attention_fgm and v21 on conventions: [[348.16146886]]\nMean cosine similarity between attention_fgm and v21: 367.41584\n\nCosine similarity between attention_fgm and v112 on cohesion: [[429.03878349]]\nCosine similarity between attention_fgm and v112 on syntax: [[317.53428209]]\nCosine similarity between attention_fgm and v112 on vocabulary: [[307.16752315]]\nCosine similarity between attention_fgm and v112 on phraseology: [[327.00401556]]\nCosine similarity between attention_fgm and v112 on grammar: [[392.96994865]]\nCosine similarity between attention_fgm and v112 on conventions: [[374.462309]]\nMean cosine similarity between attention_fgm and v112: 358.02948\n\nCosine similarity between attention_fgm and mean_attention_no_fgm on cohesion: [[422.65707785]]\nCosine similarity between attention_fgm and mean_attention_no_fgm on syntax: [[326.22398543]]\nCosine similarity between attention_fgm and mean_attention_no_fgm on vocabulary: [[297.28103924]]\nCosine similarity between attention_fgm and mean_attention_no_fgm on phraseology: [[330.93697047]]\nCosine similarity between attention_fgm and mean_attention_no_fgm on grammar: [[353.18819892]]\nCosine similarity between attention_fgm and mean_attention_no_fgm on conventions: [[367.28127885]]\nMean cosine similarity between attention_fgm and mean_attention_no_fgm: 349.59476\n\nCosine similarity between attention_fgm and v2 on cohesion: [[431.38501269]]\nCosine similarity between attention_fgm and v2 on syntax: [[389.29624295]]\nCosine similarity between attention_fgm and v2 on vocabulary: [[372.41327178]]\nCosine similarity between attention_fgm and v2 on phraseology: [[383.45575917]]\nCosine similarity between attention_fgm and v2 on grammar: [[452.15039361]]\nCosine similarity between attention_fgm and v2 on conventions: [[384.94517839]]\nMean cosine similarity between attention_fgm and v2: 402.27431\n\nCosine similarity between attention_fgm and weightedpool on cohesion: [[519.01576275]]\nCosine similarity between attention_fgm and weightedpool on syntax: [[446.46801054]]\nCosine similarity between attention_fgm and weightedpool on vocabulary: [[386.8635298]]\nCosine similarity between attention_fgm and weightedpool on phraseology: [[463.50872517]]\nCosine similarity between attention_fgm and weightedpool on grammar: [[568.69957185]]\nCosine similarity between attention_fgm and weightedpool on conventions: [[503.51761925]]\nMean cosine similarity between attention_fgm and weightedpool: 481.34554\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"1. Mỗi bước xét lấy 1 model nếu như mean khoảng cách cao hơn mean khoảng cách trước đó ","metadata":{}},{"cell_type":"code","source":"result = []\n\nfor model1 in sim_mat.keys():\n    #model1 = 'attention_large_fgm'\n    models = [model1]\n    dists = []\n    global_best = 0\n    threshold = 300\n\n    while True:\n        cur_best = 0\n        cur_models = models\n        cur_best_model = None\n        for model2, model2_dist in sim_mat[model1].items():\n            if model2 in models:\n                continue\n            expect_mean_dist = np.mean([model2_dist, *dists])\n            if cur_best < expect_mean_dist:\n                cur_best = expect_mean_dist\n                cur_models = [*models, model2]\n                cur_best_model = model2\n        if cur_best <= threshold:\n            result.append((global_best, models))\n            print()\n            print(f'Mean distance decreased by {threshold}. Stopping.')\n            print(f'Final mean distance: {global_best}. Models: {models}')\n            print()\n            break\n        print(f'Adding {cur_best_model} to stack. Mean distance: {cur_best:.4f}')\n        global_best = cur_best\n        models = cur_models\n        model1 = cur_best_model","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-28T02:43:36.312449Z","iopub.execute_input":"2022-11-28T02:43:36.314340Z","iopub.status.idle":"2022-11-28T02:43:36.333593Z","shell.execute_reply.started":"2022-11-28T02:43:36.314257Z","shell.execute_reply":"2022-11-28T02:43:36.331727Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Adding weightedpool to stack. Mean distance: 540.8507\nAdding attention_fgm_512 to stack. Mean distance: 514.1399\nAdding v2 to stack. Mean distance: 435.1400\nAdding mean_attention_no_fgm to stack. Mean distance: 424.3514\nAdding v21 to stack. Mean distance: 409.4303\nAdding v112 to stack. Mean distance: 418.5441\nAdding weighted2last_fgm_512 to stack. Mean distance: 372.3051\nAdding attention_fgm to stack. Mean distance: 310.0085\nAdding weightedmean2last_fgm_512 to stack. Mean distance: 311.1296\nAdding attention_fgm_768 to stack. Mean distance: 300.1116\n\nMean distance decreased by 300. Stopping.\nFinal mean distance: 300.1116051774349. Models: ['attention_large_fgm', 'weightedpool', 'attention_fgm_512', 'v2', 'mean_attention_no_fgm', 'v21', 'v112', 'weighted2last_fgm_512', 'attention_fgm', 'weightedmean2last_fgm_512', 'attention_fgm_768']\n\nAdding weightedpool to stack. Mean distance: 497.0153\nAdding attention_large_fgm to stack. Mean distance: 540.8507\nAdding v21 to stack. Mean distance: 451.9769\nAdding v2 to stack. Mean distance: 423.3350\nAdding attention_fgm_512 to stack. Mean distance: 435.1400\nAdding v112 to stack. Mean distance: 384.5923\nAdding weightedmean2last_fgm_512 to stack. Mean distance: 367.7869\nAdding mean_attention_no_fgm to stack. Mean distance: 369.3829\nAdding attention_fgm to stack. Mean distance: 349.5948\n\nMean distance decreased by 300. Stopping.\nFinal mean distance: 349.5947584609191. Models: ['weighted2last_fgm_512', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weightedmean2last_fgm_512', 'mean_attention_no_fgm', 'attention_fgm']\n\nAdding weightedpool to stack. Mean distance: 492.2283\nAdding attention_large_fgm to stack. Mean distance: 540.8507\nAdding v21 to stack. Mean distance: 451.9769\nAdding v2 to stack. Mean distance: 423.3350\nAdding attention_fgm_512 to stack. Mean distance: 435.1400\nAdding v112 to stack. Mean distance: 384.5923\nAdding weighted2last_fgm_512 to stack. Mean distance: 372.3051\nAdding mean_attention_no_fgm to stack. Mean distance: 369.7417\nAdding attention_fgm to stack. Mean distance: 349.5948\n\nMean distance decreased by 300. Stopping.\nFinal mean distance: 349.5947584609191. Models: ['weightedmean2last_fgm_512', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'mean_attention_no_fgm', 'attention_fgm']\n\nAdding weightedpool to stack. Mean distance: 514.1399\nAdding attention_large_fgm to stack. Mean distance: 540.8507\nAdding v21 to stack. Mean distance: 451.9769\nAdding v2 to stack. Mean distance: 423.3350\nAdding mean_attention_no_fgm to stack. Mean distance: 424.3514\nAdding weighted2last_fgm_512 to stack. Mean distance: 369.7417\nAdding v112 to stack. Mean distance: 372.3051\nAdding weightedmean2last_fgm_512 to stack. Mean distance: 367.7869\nAdding attention_fgm to stack. Mean distance: 311.1296\n\nMean distance decreased by 300. Stopping.\nFinal mean distance: 311.12959126728424. Models: ['attention_fgm_512', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'mean_attention_no_fgm', 'weighted2last_fgm_512', 'v112', 'weightedmean2last_fgm_512', 'attention_fgm']\n\nAdding weightedpool to stack. Mean distance: 492.9456\nAdding attention_large_fgm to stack. Mean distance: 540.8507\nAdding v21 to stack. Mean distance: 451.9769\nAdding v2 to stack. Mean distance: 423.3350\nAdding attention_fgm_512 to stack. Mean distance: 435.1400\nAdding v112 to stack. Mean distance: 384.5923\nAdding weighted2last_fgm_512 to stack. Mean distance: 372.3051\nAdding mean_attention_no_fgm to stack. Mean distance: 369.7417\nAdding weightedmean2last_fgm_512 to stack. Mean distance: 369.3829\nAdding attention_fgm to stack. Mean distance: 311.1296\n\nMean distance decreased by 300. Stopping.\nFinal mean distance: 311.12959126728424. Models: ['attention_fgm_768', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'attention_fgm']\n\nAdding weightedpool to stack. Mean distance: 477.6564\nAdding attention_large_fgm to stack. Mean distance: 540.8507\nAdding v2 to stack. Mean distance: 425.7667\nAdding attention_fgm_512 to stack. Mean distance: 435.1400\nAdding v112 to stack. Mean distance: 384.5923\nAdding weighted2last_fgm_512 to stack. Mean distance: 372.3051\nAdding mean_attention_no_fgm to stack. Mean distance: 369.7417\nAdding weightedmean2last_fgm_512 to stack. Mean distance: 369.3829\nAdding attention_fgm to stack. Mean distance: 311.1296\n\nMean distance decreased by 300. Stopping.\nFinal mean distance: 311.12959126728424. Models: ['v21', 'weightedpool', 'attention_large_fgm', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'attention_fgm']\n\nAdding weightedpool to stack. Mean distance: 458.4694\nAdding attention_large_fgm to stack. Mean distance: 540.8507\nAdding v21 to stack. Mean distance: 451.9769\nAdding v2 to stack. Mean distance: 423.3350\nAdding attention_fgm_512 to stack. Mean distance: 435.1400\nAdding mean_attention_no_fgm to stack. Mean distance: 372.1654\nAdding weighted2last_fgm_512 to stack. Mean distance: 369.7417\nAdding attention_fgm to stack. Mean distance: 310.0085\nAdding weightedmean2last_fgm_512 to stack. Mean distance: 311.1296\nAdding attention_fgm_768 to stack. Mean distance: 300.1116\n\nMean distance decreased by 300. Stopping.\nFinal mean distance: 300.1116051774349. Models: ['v112', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'mean_attention_no_fgm', 'weighted2last_fgm_512', 'attention_fgm', 'weightedmean2last_fgm_512', 'attention_fgm_768']\n\nAdding weightedpool to stack. Mean distance: 472.7192\nAdding attention_large_fgm to stack. Mean distance: 540.8507\nAdding v21 to stack. Mean distance: 451.9769\nAdding v2 to stack. Mean distance: 423.3350\nAdding attention_fgm_512 to stack. Mean distance: 435.1400\nAdding v112 to stack. Mean distance: 384.5923\nAdding weighted2last_fgm_512 to stack. Mean distance: 372.3051\nAdding attention_fgm to stack. Mean distance: 310.0085\nAdding weightedmean2last_fgm_512 to stack. Mean distance: 311.1296\nAdding attention_fgm_768 to stack. Mean distance: 300.1116\n\nMean distance decreased by 300. Stopping.\nFinal mean distance: 300.1116051774349. Models: ['mean_attention_no_fgm', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'attention_fgm', 'weightedmean2last_fgm_512', 'attention_fgm_768']\n\nAdding weightedpool to stack. Mean distance: 497.3610\nAdding attention_large_fgm to stack. Mean distance: 540.8507\nAdding v21 to stack. Mean distance: 451.9769\nAdding v112 to stack. Mean distance: 418.5441\nAdding attention_fgm_512 to stack. Mean distance: 384.5923\nAdding mean_attention_no_fgm to stack. Mean distance: 372.1654\nAdding weighted2last_fgm_512 to stack. Mean distance: 369.7417\nAdding attention_fgm to stack. Mean distance: 310.0085\nAdding weightedmean2last_fgm_512 to stack. Mean distance: 311.1296\nAdding attention_fgm_768 to stack. Mean distance: 300.1116\n\nMean distance decreased by 300. Stopping.\nFinal mean distance: 300.1116051774349. Models: ['v2', 'weightedpool', 'attention_large_fgm', 'v21', 'v112', 'attention_fgm_512', 'mean_attention_no_fgm', 'weighted2last_fgm_512', 'attention_fgm', 'weightedmean2last_fgm_512', 'attention_fgm_768']\n\nAdding attention_large_fgm to stack. Mean distance: 540.8507\nAdding v21 to stack. Mean distance: 451.9769\nAdding v2 to stack. Mean distance: 423.3350\nAdding attention_fgm_512 to stack. Mean distance: 435.1400\nAdding v112 to stack. Mean distance: 384.5923\nAdding weighted2last_fgm_512 to stack. Mean distance: 372.3051\nAdding mean_attention_no_fgm to stack. Mean distance: 369.7417\nAdding weightedmean2last_fgm_512 to stack. Mean distance: 369.3829\nAdding attention_fgm to stack. Mean distance: 311.1296\n\nMean distance decreased by 300. Stopping.\nFinal mean distance: 311.12959126728424. Models: ['weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'attention_fgm']\n\nAdding weightedpool to stack. Mean distance: 481.3455\nAdding attention_large_fgm to stack. Mean distance: 540.8507\nAdding v21 to stack. Mean distance: 451.9769\nAdding v2 to stack. Mean distance: 423.3350\nAdding attention_fgm_512 to stack. Mean distance: 435.1400\nAdding v112 to stack. Mean distance: 384.5923\nAdding weighted2last_fgm_512 to stack. Mean distance: 372.3051\nAdding mean_attention_no_fgm to stack. Mean distance: 369.7417\nAdding weightedmean2last_fgm_512 to stack. Mean distance: 369.3829\nAdding attention_fgm_768 to stack. Mean distance: 300.1116\n\nMean distance decreased by 300. Stopping.\nFinal mean distance: 300.1116051774349. Models: ['attention_fgm', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'attention_fgm_768']\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for s, m in sorted(result, key=lambda tup: tup[0]):\n    cfgs = [cfg for cfg in all_models if cfg.name in m]\n    score = get_ensemble_oof(cfgs, False, False)[0]\n    print(f'Distance: {s} with {len(m)} models {m} with score {score:.5f}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-28T02:43:36.336091Z","iopub.execute_input":"2022-11-28T02:43:36.336650Z","iopub.status.idle":"2022-11-28T02:44:16.279549Z","shell.execute_reply.started":"2022-11-28T02:43:36.336598Z","shell.execute_reply":"2022-11-28T02:44:16.277812Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Distance: 300.1116051774349 with 11 models ['attention_large_fgm', 'weightedpool', 'attention_fgm_512', 'v2', 'mean_attention_no_fgm', 'v21', 'v112', 'weighted2last_fgm_512', 'attention_fgm', 'weightedmean2last_fgm_512', 'attention_fgm_768'] with score 0.44664\nDistance: 300.1116051774349 with 11 models ['v112', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'mean_attention_no_fgm', 'weighted2last_fgm_512', 'attention_fgm', 'weightedmean2last_fgm_512', 'attention_fgm_768'] with score 0.44664\nDistance: 300.1116051774349 with 11 models ['mean_attention_no_fgm', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'attention_fgm', 'weightedmean2last_fgm_512', 'attention_fgm_768'] with score 0.44664\nDistance: 300.1116051774349 with 11 models ['v2', 'weightedpool', 'attention_large_fgm', 'v21', 'v112', 'attention_fgm_512', 'mean_attention_no_fgm', 'weighted2last_fgm_512', 'attention_fgm', 'weightedmean2last_fgm_512', 'attention_fgm_768'] with score 0.44664\nDistance: 300.1116051774349 with 11 models ['attention_fgm', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'attention_fgm_768'] with score 0.44664\nDistance: 311.12959126728424 with 10 models ['attention_fgm_512', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'mean_attention_no_fgm', 'weighted2last_fgm_512', 'v112', 'weightedmean2last_fgm_512', 'attention_fgm'] with score 0.44658\nDistance: 311.12959126728424 with 11 models ['attention_fgm_768', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'attention_fgm'] with score 0.44664\nDistance: 311.12959126728424 with 10 models ['v21', 'weightedpool', 'attention_large_fgm', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'attention_fgm'] with score 0.44658\nDistance: 311.12959126728424 with 10 models ['weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'attention_fgm'] with score 0.44658\nDistance: 349.5947584609191 with 10 models ['weighted2last_fgm_512', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weightedmean2last_fgm_512', 'mean_attention_no_fgm', 'attention_fgm'] with score 0.44658\nDistance: 349.5947584609191 with 10 models ['weightedmean2last_fgm_512', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512', 'v112', 'weighted2last_fgm_512', 'mean_attention_no_fgm', 'attention_fgm'] with score 0.44658\n","output_type":"stream"}]},{"cell_type":"code","source":"get_ensemble_oof([weightedmean2last_fgm_512_CFG, attention_fgm_768_CFG], False, False)[0]","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:44:16.285332Z","iopub.execute_input":"2022-11-28T02:44:16.285839Z","iopub.status.idle":"2022-11-28T02:44:17.139381Z","shell.execute_reply.started":"2022-11-28T02:44:16.285798Z","shell.execute_reply":"2022-11-28T02:44:17.137536Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0.4501521204120645"},"metadata":{}}]},{"cell_type":"code","source":"# Distance: 435.14002636145824 with 6 models ['mean_attention_no_fgm', 'weightedpool', 'attention_large_fgm', 'v21', 'v2', 'attention_fgm_512'] with score 0.44673\n# Distance: 372.3051254451275 with 3 models ['attention_fgm', 'v112', 'weighted2last_fgm_512'] with score 0.44876\n# Distance: 300.1116051774349 with 2 models [weightedmean2last_fgm_512, attention_fgm_768] with score 0.45015","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:44:17.140958Z","iopub.execute_input":"2022-11-28T02:44:17.141367Z","iopub.status.idle":"2022-11-28T02:44:17.147626Z","shell.execute_reply.started":"2022-11-28T02:44:17.141328Z","shell.execute_reply":"2022-11-28T02:44:17.145593Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"group1 = [mean_attention_no_fgm_CFG, weightedpool_CFG, attention_large_fgm_CFG, v21_CFG, v2_CFG, attention_fgm_512_CFG]\ngroup2 = [attention_fgm_CFG, v112_CFG, weighted2last_fgm_512_CFG]\ngroup3 = [weightedmean2last_fgm_512_CFG, attention_fgm_768_CFG]","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:44:17.150110Z","iopub.execute_input":"2022-11-28T02:44:17.150679Z","iopub.status.idle":"2022-11-28T02:44:17.161040Z","shell.execute_reply.started":"2022-11-28T02:44:17.150639Z","shell.execute_reply":"2022-11-28T02:44:17.159518Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group1_oof_train = get_ensemble_oof(group1, True, False)\nlearners = [\n    Ridge(alpha=0.0125, tol=1e-4, fit_intercept=True,normalize=True,random_state=CFG.seed),\n]\ntrain_stacking(group1_oof_train, learners, save=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-28T02:44:17.163079Z","iopub.execute_input":"2022-11-28T02:44:17.163651Z","iopub.status.idle":"2022-11-28T02:44:19.758282Z","shell.execute_reply.started":"2022-11-28T02:44:17.163527Z","shell.execute_reply":"2022-11-28T02:44:19.757331Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"score: 0.44672897  scores: [0.4767729692779155, 0.4410762828440261, 0.4094831656155451, 0.4479551640355943, 0.46539906358105076, 0.4396871892407088]\n\nFold 1/15\nScore: 0.45870317724899023\n##################################################\n\nFold 2/15\nScore: 0.44234899305917225\n##################################################\n\nFold 3/15\nScore: 0.44105602887892426\n##################################################\n\nFold 4/15\nScore: 0.45117547480859604\n##################################################\n\nFold 5/15\nScore: 0.4406279962028159\n##################################################\n\nFold 6/15\nScore: 0.4527824419978515\n##################################################\n\nFold 7/15\nScore: 0.4361227610530469\n##################################################\n\nFold 8/15\nScore: 0.42857617867629977\n##################################################\n\nFold 9/15\nScore: 0.4556767472695966\n##################################################\n\nFold 10/15\nScore: 0.4548460597965906\n##################################################\n\nFold 11/15\nScore: 0.44205340620160166\n##################################################\n\nFold 12/15\nScore: 0.44865417027403215\n##################################################\n\nFold 13/15\nScore: 0.44841143468402217\n##################################################\n\nFold 14/15\nScore: 0.4471892115630039\n##################################################\n\nFold 15/15\nScore: 0.45151226426988794\n##################################################\nscore: 0.44692373  scores: [0.47647859338509235, 0.4417313637203762, 0.4101194701280258, 0.4484124738583593, 0.46549992458650474, 0.43930054622589454]\n","output_type":"stream"}]},{"cell_type":"code","source":"group2_oof_train = get_ensemble_oof(group2, True, False)\nlearners = [\n    Ridge(alpha=0.0125, fit_intercept=True,normalize=True,random_state=CFG.seed),\n]\ntrain_stacking(group2_oof_train, learners, save=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-28T02:44:19.759665Z","iopub.execute_input":"2022-11-28T02:44:19.760439Z","iopub.status.idle":"2022-11-28T02:44:21.379355Z","shell.execute_reply.started":"2022-11-28T02:44:19.760383Z","shell.execute_reply":"2022-11-28T02:44:21.378565Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"score: 0.44876347  scores: [0.47856353179776495, 0.4431374902723514, 0.41045984537779256, 0.4497649613139139, 0.46755120114985516, 0.44310376124802897]\n\nFold 1/15\nScore: 0.45855608189920255\n##################################################\n\nFold 2/15\nScore: 0.44299971585478737\n##################################################\n\nFold 3/15\nScore: 0.4443066022529081\n##################################################\n\nFold 4/15\nScore: 0.4589972675350678\n##################################################\n\nFold 5/15\nScore: 0.4464481348270668\n##################################################\n\nFold 6/15\nScore: 0.45771956344486564\n##################################################\n\nFold 7/15\nScore: 0.43957911463537075\n##################################################\n\nFold 8/15\nScore: 0.4314743963579654\n##################################################\n\nFold 9/15\nScore: 0.4540069431269629\n##################################################\n\nFold 10/15\nScore: 0.4587212889097298\n##################################################\n\nFold 11/15\nScore: 0.44282597149627745\n##################################################\n\nFold 12/15\nScore: 0.4501642215900566\n##################################################\n\nFold 13/15\nScore: 0.4479649504589645\n##################################################\n\nFold 14/15\nScore: 0.4504680258363389\n##################################################\n\nFold 15/15\nScore: 0.44916354900074634\n##################################################\nscore: 0.44916275  scores: [0.4789544571485829, 0.44354346989427323, 0.41094275065595576, 0.45030705833313506, 0.46759956907558053, 0.44362917964265947]\n","output_type":"stream"}]},{"cell_type":"code","source":"group3_oof_train = get_ensemble_oof(group3, True, False)\nlearners = [\n    Ridge(alpha=0.0125, fit_intercept=True,normalize=True,random_state=CFG.seed),\n]\ntrain_stacking(group3_oof_train, learners, save=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-28T02:44:21.383059Z","iopub.execute_input":"2022-11-28T02:44:21.383810Z","iopub.status.idle":"2022-11-28T02:44:22.611616Z","shell.execute_reply.started":"2022-11-28T02:44:21.383777Z","shell.execute_reply":"2022-11-28T02:44:22.608371Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"score: 0.45015212  scores: [0.4783895750702563, 0.4441981438590136, 0.4132218948555447, 0.45136487125318286, 0.4692148522457962, 0.4445233851885932]\n\nFold 1/15\nScore: 0.4636697304428999\n##################################################\n\nFold 2/15\nScore: 0.44333697577960907\n##################################################\n\nFold 3/15\nScore: 0.44585905406795673\n##################################################\n\nFold 4/15\nScore: 0.46042443017276163\n##################################################\n\nFold 5/15\nScore: 0.4449849373703035\n##################################################\n\nFold 6/15\nScore: 0.45775639148066166\n##################################################\n\nFold 7/15\nScore: 0.44134489833282525\n##################################################\n\nFold 8/15\nScore: 0.433098556562234\n##################################################\n\nFold 9/15\nScore: 0.4530484851798466\n##################################################\n\nFold 10/15\nScore: 0.4609800425472033\n##################################################\n\nFold 11/15\nScore: 0.4478382126592321\n##################################################\n\nFold 12/15\nScore: 0.45157602100803845\n##################################################\n\nFold 13/15\nScore: 0.44719472078169464\n##################################################\n\nFold 14/15\nScore: 0.45157289696448705\n##################################################\n\nFold 15/15\nScore: 0.45134966466312054\n##################################################\nscore: 0.45053908  scores: [0.47858809801518376, 0.444871002408305, 0.4138739793539962, 0.4515931907029785, 0.4695319124448371, 0.4447762918057766]\n","output_type":"stream"}]},{"cell_type":"code","source":"len(all_models)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T02:50:43.851488Z","iopub.execute_input":"2022-11-28T02:50:43.851861Z","iopub.status.idle":"2022-11-28T02:50:43.858874Z","shell.execute_reply.started":"2022-11-28T02:50:43.851829Z","shell.execute_reply":"2022-11-28T02:50:43.857500Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"code","source":"all_models = [\n    attention_large_fgm_CFG, weighted2last_fgm_512_CFG, weightedmean2last_fgm_512_CFG, attention_fgm_512_CFG, attention_fgm_768_CFG, \n    v21_CFG, v112_CFG, mean_attention_no_fgm_CFG, v2_CFG, weightedpool_CFG           \n]\n# optimal_weights = [\n#     0.353356890459364,\n#     0.1660777385159011,\n#     0.10247349823321557,\n#     0.05653710247349824,\n#     0.06007067137809188,\n#     0.049469964664310966,\n#     0.08127208480565372,\n#     0.06713780918727916,\n#     0.03886925795053004,\n#     0.02473498233215548\n# ]\nall_oof_train = get_ensemble_oof(all_models, True, False)\nuse_weighted_mean_prediction = True\n\n# if use_weighted_mean_prediction:\n#     for i, ftm_cfg in enumerate(all_models):\n#         print(ftm_cfg.path, optimal_weights[i])\n#         all_models[i].inference_weight = optimal_weights[i]\n\n#     for target in tqdm(['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']):\n#         for cfg in all_models:\n#             if f'pred_{target}' in all_oof_train.columns:\n#                 all_oof_train[f'pred_{target}'] += all_oof_train[cfg.name + '_' + f'pred_{target}'] * cfg.inference_weight\n#             else:\n#                 all_oof_train[f'pred_{target}'] = all_oof_train[cfg.name + '_' + f'pred_{target}'] * cfg.inference_weight\nlearners = [\n    Ridge(alpha=0.0125, fit_intercept=True,normalize=True,random_state=CFG.seed),\n]\ntrain_stacking(all_oof_train, learners, save=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-28T02:49:46.026664Z","iopub.execute_input":"2022-11-28T02:49:46.027007Z","iopub.status.idle":"2022-11-28T02:49:49.610569Z","shell.execute_reply.started":"2022-11-28T02:49:46.026980Z","shell.execute_reply":"2022-11-28T02:49:49.609567Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"score: 0.44640312  scores: [0.47580667377642244, 0.44090222277463526, 0.40901897711244045, 0.4477116762081165, 0.4649767232192434, 0.44000242876507945]\n\nFold 1/15\nScore: 0.45826494901098097\n##################################################\n\nFold 2/15\nScore: 0.4412887931729279\n##################################################\n\nFold 3/15\nScore: 0.4408872393412681\n##################################################\n\nFold 4/15\nScore: 0.4530449403370144\n##################################################\n\nFold 5/15\nScore: 0.4417738393814558\n##################################################\n\nFold 6/15\nScore: 0.4534210967506897\n##################################################\n\nFold 7/15\nScore: 0.43637034407650366\n##################################################\n\nFold 8/15\nScore: 0.42836569611584413\n##################################################\n\nFold 9/15\nScore: 0.45469522925896283\n##################################################\n\nFold 10/15\nScore: 0.4560703214451878\n##################################################\n\nFold 11/15\nScore: 0.4412050879093569\n##################################################\n\nFold 12/15\nScore: 0.44756478937328037\n##################################################\n\nFold 13/15\nScore: 0.4493275055574613\n##################################################\n\nFold 14/15\nScore: 0.44741723700280883\n##################################################\n\nFold 15/15\nScore: 0.45040455465719664\n##################################################\nscore: 0.44695341  scores: [0.47558939021517843, 0.4419541754143107, 0.4100323206538327, 0.44855413002907407, 0.4653941531803787, 0.44019628402284117]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## subset selection","metadata":{}},{"cell_type":"code","source":"#######################################\ndef get_ensemble_oof(cfgs):\n    oof_dfs = pd.DataFrame()\n    oof_train = pd.DataFrame()\n    total_weight = 0\n    for cfg in cfgs:\n        cfg.target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    \n        infer_ = Inferencer(cfg=cfg, inference_weight=cfg.inference_weight)\n\n        if cfg.path in [attention_fgm_CFG.path, attention_fgm_768_CFG.path, attention_fgm_512_CFG.path, attention_large_fgm_CFG.path, roberta_attention_fgm_CFG.path, weightedmean2last_fgm_512_CFG.path, weighted2last_fgm_512_CFG.path]:\n            file_type = 'csv'\n        else:\n            file_type = 'pkl'\n        oof_df = infer_.get_oof_df(file_type)\n        total_weight += infer_.cfg.inference_weight\n\n        #print(cfg.path)\n        #get_result(cfg, oof_df)\n        #print('\\n')\n\n        pred_cols = [f'pred_{col}' for col in ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']]\n        oof_df_copy = oof_df.copy()\n        oof_df[pred_cols] = oof_df[pred_cols] * infer_.cfg.inference_weight\n        oof_dfs = oof_dfs.append(oof_df)\n\n        oof_df = oof_df_copy.copy()\n        oof_df = oof_df[['text_id'] + pred_cols]\n        oof_df.columns = ['text_id'] + [cfg.name + '_' + col for col in pred_cols]\n\n        if len(oof_train) == 0:\n            oof_train = (\n                train\n                .merge(oof_df, on=['text_id'], how='left')\n                .drop(columns=['full_text']))\n        else:\n            oof_train = (\n                oof_train.merge(oof_df, on=['text_id'], how='left'))\n        del infer_; gc.collect()\n    \n    pred_cols = [f'pred_{col}' for col in ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']]\n    # oof_dfs_mean = oof_dfs.groupby('text_id')[pred_cols].mean()\n    oof_dfs_mean = oof_dfs.groupby('text_id')[pred_cols].sum() / total_weight\n    oof_dfs_mean = oof_dfs_mean.join(train.set_index('text_id'))\n    \n    return get_result(CFG, oof_dfs_mean, verbose=0), cfgs","metadata":{"execution":{"iopub.status.busy":"2022-11-25T17:05:25.436114Z","iopub.execute_input":"2022-11-25T17:05:25.436513Z","iopub.status.idle":"2022-11-25T17:05:25.452236Z","shell.execute_reply.started":"2022-11-25T17:05:25.436480Z","shell.execute_reply":"2022-11-25T17:05:25.451043Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"model_selection = False\n\nif model_selection:\n    fine_tuned_models_cfg = [\n        attention_fgm_CFG, \n        v21_CFG, \n        v112_CFG, \n        mean_attention_no_fgm_CFG, \n        weighted_attention_CFG, \n        attention_large_fgm_CFG, \n        attention_fgm_768_CFG, \n        attention_fgm_512_CFG,\n        weightedmean2last_fgm_512_CFG,\n        weighted2last_fgm_512_CFG,\n\n        v2_CFG, \n        v116_CFG, \n        weightedpool_CFG, \n        weighted_fgm_CFG\n    ]\n\n    for c in fine_tuned_models_cfg:\n        if len(c.trn_fold) < 4:\n            print(c.name)\n\n    for i, first_model in enumerate(tqdm(fine_tuned_models_cfg)):\n        features = [first_model]\n        prev_score,_ = get_ensemble_oof(features)\n        cur_score = 0\n        \n        while True:\n            models = [feat.name for feat in features]\n            if len(models) == len(fine_tuned_models_cfg):\n                break\n                \n            scores_and_features = [get_ensemble_oof(features + [feat]) for feat in fine_tuned_models_cfg if feat.name not in models]\n            scores = [s for s,c in scores_and_features]\n            cur_features = [c for s,c in scores_and_features]\n            cur_score = np.min(scores)\n            cur_best_feature = cur_features[np.argmin(scores)][-1]\n            features.append(cur_best_feature)\n            \n            if prev_score < cur_score:\n                break\n            prev_score = cur_score\n\n            del scores_and_features, scores, cur_best_feature, cur_features; gc.collect(); torch.cuda.empty_cache();        \n        \n        logger.info(f'Interation {i+1}:')\n        logger.info(f'model_set={[c.name for c in features]} \\nbest_score={cur_score}')\n        logger.info('#'*50)\n        logger.info('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-11-25T17:05:26.201919Z","iopub.execute_input":"2022-11-25T17:05:26.202565Z","iopub.status.idle":"2022-11-25T17:27:38.249634Z","shell.execute_reply.started":"2022-11-25T17:05:26.202509Z","shell.execute_reply":"2022-11-25T17:27:38.248851Z"},"scrolled":true,"trusted":true},"execution_count":131,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb91f4dd0cd94c47b2ecf65cf2641983"}},"metadata":{}},{"name":"stderr","text":"Interation 1:\nmodel_set=['attention_fgm', 'attention_large_fgm', 'v112', 'weightedmean2last_fgm_512', 'mean_attention_no_fgm', 'v2', 'weighted2last_fgm_512'] \nbest_score=0.4463336717953639\n##################################################\n\n\nInteration 2:\nmodel_set=['v21', 'attention_large_fgm', 'v112', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'v2', 'attention_fgm_512', 'v116'] \nbest_score=0.4462124748982587\n##################################################\n\n\nInteration 3:\nmodel_set=['v112', 'attention_large_fgm', 'weightedmean2last_fgm_512', 'mean_attention_no_fgm', 'v2', 'attention_fgm_768', 'v21'] \nbest_score=0.44612802576036886\n##################################################\n\n\nInteration 4:\nmodel_set=['mean_attention_no_fgm', 'attention_large_fgm', 'weightedmean2last_fgm_512', 'v112', 'v2', 'attention_fgm_768', 'v21'] \nbest_score=0.44612802576036886\n##################################################\n\n\nInteration 5:\nmodel_set=['weighted_attention', 'attention_large_fgm', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'v112', 'v2', 'attention_fgm_512', 'v21'] \nbest_score=0.44638075175867353\n##################################################\n\n\nInteration 6:\nmodel_set=['attention_large_fgm', 'weightedmean2last_fgm_512', 'mean_attention_no_fgm', 'v112', 'v2', 'attention_fgm_768', 'v21'] \nbest_score=0.44612802576036886\n##################################################\n\n\nInteration 7:\nmodel_set=['attention_fgm_768', 'attention_large_fgm', 'v112', 'mean_attention_no_fgm', 'v2', 'weightedmean2last_fgm_512', 'v21'] \nbest_score=0.44612802576036886\n##################################################\n\n\nInteration 8:\nmodel_set=['attention_fgm_512', 'attention_large_fgm', 'v112', 'mean_attention_no_fgm', 'v2', 'weightedmean2last_fgm_512', 'v21'] \nbest_score=0.44609399605554056\n##################################################\n\n\nInteration 9:\nmodel_set=['weightedmean2last_fgm_512', 'attention_large_fgm', 'mean_attention_no_fgm', 'v112', 'v2', 'attention_fgm_768', 'v21'] \nbest_score=0.44612802576036886\n##################################################\n\n\nInteration 10:\nmodel_set=['weighted2last_fgm_512', 'attention_large_fgm', 'v112', 'mean_attention_no_fgm', 'v2', 'attention_fgm_768'] \nbest_score=0.44606413119515453\n##################################################\n\n\nInteration 11:\nmodel_set=['v2', 'attention_large_fgm', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'v112', 'attention_fgm_768', 'v21'] \nbest_score=0.44612802576036886\n##################################################\n\n\nInteration 12:\nmodel_set=['v116', 'attention_large_fgm', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'v112', 'v2', 'attention_fgm_512', 'v21', 'attention_fgm_768'] \nbest_score=0.44630003592133244\n##################################################\n\n\nInteration 13:\nmodel_set=['weightedpool', 'attention_large_fgm', 'weightedmean2last_fgm_512', 'mean_attention_no_fgm', 'v112', 'v2', 'attention_fgm_512', 'attention_fgm_768'] \nbest_score=0.4462821341355358\n##################################################\n\n\nInteration 14:\nmodel_set=['weighted_fgm', 'attention_large_fgm', 'mean_attention_no_fgm', 'weightedmean2last_fgm_512', 'v112', 'v2', 'attention_fgm_512', 'attention_fgm_768'] \nbest_score=0.44640435763792025\n##################################################\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# model_set=['attention_fgm_512', 'attention_large_fgm', 'v112', 'mean_attention_no_fgm', 'v2', 'weightedmean2last_fgm_512', 'v21'] \n# best_score=0.44609399605554056\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-25T16:56:32.828056Z","iopub.execute_input":"2022-11-25T16:56:32.828503Z","iopub.status.idle":"2022-11-25T16:56:32.836068Z","shell.execute_reply.started":"2022-11-25T16:56:32.828464Z","shell.execute_reply":"2022-11-25T16:56:32.834984Z"},"trusted":true},"execution_count":126,"outputs":[{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"code","source":"# features_df = pd.read_csv('../input/fb3-feature-engineering/train_fe.csv')\n# feature_cols = [col for col in features_df.columns if col not in ['full_text', 'text_id'] + CFG.target_cols]\n\n# features = features_df.set_index('text_id')[feature_cols]\n\n# pred_cols = [col for col in oof_train.columns if col not in ['text_id', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions', 'fold']]\n# oof_train[pred_cols] = (oof_train[pred_cols] - oof_train[pred_cols].mean().values) / oof_train[pred_cols].std().values\n# oof_train = oof_train.set_index('text_id').join(features)\n\n# for col in oof_train.columns[8:]:\n#     oof_train[col] = (oof_train[col] - oof_train[col].mean())/(oof_train[col].std())\n#     #print(oof_train[col].mean(), oof_train[col].std())","metadata":{"execution":{"iopub.status.busy":"2022-11-25T14:55:05.288775Z","iopub.execute_input":"2022-11-25T14:55:05.289245Z","iopub.status.idle":"2022-11-25T14:55:05.295130Z","shell.execute_reply.started":"2022-11-25T14:55:05.289204Z","shell.execute_reply":"2022-11-25T14:55:05.293862Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-11-25T14:55:05.557719Z","iopub.execute_input":"2022-11-25T14:55:05.558187Z","iopub.status.idle":"2022-11-25T14:55:05.564106Z","shell.execute_reply.started":"2022-11-25T14:55:05.558139Z","shell.execute_reply":"2022-11-25T14:55:05.562699Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"oof_scores = []\noof_oof_train = pd.DataFrame()\ndef models_fit_predict(models, X_train, y_train, X_val, y_val, fold, save=False):\n    preds = []\n    for model in models:\n        model_name = type(model).__name__.lower()\n        if type(model) != LinearRegression:\n            model = MultiOutputRegressor(model)\n        model.fit(X_train, y_train)\n        if save:\n            #dump(model, f'{model_name}_strong_fold{fold}.model')\n            dump(model, f'{model_name}_weak_fold{fold}.model')\n        preds.append(model.predict(X_val))\n        #for estimator in model.estimators_:\n        #    print(estimator.coef_)\n    return np.mean(preds, axis=0)\n\nfor fold in range(CFG.n_fold):\n    print(f'\\nFold {fold+1}/{CFG.n_fold}')\n    \n    X_train = oof_train[oof_train['fold']!=fold][oof_train.columns[8:]].values\n    #assert X_train.shape[1] == len(fine_tuned_models_cfg)*6 + 6\n    y_train = oof_train[oof_train['fold']!=fold][CFG.target_cols].values\n    X_val = oof_train[oof_train['fold']==fold][oof_train.columns[8:]].values\n    y_val = oof_train[oof_train['fold']==fold][CFG.target_cols].values\n    \n    pred_val = models_fit_predict(\n        [\n            Ridge(alpha=48.0, random_state=CFG.seed), \n            #BayesianRidge(),\n            #Lasso(alpha=1.0, random_state=CFG.seed),\n            LinearRegression(normalize=True, positive=True),\n            #SVR(kernel='linear', gamma='auto'),\n            #LGBMRegressor(max_depth=24, random_state=42, learning_rate=0.01),\n        ],\n        X_train, y_train, X_val, y_val,\n        fold, save=False,\n    )\n    \n    val_fold = oof_train[oof_train['fold']==fold].reset_index(drop=True)\n    val_fold[[f'pred_{c}' for c in CFG.target_cols]] = pred_val\n    \n    oof_oof_train = pd.concat([oof_oof_train, val_fold])\n\n    oof_score, _ = mc_rmse(y_val, pred_val)\n    oof_scores.append(oof_score)\n    print(f'Score: {oof_score}')\n    print('#'*50)\n\nget_result(CFG, oof_oof_train)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-26T12:22:06.456969Z","iopub.execute_input":"2022-11-26T12:22:06.458449Z","iopub.status.idle":"2022-11-26T12:22:06.960275Z","shell.execute_reply.started":"2022-11-26T12:22:06.458385Z","shell.execute_reply":"2022-11-26T12:22:06.959005Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\nFold 1/4\nScore: 0.44758374702096937\n##################################################\n\nFold 2/4\nScore: 0.45311470919605995\n##################################################\n\nFold 3/4\nScore: 0.45800629488427996\n##################################################\n\nFold 4/4\nScore: 0.4432123703793988\n##################################################\nscore: 0.45053400  scores: [0.4787650786478345, 0.44443982088474465, 0.4130861316193527, 0.4520671715257306, 0.47022854514651, 0.4446172264203405]\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0.4505339957074188"},"metadata":{}}]},{"cell_type":"code","source":"from datetime import datetime\nwith open('info.txt', 'w') as f:\n    f.write(f'{datetime.today().strftime(\"%Y%m%d\")}-{np.mean(oof_scores)}')","metadata":{"execution":{"iopub.status.busy":"2022-11-24T08:52:47.074781Z","iopub.execute_input":"2022-11-24T08:52:47.075174Z","iopub.status.idle":"2022-11-24T08:52:47.081908Z","shell.execute_reply.started":"2022-11-24T08:52:47.075137Z","shell.execute_reply":"2022-11-24T08:52:47.080650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for f in os.listdir('./'):\n#     if '.model' in f:\n#         os.remove(f'./{f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# embedding stacking","metadata":{}},{"cell_type":"code","source":"# from sklearn.linear_model import BayesianRidge, LinearRegression\n# from sklearn.multioutput import MultiOutputRegressor","metadata":{"execution":{"iopub.status.busy":"2022-11-23T05:39:25.751833Z","iopub.execute_input":"2022-11-23T05:39:25.752215Z","iopub.status.idle":"2022-11-23T05:39:25.757051Z","shell.execute_reply.started":"2022-11-23T05:39:25.752181Z","shell.execute_reply":"2022-11-23T05:39:25.755664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fine_tuned_models_cfg = [v112_CFG, v21_CFG, weighted_attention_CFG, mean_attention_no_fgm_CFG, attention_fgm_CFG, attention_fgm_768_CFG]","metadata":{"execution":{"iopub.status.busy":"2022-11-23T05:16:35.555478Z","iopub.execute_input":"2022-11-23T05:16:35.556185Z","iopub.status.idle":"2022-11-23T05:16:35.561186Z","shell.execute_reply.started":"2022-11-23T05:16:35.556148Z","shell.execute_reply":"2022-11-23T05:16:35.560251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_embedding(df, cfg, fold):\n#     inferencer = Inferencer(cfg=cfg, inference_weight=cfg.inference_weight)\n#     train_dataset = TestDataset(cfg, df)\n#     train_loader = DataLoader(\n#             train_dataset,\n#             batch_size=12,\n#             shuffle=False,\n#             collate_fn=DataCollatorWithPadding(tokenizer=cfg.tokenizer, padding='longest'),\n#             num_workers=2, \n#             pin_memory=True, \n#             drop_last=False)\n#     embedding = inferencer.get_text_embedding(train_loader, device, fold=fold)\n#     return embedding\n\n# oof_df = train.copy()\n# for fold in range(4):\n#     print(f'fold={fold}')\n#     train_folds = train[train['fold'] != fold]\n#     train_labels = train_folds[fine_tuned_models_cfg[-1].target_cols].values \n#     train_idx = train_folds.index\n    \n#     valid_folds = train[train['fold'] == fold]\n#     valid_labels = valid_folds[fine_tuned_models_cfg[-1].target_cols].values \n#     valid_idx = valid_folds.index\n    \n#     fine_tuned_embeddings = []\n#     for cfg in tqdm(fine_tuned_models_cfg):\n#         embedding = get_embedding(train, cfg, fold)        \n#         #print(embedding.std(), embedding.mean())\n#         fine_tuned_embeddings.append(embedding)\n#         print(cfg.name, f'embedding_{cfg.name}_{embedding.shape[1]}.npy')\n        \n#         with open(f'./embedding_{cfg.name}_{embedding.shape[1]}.npy', 'wb') as f:\n#             np.save(f, embedding)\n        \n#         del embedding; gc.collect(); torch.cuda.empty_cache()\n        \n#     fine_tuned_embeddings = np.concatenate(fine_tuned_embeddings, axis=1)\n#     train_embeddings = fine_tuned_embeddings[train_idx, :]\n#     valid_embeddings = fine_tuned_embeddings[val_idx, :]\n    \n#     model_ridge = MultiOutputRegressor(BayesianRidge())\n#     model_ridge.fit(train_embeddings, train_labels)\n#     ridge_val_preds = model_ridge.predict(valid_embeddings)\n#     dump(model_ridge, f'ridge_embed_stacking_{fold}.model')\n#     score, _ = mc_rmse(valid_labels, ridge_val_preds)\n    \n#     oof_df.loc[val_idx, ['pred_' + target_col for target_col in fine_tuned_models_cfg[-1].target_cols]] = ridge_val_preds    \n#     print(f'fold:{fold} - score:{score}')\n\n# get_result(cfg, oof_df)","metadata":{"execution":{"iopub.status.busy":"2022-11-23T05:17:01.334605Z","iopub.execute_input":"2022-11-23T05:17:01.335236Z","iopub.status.idle":"2022-11-23T05:33:01.160919Z","shell.execute_reply.started":"2022-11-23T05:17:01.335200Z","shell.execute_reply":"2022-11-23T05:33:01.159238Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# oof train - groups","metadata":{}},{"cell_type":"code","source":"group1 =  [v21, v112, mean_attention_no_fgm, v2, weightedpool]\ngroup2 = []\n# [weighted2last_fgm_512, weightedmean2last_fgm_512] - [attention_fgm_512, attention_fgm_768] - [attention_large_fgm, v21, v112, mean_attention_no_fgm, v2, weightedpool]","metadata":{},"execution_count":null,"outputs":[]}]}