{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math \nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython. display import clear_output\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\n#os.system('pip install iterative-stratification==0.1.7')\n#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nsys.path.append('../input/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nos.system('pip uninstall -y transformers')\nos.system('pip uninstall -y tokenizers')\nos.system('python -m pip install --no-index --find-links=../input/fb3-my-pip-wheels transformers')\nos.system('python -m pip install --no-index --find-links=../input/fb3-my-pip-wheels tokenizers')\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom transformers import DataCollatorWithPadding\n%env TOKENIZERS_PARALLELISM=false\n\nfrom nltk import sent_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nclear_output()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nprint('device:', device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-27T16:13:25.897483Z","iopub.execute_input":"2022-11-27T16:13:25.898436Z","iopub.status.idle":"2022-11-27T16:13:58.603892Z","shell.execute_reply.started":"2022-11-27T16:13:25.898343Z","shell.execute_reply":"2022-11-27T16:13:58.602851Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"tokenizers.__version__: 0.12.1\ntransformers.__version__: 4.20.1\ndevice: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/feedback-prize-english-language-learning'\nSUBMISSION_PATH = os.path.join(BASE_PATH, 'sample_submission.csv')\nTRAIN_PATH = os.path.join(BASE_PATH, 'train.csv')\nTEST_PATH = os.path.join(BASE_PATH, 'test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:13:58.605690Z","iopub.execute_input":"2022-11-27T16:13:58.607025Z","iopub.status.idle":"2022-11-27T16:13:58.613292Z","shell.execute_reply.started":"2022-11-27T16:13:58.606986Z","shell.execute_reply":"2022-11-27T16:13:58.611800Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Config(dict):\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n    \n    def init(self, kwargs):\n        super().init(kwargs)\n\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n\n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n        \ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.propagate = False\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:13:58.614650Z","iopub.execute_input":"2022-11-27T16:13:58.615226Z","iopub.status.idle":"2022-11-27T16:13:58.631318Z","shell.execute_reply.started":"2022-11-27T16:13:58.615190Z","shell.execute_reply":"2022-11-27T16:13:58.630373Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    '''\n    Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.\n    '''\n    random.seed(seed)\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # When running on the CuDNN backend, two further options must be set\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\nseed_everything(seed=42)\n\ndef mc_rmse(y_true, y_pred):\n    scores = []\n    ncols = y_true.shape[1]\n    \n    for n in range(ncols):\n        yn_true = y_true[:, n]\n        yn_pred = y_pred[:, n]\n        rmse_ = mean_squared_error(yn_true, yn_pred, squared=False)\n        scores.append(rmse_)\n    score = np.mean(scores) \n    return score, scores\n\ndef get_result(cfg, oof_df):\n    labels = oof_df[cfg.target_cols].values\n    preds = oof_df[[f\"pred_{c}\" for c in cfg.target_cols]].values\n    score, scores = mc_rmse(labels, preds)\n    print(f'score: {score:<.6f}  scores: {scores}')","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:13:58.633966Z","iopub.execute_input":"2022-11-27T16:13:58.634345Z","iopub.status.idle":"2022-11-27T16:13:58.646594Z","shell.execute_reply.started":"2022-11-27T16:13:58.634313Z","shell.execute_reply":"2022-11-27T16:13:58.645737Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\ndef encode_text(cfg, text):\n    if cfg.pretrained:\n        inputs = cfg.tokenizer(\n            text,\n            None,\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=cfg.max_len,\n            return_tensors='pt'\n        )\n        inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n    else:\n        inputs = cfg.tokenizer.encode_plus(\n            text, \n            return_tensors=None, \n            add_special_tokens=True, \n            #max_length=CFG.max_len,\n            #pad_to_max_length=True,\n            #truncation=True\n        )\n        for k, v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs \n\ndef preprocess(texts):\n    texts = (\n        texts\n        .str.replace(r'\\r\\n', '<newline>', regex=True)\n        .str.replace(r'\\n', '<newline>', regex=True)\n        .str.replace('<newline><newline>', '<newline>', regex=False)\n        .values \n    )\n    return texts\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        if cfg.pretrained:\n            self.texts = df['full_text'].values\n        else:\n            self.texts = preprocess(df['full_text'])\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = encode_text(self.cfg, self.texts[item])\n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:13:58.647791Z","iopub.execute_input":"2022-11-27T16:13:58.648660Z","iopub.status.idle":"2022-11-27T16:13:58.659770Z","shell.execute_reply.started":"2022-11-27T16:13:58.648598Z","shell.execute_reply":"2022-11-27T16:13:58.658741Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def load_config(input_path, inference_weight=1):\n    # Load CFG class.\n    cfg = Config(**json.load(open(os.path.join(input_path, 'CFG.json'), 'r')))\n    cfg.path = input_path\n    cfg.config_path = os.path.join(cfg.path, 'config.pth')\n    # Load tokenizer.\n    tokenizer = AutoTokenizer.from_pretrained(os.path.join(cfg.path, 'tokenizer'))\n    cfg.tokenizer = tokenizer\n    \n    cfg.inference_weight = inference_weight\n    return cfg\n\ndef load_model(cfg, fold, **model_kwargs):\n    # Load torch model.\n    model = FB3Model(cfg, config_path=cfg.config_path, pretrained=False, **model_kwargs)\n    state = torch.load(\n        os.path.join(cfg.path, f\"{cfg.model.replace('/', '-')}_fold{fold}_best.pth\"),\n        map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:13:58.661447Z","iopub.execute_input":"2022-11-27T16:13:58.661785Z","iopub.status.idle":"2022-11-27T16:13:58.673084Z","shell.execute_reply.started":"2022-11-27T16:13:58.661752Z","shell.execute_reply":"2022-11-27T16:13:58.672035Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output.last_hidden_state.detach().cpu()\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    #tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in test_loader:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\n\nclass Inferencer:\n    def __init__(self, input_path=None, cfg=None, inference_weight=1):\n        if cfg == None:\n            self.cfg = load_config(input_path, inference_weight)\n        else:\n            self.cfg = cfg\n    \n    def predict(self, test_loader, device, stat_fn=np.mean):\n        preds = []\n        start = time.time()\n        print('#'*10, cfg.path, '#'*10)\n        for fold in self.cfg.trn_fold:\n            print(f'Predicting fold {fold}...')\n            model = load_model(self.cfg, fold, pool=self.cfg.pool_head)\n            pred = inference_fn(test_loader, model, device)\n            preds.append(pred)\n            del model, pred; gc.collect()\n            torch.cuda.empty_cache()\n        end = time.time() - start\n        print('#'*10, f'ETA: {end:.2f}s', '#'*10, '\\n')\n        \n        self.preds = stat_fn(preds, axis=0) \n        self.preds = np.clip(self.preds, 1, 5)\n        return self.preds\n    \n    def get_oof_result(self):\n        return get_result(pd.read_pickle(os.path.join(cfg.path, 'oof_df.pkl')))\n    \n    def get_text_embedding(self, data_loader, device, fold=None): \n        # pretrained=True: not fine-tuned models.\n        if not self.cfg.pretrained:\n            model = load_model(self.cfg, fold, pool=self.cfg.pool_head)            \n        else:\n            model = AutoModel.from_pretrained(self.cfg.model)\n        model.to(device)\n        model.eval()\n            \n        fold_emb = []\n        for inputs in data_loader:\n            for k, v in inputs.items():\n                inputs[k] = v.to(device)\n            if not self.cfg.pretrained:\n                with torch.no_grad():\n                    emb = model.feature(**inputs)\n            else:\n                input_ids = inputs['input_ids'].to(device)\n                attention_mask = inputs['attention_mask'].to(device)\n                token_type_ids = inputs['token_type_ids'].to(device)\n                \n                with torch.no_grad():\n                    output = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                emb = mean_pooling(output, attention_mask.detach().cpu())\n                emb = F.normalize(emb, p=2, dim=1)\n                emb = emb.squeeze(0)\n            fold_emb.extend(emb.detach().cpu().numpy())\n            del emb; gc.collect(); torch.cuda.empty_cache();\n            #print(torch.cuda.memory_allocated() /1024/1024)\n            \n        fold_emb = np.array(fold_emb)\n        return fold_emb","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:13:58.674453Z","iopub.execute_input":"2022-11-27T16:13:58.674958Z","iopub.status.idle":"2022-11-27T16:13:58.694646Z","shell.execute_reply.started":"2022-11-27T16:13:58.674925Z","shell.execute_reply":"2022-11-27T16:13:58.693767Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# SVR","metadata":{}},{"cell_type":"markdown","source":"## config","metadata":{}},{"cell_type":"code","source":"##################################################\ndeberta_base = Config(\n    model='../input/huggingface-deberta-variants/deberta-base/deberta-base',\n    file_name='microsoft_deberta_base_768',\n    pretrained=True, inference_weight=1, max_len=1024) #\ndeberta_large = Config(\n    model='../input/huggingface-deberta-variants/deberta-large/deberta-large', \n    file_name='microsoft_deberta_large_1024',\n    pretrained=True, inference_weight=1, max_len=1024) #\ndeberta_xlarge = Config(\n    model='../input/huggingface-deberta-variants/deberta-xlarge/deberta-xlarge', \n    file_name='microsoft_deberta_xlarge_1024',\n    pretrained=True, inference_weight=1, max_len=1024)\ndeberta_v2_xlarge = Config(\n    model='../input/bert-shopping-mall/deberta-v2-xlarge', \n    file_name='microsoft_deberta_v2_xlarge_1536',\n    pretrained=True, inference_weight=1, max_len=1024)\ndeberta_v2_xxlarge = Config(\n    model='../input/bert-shopping-mall/deberta-v2-xxlarge', \n    file_name='microsoft_deberta_v2_xxlarge_1536',\n    pretrained=True, inference_weight=1, max_len=1024)\n\ndeberta_v3_base = Config(\n    model='../input/bert-shopping-mall/deberta-v3-base',\n    file_name='microsoft_deberta_v3_base_768',\n    pretrained=True, inference_weight=1, max_len=1024) #\ndeberta_v3_large = Config(\n    model='../input/bert-shopping-mall/deberta-v3-large', \n    file_name='microsoft_deberta_v3_large_1024',\n    pretrained=True, inference_weight=1, max_len=1024) # \n\ndeberta_large_mnli = Config(\n    model='../input/huggingface-deberta-variants/deberta-large-mnli/deberta-large-mnli',\n    file_name='microsoft_deberta_large_mnli_1024',\n    pretrained=True, inference_weight=1, max_len=1024) # \n\ngpt2 = Config(\n    model='../input/hugging-face-gpt2/gpt2',\n    file_name='gpt2_768',\n    pretrained=True, inference_weight=1, max_len=512) #\n\nroberta_base = Config(\n    model='../input/transformers/roberta-base', \n    file_name='roberta_base_768',\n    pretrained=True, inference_weight=1, max_len=512) #\nroberta_large = Config(\n    model='../input/transformers/roberta-large',\n    file_name='roberta_large_1024',\n    pretrained=True, inference_weight=1, max_len=512) # \n\nxlnet_base = Config(\n    model='../input/transformers/xlnet-base-cased',\n    file_name='xlnet_base_cased_768',\n    pretrained=True, inference_weight=1, max_len=1024) #\nxlnet_large = Config(\n    model='../input/transformers/xlnet-large-cased', \n    file_name='xlnet_large_cased_1024',\n    pretrained=True, inference_weight=1, max_len=1024) #\n\nbart_base = Config(\n    model='../input/transformers/facebook-bart-base',\n    file_name='facebook_bart_base_768',\n    pretrained=True, inference_weight=1, max_len=1024)\nbart_large = Config(\n    model='../input/transformers/facebook-bart-large',\n    file_name='facebook_bart_large_1024',\n    pretrained=True, inference_weight=1, max_len=1024)\nbart_large_mnli = Config(\n    model='../input/facebook-bart-large-mnli',\n    file_name='facebook_bart_large_mnli_1024',\n    pretrained=True, inference_weight=1, max_len=1024)\n\nbert_base_uncased = Config(\n    model='../input/transformers/bert-base-uncased',\n    file_name='bert_base_uncased_768',\n    pretrained=True, inference_weight=1, max_len=512)\nbert_large_uncased = Config(\n    model='../input/transformers/bert-large-uncased',\n    file_name='bert_large_uncased_1024',\n    pretrained=True, inference_weight=1, max_len=512)\n\nmuppet_roberta_large = Config(\n    model='../input/muppet-roberta-large',\n    file_name='facebook_muppet_roberta_large_1024',\n    pretrained=True, inference_weight=1, max_len=512)\n# muppet_roberta_base = Config(model='facebook/muppet-roberta-base', pretrained=True, inference_weight=1, max_len=512)\n\nfunnel_small = Config(\n    model='../input/transformers/funnel-transformer-small',\n    file_name='funnel_transformer_small_768',\n    pretrained=True, inference_weight=1, max_len=1024)\nfunnel_large = Config(\n    model='../input/transformers/funnel-transformer-large',\n    file_name='funnel_transformer_large_1024',\n    pretrained=True, inference_weight=1, max_len=1024)\n\n##################################################\n\ntarget_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:13:58.695931Z","iopub.execute_input":"2022-11-27T16:13:58.696761Z","iopub.status.idle":"2022-11-27T16:13:58.712755Z","shell.execute_reply.started":"2022-11-27T16:13:58.696724Z","shell.execute_reply":"2022-11-27T16:13:58.711892Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## load embeddings","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import make_scorer\nfrom joblib import dump, load\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.linear_model import RidgeCV, Ridge, Lasso, BayesianRidge, LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\nif str(device) == 'cpu':\n    from sklearn.svm import SVR\nelse:\n    from cuml.svm import SVR\n    import cuml\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:13:58.713822Z","iopub.execute_input":"2022-11-27T16:13:58.714909Z","iopub.status.idle":"2022-11-27T16:14:02.217474Z","shell.execute_reply.started":"2022-11-27T16:13:58.714882Z","shell.execute_reply":"2022-11-27T16:14:02.216432Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"train = pd.read_csv(TRAIN_PATH)\nfeature_names = pd.read_csv('../input/fb3-feature-engineering/train_fe_mean_std.csv')['feature'].tolist()\ntrain_fe = pd.read_csv('../input/fb3-feature-engineering/train_fe.csv')\ntest = pd.read_csv(TEST_PATH)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:14:02.222443Z","iopub.execute_input":"2022-11-27T16:14:02.224785Z","iopub.status.idle":"2022-11-27T16:14:02.757440Z","shell.execute_reply.started":"2022-11-27T16:14:02.224746Z","shell.execute_reply":"2022-11-27T16:14:02.756504Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nsvr_folds = 15\n\nskf = MultilabelStratifiedKFold(n_splits=svr_folds, shuffle=True, random_state=42)\nfor i,(train_index, val_index) in enumerate(skf.split(train,train[target_cols])):\n    train.loc[val_index,'fold'] = i","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-27T16:14:02.758969Z","iopub.execute_input":"2022-11-27T16:14:02.759323Z","iopub.status.idle":"2022-11-27T16:14:02.918317Z","shell.execute_reply.started":"2022-11-27T16:14:02.759287Z","shell.execute_reply":"2022-11-27T16:14:02.917322Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train['fold'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:14:02.920640Z","iopub.execute_input":"2022-11-27T16:14:02.921103Z","iopub.status.idle":"2022-11-27T16:14:02.934515Z","shell.execute_reply.started":"2022-11-27T16:14:02.921066Z","shell.execute_reply":"2022-11-27T16:14:02.933562Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"14.0    261\n9.0     261\n5.0     261\n12.0    261\n10.0    261\n8.0     261\n0.0     261\n6.0     261\n2.0     261\n1.0     261\n11.0    261\n4.0     260\n7.0     260\n3.0     260\n13.0    260\nName: fold, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from glob import glob \n\ndef get_text_embedding(cfg, dfs):\n    cfg.tokenizer = AutoTokenizer.from_pretrained(cfg.model)\n    infer_ = Inferencer(cfg=cfg, inference_weight=cfg.inference_weight)\n    if cfg.model == 'gpt2':\n        cfg.tokenizer.pad_token = cfg.tokenizer.eos_token\n    text_embs = []\n    for df in dfs:\n        dataset = TestDataset(cfg, df)\n        loader = DataLoader(\n            dataset,\n            batch_size=4,\n            shuffle=False)\n\n        # Text embedding for SVM\n        test_text_emb = []\n        if not cfg.pretrained:\n            for fold in infer_.cfg.trn_fold:\n                test_text_emb.append(infer_.get_text_embedding(loader, device, fold))\n            text_emb = np.mean(text_emb, axis=0)\n        else:\n            text_emb = infer_.get_text_embedding(loader, device)\n        text_embs.append(text_emb)\n        del dataset, loader; gc.collect(); torch.cuda.empty_cache();\n    del infer_; gc.collect(); torch.cuda.empty_cache();\n    return text_embs\n\ndef learner_cv(features, learner, folds=15, save=False, verbose=False):\n    scores = []\n    for fold in range(folds):\n        dftr_ = train[train['fold']!=fold]\n        dfev_ = train[train['fold']==fold]\n\n        tr_text_feats = features[list(dftr_.index),:]\n        ev_text_feats = features[list(dfev_.index),:]\n        #print(f'Number of features: {len(tr_text_feats)}')\n\n        # clf = MultiOutputRegressor(SVR(C=2.0))\n        clf = MultiOutputRegressor(learner)\n        clf.fit(tr_text_feats, dftr_[target_cols].values)\n        ev_preds = clf.predict(ev_text_feats)\n\n        score,_ = mc_rmse(dfev_[target_cols].values, ev_preds)\n        scores.append(score)\n\n        if verbose:\n            print('#'*25)\n            print('### Fold',fold+1)\n            print(\"Score: {}\".format(score))\n        if save:\n            dump(clf, f'svr_{fold}.model')\n\n    # print('#'*25)\n    # print('Overall CV =', np.mean(scores))\n    return np.mean(scores)\n\ndef get_learner_score(models_cfg, learner, folds=5, manual_features=None, save=False, verbose=False):\n    for i, model_cfg in enumerate(models_cfg):\n        model_name = model_cfg.model.split('/')[-1].replace('-', '_')\n        models_cfg[i].model_name = model_name\n        model_file = f'../input/fb3embeddings/train_text_emb_{model_cfg.file_name}.npy'\n        if 'embedding' in model_cfg:\n            continue\n        with open(model_file, 'rb') as f:\n            models_cfg[i].embedding = np.load(f)   \n    embeddings = np.concatenate(\n        [model_cfg.embedding for model_cfg in models_cfg],\n        axis=1)\n    if type(manual_features) != type(None):\n        embeddings = np.concatenate(\n            [embeddings, manual_features],\n            axis=1)\n    #print(embeddings.shape)\n    svr_score = learner_cv(embeddings, learner, folds=folds, save=save, verbose=verbose)\n    #print('\\n')\n    print(f'model_set={[m.model_name for m in models_cfg]};   score={svr_score}')\n    return svr_score, models_cfg","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:14:02.935901Z","iopub.execute_input":"2022-11-27T16:14:02.936405Z","iopub.status.idle":"2022-11-27T16:14:02.953566Z","shell.execute_reply.started":"2022-11-27T16:14:02.936370Z","shell.execute_reply":"2022-11-27T16:14:02.952624Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# models selection","metadata":{}},{"cell_type":"code","source":"model_selection = False\n\npretrained_models_cfg = [\n    deberta_large_mnli,\n    gpt2,\n    roberta_base,\n    roberta_large,\n    xlnet_base, \n    xlnet_large,\n    deberta_base, \n    deberta_large, \n    deberta_xlarge,\n    deberta_v2_xlarge, \n    deberta_v2_xxlarge,\n    deberta_v3_base, \n    deberta_v3_large,\n    \n    bart_base,\n    bart_large,\n    bart_large_mnli,\n    bert_base_uncased,\n    bert_large_uncased,\n    muppet_roberta_large,\n    funnel_small,\n    funnel_large\n]","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:14:02.954916Z","iopub.execute_input":"2022-11-27T16:14:02.955439Z","iopub.status.idle":"2022-11-27T16:14:02.965507Z","shell.execute_reply.started":"2022-11-27T16:14:02.955360Z","shell.execute_reply":"2022-11-27T16:14:02.964590Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"pretrained_models_cfg_score = []\nfor cfg in pretrained_models_cfg:\n    score, c = get_learner_score([cfg], Ridge(alpha=4.4), folds=4, save=False)\n    pretrained_models_cfg_score.append((score, c[0]))","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:14:02.966730Z","iopub.execute_input":"2022-11-27T16:14:02.967360Z","iopub.status.idle":"2022-11-27T16:15:14.109520Z","shell.execute_reply.started":"2022-11-27T16:14:02.967326Z","shell.execute_reply":"2022-11-27T16:15:14.108266Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"model_set=['deberta_large_mnli'];   score=0.4780494349953554\nmodel_set=['gpt2'];   score=0.6488122378338913\nmodel_set=['roberta_base'];   score=0.5148243161379941\nmodel_set=['roberta_large'];   score=0.5823614424366776\nmodel_set=['xlnet_base_cased'];   score=0.6012977007579187\nmodel_set=['xlnet_large_cased'];   score=0.6198186255238233\nmodel_set=['deberta_base'];   score=0.5119876822876401\nmodel_set=['deberta_large'];   score=0.4738268967405775\nmodel_set=['deberta_xlarge'];   score=0.4828337051123891\nmodel_set=['deberta_v2_xlarge'];   score=0.5163828322242732\nmodel_set=['deberta_v2_xxlarge'];   score=0.5271556434448376\nmodel_set=['deberta_v3_base'];   score=0.47566183177648963\nmodel_set=['deberta_v3_large'];   score=0.47098341043780884\nmodel_set=['facebook_bart_base'];   score=0.47838965248719434\nmodel_set=['facebook_bart_large'];   score=0.4801957967788051\nmodel_set=['facebook_bart_large_mnli'];   score=0.47818931986075836\nmodel_set=['bert_base_uncased'];   score=0.4867411715776929\nmodel_set=['bert_large_uncased'];   score=0.48641239801356945\nmodel_set=['muppet_roberta_large'];   score=0.49887172109474587\nmodel_set=['funnel_transformer_small'];   score=0.48915132528927924\nmodel_set=['funnel_transformer_large'];   score=0.5010560667363873\n","output_type":"stream"}]},{"cell_type":"code","source":"pretrained_models_cfg_score = sorted(pretrained_models_cfg_score, key=lambda tup:tup[0], reverse=False)\npretrained_models_cfg = [c for score, c in pretrained_models_cfg_score]","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:15:14.115394Z","iopub.execute_input":"2022-11-27T16:15:14.118497Z","iopub.status.idle":"2022-11-27T16:15:14.132250Z","shell.execute_reply.started":"2022-11-27T16:15:14.118440Z","shell.execute_reply":"2022-11-27T16:15:14.130506Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"if model_selection:\n    for i, first_model in enumerate(tqdm(pretrained_models_cfg)):\n        features = [first_model]\n        prev_score,_ = get_learner_score(features, Ridge(alpha=4.4), folds=4, save=False)\n        cur_score = 0\n        \n        while True:\n            models = [feat.model for feat in features]\n            if len(models) == len(pretrained_models_cfg):\n                break\n                \n            scores_and_cfgs = [get_learner_score(features + [feat], Ridge(alpha=4.4), folds=4, save=False) for feat in pretrained_models_cfg if feat.model not in models]\n            scores = [s for s,c in scores_and_cfgs]\n            cur_features = [c for s,c in scores_and_cfgs]\n            \n            cur_score = np.min(scores)\n            cur_best_feature = cur_features[np.argmin(scores)][-1]\n            features.append(cur_best_feature)\n            \n            if prev_score < cur_score:\n                break\n            prev_score = cur_score\n\n            del scores_and_cfgs, scores, cur_best_feature, cur_features; gc.collect(); torch.cuda.empty_cache();\n\n        #BEST SCORE:0.4483291121589826\n        #MODELS SET: ['deberta_large_mnli', 'deberta_v3_large', 'deberta_v3_base', 'roberta_large', 'deberta_v3_large', 'deberta_v2_xlarge', 'deberta_v3_large', 'deberta_v2_xlarge', 'deberta_v3_large', 'deberta_v2_xlarge', 'roberta_large', 'deberta_v3_large', 'deberta_v2_xlarge']\n        \n        LOGGER.info(f'Interation {i+1}:')\n        LOGGER.info(f'model_set={[c.model_name for c in features]} \\nbest_score={cur_score}')\n        LOGGER.info('#'*50)\n        LOGGER.info('\\n')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-27T16:15:14.139642Z","iopub.execute_input":"2022-11-27T16:15:14.144372Z","iopub.status.idle":"2022-11-27T16:15:14.177661Z","shell.execute_reply.started":"2022-11-27T16:15:14.144321Z","shell.execute_reply":"2022-11-27T16:15:14.176222Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_set=['deberta_large_mnli', 'deberta_v3_large', 'facebook_bart_large', 'deberta_v3_base', 'muppet_roberta_large', 'funnel_transformer_small', 'roberta_base', '', 'funnel_transformer_large', 'gpt2', 'deberta_v2_xxlarge', 'roberta_large', 'xlnet_base_cased', 'xlnet_large_cased'] \n# best_score=0.45262457850522697","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:15:14.183305Z","iopub.execute_input":"2022-11-27T16:15:14.185337Z","iopub.status.idle":"2022-11-27T16:15:14.197453Z","shell.execute_reply.started":"2022-11-27T16:15:14.185301Z","shell.execute_reply":"2022-11-27T16:15:14.196179Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# fit & save svr","metadata":{}},{"cell_type":"code","source":"pretrained_models_cfg = [\n    deberta_large_mnli,\n    roberta_base,\n    roberta_large,\n    #xlnet_base, \n    #xlnet_large,\n    deberta_base, \n    deberta_large, \n    deberta_xlarge,\n    deberta_v2_xlarge, \n    deberta_v2_xxlarge,\n    deberta_v3_base, \n    deberta_v3_large\n]","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:15:14.199291Z","iopub.execute_input":"2022-11-27T16:15:14.199955Z","iopub.status.idle":"2022-11-27T16:15:14.210446Z","shell.execute_reply.started":"2022-11-27T16:15:14.199920Z","shell.execute_reply":"2022-11-27T16:15:14.209173Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"filtered_model_cfgs = []\nfor cfg in pretrained_models_cfg:\n    score, c = get_learner_score([cfg], Ridge(alpha=4.4), folds=4, save=False)\n    if score < 0.5:\n        filtered_model_cfgs.append(cfg)\n#learner = Ridge(alpha=4.4)\nlearner = SVR(C=2.0)\n#learner = BayesianRidge()\nsvr_score, models_cfg = get_learner_score(filtered_model_cfgs, learner, folds=svr_folds, save=False, verbose=True)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-27T16:15:14.212297Z","iopub.execute_input":"2022-11-27T16:15:14.213047Z","iopub.status.idle":"2022-11-27T16:16:20.103760Z","shell.execute_reply.started":"2022-11-27T16:15:14.213012Z","shell.execute_reply":"2022-11-27T16:16:20.102678Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"model_set=['deberta_large_mnli'];   score=0.4780494349953554\nmodel_set=['roberta_base'];   score=0.5148243161379941\nmodel_set=['roberta_large'];   score=0.5823614424366776\nmodel_set=['deberta_base'];   score=0.5119876822876401\nmodel_set=['deberta_large'];   score=0.4738268967405775\nmodel_set=['deberta_xlarge'];   score=0.4828337051123891\nmodel_set=['deberta_v2_xlarge'];   score=0.5163828322242732\nmodel_set=['deberta_v2_xxlarge'];   score=0.5271556434448376\nmodel_set=['deberta_v3_base'];   score=0.47566183177648963\nmodel_set=['deberta_v3_large'];   score=0.47098341043780884\n#########################\n### Fold 1\nScore: 0.4649014278164291\n#########################\n### Fold 2\nScore: 0.44381013713319484\n#########################\n### Fold 3\nScore: 0.45025832075534894\n#########################\n### Fold 4\nScore: 0.45890894464202114\n#########################\n### Fold 5\nScore: 0.44670712003356017\n#########################\n### Fold 6\nScore: 0.45484160250934164\n#########################\n### Fold 7\nScore: 0.43771691876010904\n#########################\n### Fold 8\nScore: 0.43505213875525445\n#########################\n### Fold 9\nScore: 0.45905146953468523\n#########################\n### Fold 10\nScore: 0.45991283243894676\n#########################\n### Fold 11\nScore: 0.44476630756101715\n#########################\n### Fold 12\nScore: 0.4498550568695716\n#########################\n### Fold 13\nScore: 0.44540245176973386\n#########################\n### Fold 14\nScore: 0.45855922914335073\n#########################\n### Fold 15\nScore: 0.45674624513430695\nmodel_set=['deberta_large_mnli', 'deberta_large', 'deberta_xlarge', 'deberta_v3_base', 'deberta_v3_large'];   score=0.45109934685712477\n","output_type":"stream"}]},{"cell_type":"code","source":"from cuml import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet, ForestInference\nfrom cuml.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB, CategoricalNB\nfrom cuml.ensemble import RandomForestRegressor","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:16:20.105114Z","iopub.execute_input":"2022-11-27T16:16:20.105941Z","iopub.status.idle":"2022-11-27T16:16:20.111940Z","shell.execute_reply.started":"2022-11-27T16:16:20.105893Z","shell.execute_reply":"2022-11-27T16:16:20.110843Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# best_score = np.inf\n# best_model = None\n# for model in [\n#     LinearRegression,\n#     LogisticRegression, \n#     Ridge, \n#     Lasso, \n#     ElasticNet, \n#     SVR,\n#     MultinomialNB, \n#     BernoulliNB, \n#     GaussianNB, \n#     CategoricalNB,\n#     RandomForestRegressor\n# ]:\n#     svr_score, models_cfg = get_learner_score(pretrained_models_cfg, model(), folds=svr_folds, save=False, verbose=True)\n#     if svr_score < best_score:\n#         best_score = svr_score \n#         best_model = model\n#         print(best_model)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-11-27T16:16:20.113823Z","iopub.execute_input":"2022-11-27T16:16:20.114544Z","iopub.status.idle":"2022-11-27T16:16:20.122475Z","shell.execute_reply.started":"2022-11-27T16:16:20.114507Z","shell.execute_reply":"2022-11-27T16:16:20.121658Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#learner = Ridge(alpha=2.0)\nlearner = SVR(C=2.0)\n#learner = BayesianRidge()\n#learner = LinearRegression(positive=True)\nsvr_score, models_cfg = get_learner_score(pretrained_models_cfg, learner, folds=svr_folds, save=False, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:16:20.124130Z","iopub.execute_input":"2022-11-27T16:16:20.124509Z","iopub.status.idle":"2022-11-27T16:17:22.727306Z","shell.execute_reply.started":"2022-11-27T16:16:20.124475Z","shell.execute_reply":"2022-11-27T16:17:22.726276Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"#########################\n### Fold 1\nScore: 0.4611352297648386\n#########################\n### Fold 2\nScore: 0.4455631938399686\n#########################\n### Fold 3\nScore: 0.44824615108490207\n#########################\n### Fold 4\nScore: 0.45683002067370887\n#########################\n### Fold 5\nScore: 0.4463190051086067\n#########################\n### Fold 6\nScore: 0.4529797722366456\n#########################\n### Fold 7\nScore: 0.4376779134862339\n#########################\n### Fold 8\nScore: 0.43295210081368674\n#########################\n### Fold 9\nScore: 0.45809241686972846\n#########################\n### Fold 10\nScore: 0.4570004911259682\n#########################\n### Fold 11\nScore: 0.44454187373990606\n#########################\n### Fold 12\nScore: 0.44979555053609893\n#########################\n### Fold 13\nScore: 0.4431794291273042\n#########################\n### Fold 14\nScore: 0.45831270686047226\n#########################\n### Fold 15\nScore: 0.456480321861058\nmodel_set=['deberta_large_mnli', 'roberta_base', 'roberta_large', 'deberta_base', 'deberta_large', 'deberta_xlarge', 'deberta_v2_xlarge', 'deberta_v2_xxlarge', 'deberta_v3_base', 'deberta_v3_large'];   score=0.4499404118086085\n","output_type":"stream"}]},{"cell_type":"code","source":"len(models_cfg)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:17:22.729433Z","iopub.execute_input":"2022-11-27T16:17:22.730096Z","iopub.status.idle":"2022-11-27T16:17:22.736917Z","shell.execute_reply.started":"2022-11-27T16:17:22.730057Z","shell.execute_reply":"2022-11-27T16:17:22.735986Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"pretrained_models_cfg = [\n    deberta_large_mnli,\n    #gpt2,\n    roberta_base,\n    roberta_large,\n    #xlnet_base, \n    #xlnet_large,\n    deberta_base, \n    deberta_large, \n    deberta_xlarge,\n    deberta_v2_xlarge, \n    deberta_v2_xxlarge,\n    deberta_v3_base, \n    deberta_v3_large,\n    \n    #bart_base,\n    bart_large,\n    #bart_lage_mnli,\n    #bert_base_uncased,\n    bert_large_uncased,\n    #muppet_roberta_large,\n    funnel_small,\n    funnel_large\n]\nprint(len(pretrained_models_cfg))\n\n# learner = Ridge(alpha=2.0)\nlearner = SVR(C=2.0, tol=0.0001)\nsvr_score, models_cfg = get_learner_score(pretrained_models_cfg, learner, folds=svr_folds, save=True, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T16:29:57.059978Z","iopub.execute_input":"2022-11-27T16:29:57.060338Z","iopub.status.idle":"2022-11-27T16:31:42.539984Z","shell.execute_reply.started":"2022-11-27T16:29:57.060307Z","shell.execute_reply":"2022-11-27T16:31:42.538201Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"14\n#########################\n### Fold 1\nScore: 0.46187953648724395\n#########################\n### Fold 2\nScore: 0.44586148247944607\n#########################\n### Fold 3\nScore: 0.44858766598142025\n#########################\n### Fold 4\nScore: 0.4559360917770415\n#########################\n### Fold 5\nScore: 0.44542110910704596\n#########################\n### Fold 6\nScore: 0.4521296113672048\n#########################\n### Fold 7\nScore: 0.4391163709721421\n#########################\n### Fold 8\nScore: 0.43110553529886464\n#########################\n### Fold 9\nScore: 0.45439678510384834\n#########################\n### Fold 10\nScore: 0.4575696147121531\n#########################\n### Fold 11\nScore: 0.44354563236007777\n#########################\n### Fold 12\nScore: 0.4502317118302322\n#########################\n### Fold 13\nScore: 0.44421093988159877\n#########################\n### Fold 14\nScore: 0.4567753881450587\n#########################\n### Fold 15\nScore: 0.45616062403519525\nmodel_set=['deberta_large_mnli', 'roberta_base', 'roberta_large', 'deberta_base', 'deberta_large', 'deberta_xlarge', 'deberta_v2_xlarge', 'deberta_v2_xxlarge', 'deberta_v3_base', 'deberta_v3_large', 'facebook_bart_large', 'bert_large_uncased', 'funnel_transformer_small', 'funnel_transformer_large'];   score=0.4495285399692383\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}